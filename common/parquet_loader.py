#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Parquet ê¸°ë°˜ ê³ ì† ë°ì´í„° ë¡œë”
- CSV/Parquet bulk ingest ìµœì í™”
- HDD í™˜ê²½ì—ì„œ ìµœê³  ì„±ëŠ¥ ì¶”ì¶œ
- WAL ì—†ëŠ” append-only êµ¬ì¡°
"""

import pandas as pd
import duckdb
import os
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Union
import logging
import pyarrow as pa
import pyarrow.parquet as pq
from concurrent.futures import ThreadPoolExecutor, as_completed
import tempfile
import json


class ParquetDataLoader:
    """Parquet ê¸°ë°˜ ê³ ì† ë°ì´í„° ë¡œë”"""
    
    def __init__(self, base_path: str = "/data/parquet_storage"):
        """
        ì´ˆê¸°í™”
        
        Args:
            base_path: Parquet íŒŒì¼ ì €ì¥ ê¸°ë³¸ ê²½ë¡œ
        """
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ê²½ë¡œ ì„¤ì •
        self.staging_path = self.base_path / "staging"
        self.archive_path = self.base_path / "archive"
        self.temp_path = self.base_path / "temp"
        
        for path in [self.staging_path, self.archive_path, self.temp_path]:
            path.mkdir(exist_ok=True)
        
        self.logger.info(f"Parquet ë°ì´í„° ë¡œë” ì´ˆê¸°í™”: {base_path}")
    
    def save_to_parquet(self, df: pd.DataFrame, 
                       table_name: str, 
                       partition_cols: Optional[List[str]] = None,
                       compression: str = 'snappy') -> str:
        """
        DataFrameì„ Parquet íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            df: ì €ì¥í•  DataFrame
            table_name: í…Œì´ë¸” ì´ë¦„
            partition_cols: íŒŒí‹°ì…˜ ì»¬ëŸ¼
            compression: ì••ì¶• ë°©ì‹
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{table_name}_{timestamp}.parquet"
            file_path = self.staging_path / filename
            
            # íŒŒí‹°ì…˜ì´ ìˆëŠ” ê²½ìš°
            if partition_cols:
                # íŒŒí‹°ì…˜ë³„ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±í•˜ì—¬ ì €ì¥
                partition_path = self.staging_path / table_name / timestamp
                partition_path.mkdir(parents=True, exist_ok=True)
                
                table = pa.Table.from_pandas(df)
                pq.write_to_dataset(
                    table, 
                    root_path=str(partition_path),
                    partition_cols=partition_cols,
                    compression=compression,
                    use_legacy_dataset=False
                )
                
                saved_path = str(partition_path)
                
            else:
                # ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥
                df.to_parquet(
                    file_path, 
                    compression=compression, 
                    index=False,
                    engine='pyarrow'
                )
                
                saved_path = str(file_path)
            
            self.logger.info(f"Parquet ì €ì¥ ì™„ë£Œ: {saved_path} ({len(df)} ë ˆì½”ë“œ)")
            return saved_path
            
        except Exception as e:
            self.logger.error(f"Parquet ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def bulk_load_from_parquet(self, db_path: str, 
                              table_name: str, 
                              parquet_files: List[str],
                              create_table_sql: Optional[str] = None) -> bool:
        """
        Parquet íŒŒì¼ë“¤ì„ DuckDBë¡œ ê³ ì† ë¡œë“œ
        
        Args:
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
            table_name: ëŒ€ìƒ í…Œì´ë¸” ì´ë¦„
            parquet_files: Parquet íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            create_table_sql: í…Œì´ë¸” ìƒì„± SQL (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            with duckdb.connect(db_path) as conn:
                conn.execute('SET memory_limit="2GB"')
                conn.execute("SET threads=4")
                
                # í…Œì´ë¸” ìƒì„± (í•„ìš”í•œ ê²½ìš°)
                if create_table_sql:
                    conn.execute(create_table_sql)
                
                total_records = 0
                start_time = datetime.now()
                
                for parquet_file in parquet_files:
                    if os.path.exists(parquet_file):
                        # Parquet íŒŒì¼ì—ì„œ ì§ì ‘ ì½ì–´ì„œ ì‚½ì… (ë§¤ìš° ë¹ ë¦„)
                        if os.path.isdir(parquet_file):
                            # íŒŒí‹°ì…˜ëœ ë””ë ‰í† ë¦¬
                            conn.execute(f"""
                                INSERT INTO {table_name} 
                                SELECT * FROM read_parquet('{parquet_file}/**/*.parquet')
                            """)
                        else:
                            # ë‹¨ì¼ íŒŒì¼
                            conn.execute(f"""
                                INSERT INTO {table_name} 
                                SELECT * FROM read_parquet('{parquet_file}')
                            """)
                        
                        # ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
                        result = conn.execute(f"""
                            SELECT COUNT(*) FROM read_parquet('{parquet_file}/**/*.parquet' if '{parquet_file}'.endswith('/') else '{parquet_file}')
                        """).fetchone()
                        
                        if result:
                            records = result[0]
                            total_records += records
                            self.logger.info(f"ë¡œë“œ ì™„ë£Œ: {parquet_file} ({records} ë ˆì½”ë“œ)")
                
                # ìµœì¢… ì²´í¬í¬ì¸íŠ¸
                conn.execute("CHECKPOINT")
                
                duration = (datetime.now() - start_time).total_seconds()
                self.logger.info(f"ì „ì²´ ë¡œë“œ ì™„ë£Œ: {total_records} ë ˆì½”ë“œ, {duration:.2f}ì´ˆ")
                
                return True
                
        except Exception as e:
            self.logger.error(f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def create_append_only_structure(self, db_path: str, table_name: str) -> bool:
        """
        Append-only ë¡œê·¸ í…Œì´ë¸” êµ¬ì¡° ìƒì„±
        
        Args:
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
            table_name: ê¸°ë³¸ í…Œì´ë¸” ì´ë¦„
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            with duckdb.connect(db_path) as conn:
                # 1. ë©”ì¸ í…Œì´ë¸” (ì½ê¸° ì „ìš©) - yfinance í˜¸í™˜ì„±ì„ ìœ„í•œ Adjusted Close ê¸°ì¤€
                # ì‹œìŠ¤í…œ ì¼ê´€ì„±: yfinance auto_adjust=Trueì™€ ë™ì¼í•œ ë°©ì‹
                # close = Adjusted Close (ë¶„í• /ë°°ë‹¹ ë°˜ì˜) - ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜
                conn.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name}_main (
                        symbol VARCHAR,
                        date DATE,
                        open DOUBLE,            -- FDR adj_open (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
                        high DOUBLE,            -- FDR adj_high (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
                        low DOUBLE,             -- FDR adj_low (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
                        close DOUBLE,           -- FDR adj_close (ë¶„í• /ë°°ë‹¹ ë°˜ì˜) = yfinance close
                        volume BIGINT,          -- ê±°ë˜ëŸ‰
                        raw_close DOUBLE,       -- FDR raw close (ë¶„í• /ë°°ë‹¹ ë¯¸ë°˜ì˜) - ì„ íƒì  ë³´ê´€
                        batch_id VARCHAR,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # 2. ë¡œê·¸ í…Œì´ë¸” (append-only) - yfinance í˜¸í™˜ì„±ì„ ìœ„í•œ Adjusted Close ê¸°ì¤€
                conn.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name}_log (
                        symbol VARCHAR,
                        date DATE,
                        open DOUBLE,            -- FDR adj_open (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
                        high DOUBLE,            -- FDR adj_high (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
                        low DOUBLE,             -- FDR adj_low (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
                        close DOUBLE,           -- FDR adj_close (ë¶„í• /ë°°ë‹¹ ë°˜ì˜) = yfinance close
                        volume BIGINT,
                        raw_close DOUBLE,       -- FDR raw close (ë¶„í• /ë°°ë‹¹ ë¯¸ë°˜ì˜) - ì„ íƒì  ë³´ê´€
                        batch_id VARCHAR,
                        operation VARCHAR DEFAULT 'INSERT',
                        log_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # 3. ë°°ì¹˜ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
                conn.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name}_batches (
                        batch_id VARCHAR PRIMARY KEY,
                        parquet_file VARCHAR,
                        record_count INTEGER,
                        start_date DATE,
                        end_date DATE,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        merged_at TIMESTAMP
                    )
                """)
                
                # 4. í†µí•© ë·° ìƒì„± - yfinance í˜¸í™˜ì„± ìš°ì„ 
                # ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ ì™„ë²½í•œ í˜¸í™˜ì„±ì„ ìœ„í•´ close = adjusted close
                conn.execute(f"""
                    CREATE VIEW IF NOT EXISTS {table_name}_view AS
                    SELECT * FROM {table_name}_main
                    UNION ALL
                    SELECT 
                        symbol, date, 
                        open,           -- Adjusted Open (yfinance í˜¸í™˜)
                        high,           -- Adjusted High (yfinance í˜¸í™˜)
                        low,            -- Adjusted Low (yfinance í˜¸í™˜)
                        close,          -- Adjusted Close (yfinance í˜¸í™˜)
                        volume, 
                        raw_close,      -- Raw Close (ë¶„ì„ìš© ì¶”ê°€ ì •ë³´)
                        batch_id, log_timestamp as created_at
                    FROM {table_name}_log
                    WHERE operation = 'INSERT'
                """)
                
                self.logger.info(f"Append-only êµ¬ì¡° ìƒì„± ì™„ë£Œ: {table_name} (yfinance í˜¸í™˜)")
                self.logger.info("ğŸ“ ì»¬ëŸ¼ ì˜ë¯¸ (yfinance ì¼ê´€ì„±):")
                self.logger.info("  - open/high/low/close: Adjusted OHLC (ë¶„í• /ë°°ë‹¹ ë°˜ì˜) = yfinance ë™ì¼")
                self.logger.info("  - raw_close: Raw Close (ë¶„í• /ë°°ë‹¹ ë¯¸ë°˜ì˜) - ì¶”ê°€ ë¶„ì„ìš©")
                self.logger.info("  - volume: ê±°ë˜ëŸ‰")
                return True
                
        except Exception as e:
            self.logger.error(f"Append-only êµ¬ì¡° ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def append_data_batch(self, df: pd.DataFrame, 
                         table_name: str, 
                         db_path: str,
                         batch_id: Optional[str] = None) -> bool:
        """
        ë°ì´í„°ë¥¼ append-only ë°©ì‹ìœ¼ë¡œ ì¶”ê°€
        
        Args:
            df: ì¶”ê°€í•  ë°ì´í„°
            table_name: í…Œì´ë¸” ì´ë¦„
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
            batch_id: ë°°ì¹˜ ID
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            if batch_id is None:
                batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # 1. Parquet íŒŒì¼ë¡œ ì„ì‹œ ì €ì¥
            df_with_batch = df.copy()
            df_with_batch['batch_id'] = batch_id
            
            parquet_file = self.save_to_parquet(df_with_batch, f"{table_name}_batch")
            
            # 2. ë¡œê·¸ í…Œì´ë¸”ì— ì§ì ‘ ë¡œë“œ (WAL ìµœì†Œí™”)
            with duckdb.connect(db_path) as conn:
                conn.execute(f"""
                    INSERT INTO {table_name}_log 
                    SELECT *, 'INSERT' as operation, CURRENT_TIMESTAMP as log_timestamp
                    FROM read_parquet('{parquet_file}')
                """)
                
                # 3. ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ì €ì¥
                record_count = len(df)
                start_date = df['date'].min() if 'date' in df.columns else None
                end_date = df['date'].max() if 'date' in df.columns else None
                
                conn.execute(f"""
                    INSERT INTO {table_name}_batches 
                    (batch_id, parquet_file, record_count, start_date, end_date)
                    VALUES (?, ?, ?, ?, ?)
                """, (batch_id, parquet_file, record_count, start_date, end_date))
                
                # ì¦‰ì‹œ ì²´í¬í¬ì¸íŠ¸ (WAL flush)
                conn.execute("CHECKPOINT")
            
            self.logger.info(f"ë°°ì¹˜ ì¶”ê°€ ì™„ë£Œ: {batch_id} ({record_count} ë ˆì½”ë“œ)")
            return True
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def merge_batches_to_main(self, table_name: str, 
                             db_path: str, 
                             max_batches: int = 10) -> bool:
        """
        ë¡œê·¸ í…Œì´ë¸”ì˜ ë°°ì¹˜ë“¤ì„ ë©”ì¸ í…Œì´ë¸”ë¡œ ë³‘í•©
        
        Args:
            table_name: í…Œì´ë¸” ì´ë¦„
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
            max_batches: í•œ ë²ˆì— ë³‘í•©í•  ìµœëŒ€ ë°°ì¹˜ ìˆ˜
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            with duckdb.connect(db_path) as conn:
                # 1. ë³‘í•©í•  ë°°ì¹˜ ì¡°íšŒ
                unmerged_batches = conn.execute(f"""
                    SELECT batch_id, parquet_file, record_count
                    FROM {table_name}_batches 
                    WHERE merged_at IS NULL 
                    ORDER BY created_at 
                    LIMIT {max_batches}
                """).fetchall()
                
                if not unmerged_batches:
                    self.logger.info("ë³‘í•©í•  ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return True
                
                total_records = 0
                merged_batch_ids = []
                
                # 2. ë°°ì¹˜ë³„ë¡œ ë©”ì¸ í…Œì´ë¸”ë¡œ ì´ë™ - yfinance í˜¸í™˜ì„± ìœ ì§€
                for batch_id, parquet_file, record_count in unmerged_batches:
                    conn.execute(f"""
                        INSERT INTO {table_name}_main
                        SELECT symbol, date, open, high, low, close, volume, raw_close,
                               batch_id, log_timestamp as created_at
                        FROM {table_name}_log 
                        WHERE batch_id = ? AND operation = 'INSERT'
                    """, (batch_id,))
                    
                    total_records += record_count
                    merged_batch_ids.append(batch_id)
                
                # 3. ë³‘í•©ëœ ë¡œê·¸ ë°ì´í„° ì‚­ì œ
                for batch_id in merged_batch_ids:
                    conn.execute(f"""
                        DELETE FROM {table_name}_log 
                        WHERE batch_id = ?
                    """, (batch_id,))
                
                # 4. ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                conn.execute(f"""
                    UPDATE {table_name}_batches 
                    SET merged_at = CURRENT_TIMESTAMP 
                    WHERE batch_id IN ({','.join(['?'] * len(merged_batch_ids))})
                """, merged_batch_ids)
                
                # 5. ìµœì¢… ì²´í¬í¬ì¸íŠ¸
                conn.execute("CHECKPOINT")
                
                self.logger.info(f"ë°°ì¹˜ ë³‘í•© ì™„ë£Œ: {len(merged_batch_ids)}ê°œ ë°°ì¹˜, {total_records} ë ˆì½”ë“œ")
                return True
                
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ë³‘í•© ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_storage(self, table_name: str, db_path: str) -> bool:
        """
        ì €ì¥ì†Œ ìµœì í™” (ì••ì¶•, ì¸ë±ìŠ¤ ë“±)
        
        Args:
            table_name: í…Œì´ë¸” ì´ë¦„
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            with duckdb.connect(db_path) as conn:
                # 1. í†µê³„ ì—…ë°ì´íŠ¸
                conn.execute(f"ANALYZE {table_name}_main")
                
                # 2. ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼)
                conn.execute(f"""
                    CREATE TABLE {table_name}_main_clean AS 
                    SELECT DISTINCT * FROM {table_name}_main
                """)
                
                conn.execute(f"DROP TABLE {table_name}_main")
                conn.execute(f"ALTER TABLE {table_name}_main_clean RENAME TO {table_name}_main")
                
                # 3. ì¸ë±ìŠ¤ ì¬ìƒì„±
                conn.execute(f"""
                    CREATE INDEX IF NOT EXISTS idx_{table_name}_symbol_date 
                    ON {table_name}_main(symbol, date)
                """)
                
                # 4. ì²´í¬í¬ì¸íŠ¸
                conn.execute("CHECKPOINT")
                
                self.logger.info(f"ì €ì¥ì†Œ ìµœì í™” ì™„ë£Œ: {table_name}")
                return True
                
        except Exception as e:
            self.logger.error(f"ì €ì¥ì†Œ ìµœì í™” ì‹¤íŒ¨: {e}")
            return False
    
    def export_to_parquet_archive(self, table_name: str, 
                                 db_path: str,
                                 partition_by: str = 'date') -> str:
        """
        ë©”ì¸ í…Œì´ë¸”ì„ Parquet ì•„ì¹´ì´ë¸Œë¡œ ë‚´ë³´ë‚´ê¸°
        
        Args:
            table_name: í…Œì´ë¸” ì´ë¦„
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
            partition_by: íŒŒí‹°ì…˜ ê¸°ì¤€ ì»¬ëŸ¼
        
        Returns:
            ì•„ì¹´ì´ë¸Œ ê²½ë¡œ
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_dir = self.archive_path / f"{table_name}_{timestamp}"
            archive_dir.mkdir(exist_ok=True)
            
            with duckdb.connect(db_path) as conn:
                # Parquetìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (íŒŒí‹°ì…˜ëœ)
                conn.execute(f"""
                    COPY (SELECT * FROM {table_name}_main) 
                    TO '{archive_dir}' 
                    (FORMAT PARQUET, PARTITION_BY ({partition_by}))
                """)
            
            self.logger.info(f"Parquet ì•„ì¹´ì´ë¸Œ ìƒì„±: {archive_dir}")
            return str(archive_dir)
            
        except Exception as e:
            self.logger.error(f"Parquet ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_storage_stats(self, table_name: str, db_path: str) -> Dict:
        """ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ"""
        try:
            stats = {}
            
            with duckdb.connect(db_path) as conn:
                # ë©”ì¸ í…Œì´ë¸” í†µê³„
                main_stats = conn.execute(f"""
                    SELECT COUNT(*) as count, 
                           MIN(date) as min_date, 
                           MAX(date) as max_date,
                           COUNT(DISTINCT symbol) as symbol_count
                    FROM {table_name}_main
                """).fetchone()
                
                # ë¡œê·¸ í…Œì´ë¸” í†µê³„
                log_stats = conn.execute(f"""
                    SELECT COUNT(*) as count,
                           COUNT(DISTINCT batch_id) as batch_count
                    FROM {table_name}_log
                """).fetchone()
                
                # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° í†µê³„
                batch_stats = conn.execute(f"""
                    SELECT COUNT(*) as total_batches,
                           COUNT(CASE WHEN merged_at IS NULL THEN 1 END) as unmerged_batches
                    FROM {table_name}_batches
                """).fetchone()
                
                stats = {
                    'main_table': {
                        'record_count': main_stats[0] if main_stats else 0,
                        'date_range': {
                            'start': main_stats[1] if main_stats else None,
                            'end': main_stats[2] if main_stats else None
                        },
                        'symbol_count': main_stats[3] if main_stats else 0
                    },
                    'log_table': {
                        'record_count': log_stats[0] if log_stats else 0,
                        'batch_count': log_stats[1] if log_stats else 0
                    },
                    'batches': {
                        'total': batch_stats[0] if batch_stats else 0,
                        'unmerged': batch_stats[1] if batch_stats else 0
                    }
                }
            
            # íŒŒì¼ ì‹œìŠ¤í…œ í†µê³„
            if os.path.exists(db_path):
                stats['file_sizes'] = {
                    'database_mb': os.path.getsize(db_path) / (1024 * 1024),
                    'wal_mb': os.path.getsize(db_path + '.wal') / (1024 * 1024) if os.path.exists(db_path + '.wal') else 0
                }
            
            return stats
            
        except Exception as e:
            self.logger.error(f"ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    
    loader = ParquetDataLoader("/tmp/parquet_test")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± - yfinance í˜¸í™˜ì„±ì„ ìœ„í•œ Adjusted Close ê¸°ì¤€
    # ì‹œìŠ¤í…œ ì¼ê´€ì„±: yfinance auto_adjust=Trueì™€ ë™ì¼í•œ ë°©ì‹
    # close = Adjusted Close (ë¶„í• /ë°°ë‹¹ ë°˜ì˜) - ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì™„ë²½ í˜¸í™˜
    test_data = pd.DataFrame({
        'symbol': ['AAPL'] * 1000 + ['GOOGL'] * 1000,
        'date': pd.date_range('2023-01-01', periods=2000, freq='1H')[:2000],
        'open': [99.5] * 2000,      # Adjusted Open (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
        'high': [119.5] * 2000,     # Adjusted High (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)  
        'low': [89.5] * 2000,       # Adjusted Low (ë¶„í• /ë°°ë‹¹ ë°˜ì˜)
        'close': [109.5] * 2000,    # Adjusted Close (ë¶„í• /ë°°ë‹¹ ë°˜ì˜) = yfinance close
        'volume': [5000] * 2000,    # ê±°ë˜ëŸ‰
        'raw_close': [110.0] * 2000 # Raw Close (ë¶„í• /ë°°ë‹¹ ë¯¸ë°˜ì˜) - ì¶”ê°€ ë¶„ì„ìš©
    })
    
    db_path = "/tmp/test_parquet.db"
    table_name = "stock_data"
    
    try:
        # 1. Append-only êµ¬ì¡° ìƒì„±
        loader.create_append_only_structure(db_path, table_name)
        
        # 2. ë°°ì¹˜ ë°ì´í„° ì¶”ê°€
        loader.append_data_batch(test_data, table_name, db_path)
        
        # 3. í†µê³„ ì¡°íšŒ
        stats = loader.get_storage_stats(table_name, db_path)
        print(f"ì €ì¥ì†Œ í†µê³„: {stats}")
        
        # 4. ë°°ì¹˜ ë³‘í•©
        loader.merge_batches_to_main(table_name, db_path)
        
        # 5. ìµœì¢… í†µê³„
        final_stats = loader.get_storage_stats(table_name, db_path)
        print(f"ìµœì¢… í†µê³„: {final_stats}")
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
