#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ì¼ë³„ ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ê¸° (yfinance ê¸°ë°˜)
- FinanceDataReader ëŒ€ëŸ‰ ìˆ˜ì§‘ ì´í›„ ì¼ë³„ ì—…ë°ì´íŠ¸ìš©
- yfinance auto_adjust=True ì‚¬ìš©ìœ¼ë¡œ ê¸°ì¡´ FDR ë°ì´í„°ì™€ ì™„ë²½ í˜¸í™˜
- Airflow DAGì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ìµœì í™”
"""

import pandas as pd
import yfinance as yf
import duckdb
import os
import time
from datetime import datetime, timedelta, date
from typing import List, Dict, Optional, Tuple
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc
import threading
from pathlib import Path

class DailyIncrementalCollector:
    """ì¼ë³„ ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ê¸° (yfinance ê¸°ë°˜)"""
    
    def __init__(self, db_path: str = "/data/duckdb/stock_data.db", 
                 max_workers: int = 5, batch_size: int = 50):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
            max_workers: ìµœëŒ€ ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜ (yfinance API ì œí•œ ê³ ë ¤)
            batch_size: ë°°ì¹˜ í¬ê¸°
        """
        self.db_path = db_path
        self.max_workers = max_workers
        self.batch_size = batch_size
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì“°ê¸° ì „ìš© ì—°ê²° (ë‹¨ì¼ ìŠ¤ë ˆë“œ)
        self._write_lock = threading.Lock()
        self._write_conn = None
        
        self._initialize_database()
    
    def _initialize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ìµœì í™” ì„¤ì •"""
        with self._get_write_connection() as conn:
            # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            conn.execute('SET memory_limit="1GB"')
            conn.execute("SET threads=2")
            
            # yfinance í˜¸í™˜ í…Œì´ë¸” í™•ì¸ ë° ìƒì„±
            # âš ï¸ ì¤‘ìš”: FinanceDataReader bulk_data_collectorì™€ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©
            conn.execute("""
                CREATE TABLE IF NOT EXISTS stock_data (
                    symbol VARCHAR,
                    date DATE,
                    open DOUBLE,            -- yfinance adjusted open (auto_adjust=True)
                    high DOUBLE,            -- yfinance adjusted high (auto_adjust=True)
                    low DOUBLE,             -- yfinance adjusted low (auto_adjust=True)
                    close DOUBLE,           -- yfinance adjusted close (auto_adjust=True)
                    volume BIGINT,          -- yfinance volume
                    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    PRIMARY KEY (symbol, date)
                )
            """)
            
            # ì¦ë¶„ ìˆ˜ì§‘ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
            conn.execute("""
                CREATE TABLE IF NOT EXISTS daily_collection_log (
                    collection_date DATE,
                    symbols_attempted INTEGER,
                    symbols_success INTEGER,
                    symbols_failed INTEGER,
                    last_data_date DATE,
                    collection_started_at TIMESTAMP,
                    collection_completed_at TIMESTAMP,
                    notes TEXT
                )
            """)
            
            self.logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ (yfinance ì¦ë¶„ ìˆ˜ì§‘)")
    
    def _get_write_connection(self):
        """ì“°ê¸° ì „ìš© ì—°ê²° ë°˜í™˜"""
        if self._write_conn is None:
            self._write_conn = duckdb.connect(self.db_path)
        return self._write_conn
    
    def get_symbols_for_update(self) -> List[str]:
        """
        ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        
        Returns:
            ì¢…ëª© ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            with duckdb.connect(self.db_path) as conn:
                # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ” ì¢…ëª©ë“¤ ì¡°íšŒ
                symbols_df = conn.execute("""
                    SELECT DISTINCT symbol 
                    FROM stock_data 
                    ORDER BY symbol
                """).fetchdf()
                
                if symbols_df.empty:
                    self.logger.warning("ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. bulk_data_collectorë¡œ ë¨¼ì € ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
                    return []
                
                symbols = symbols_df['symbol'].tolist()
                self.logger.info(f"ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì¢…ëª© ìˆ˜: {len(symbols)}")
                return symbols
                
        except Exception as e:
            self.logger.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_last_data_date(self, symbol: str) -> Optional[date]:
        """
        íŠ¹ì • ì¢…ëª©ì˜ ë§ˆì§€ë§‰ ë°ì´í„° ë‚ ì§œ ì¡°íšŒ
        
        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼
            
        Returns:
            ë§ˆì§€ë§‰ ë°ì´í„° ë‚ ì§œ ë˜ëŠ” None
        """
        try:
            with duckdb.connect(self.db_path) as conn:
                result = conn.execute("""
                    SELECT MAX(date) 
                    FROM stock_data 
                    WHERE symbol = ?
                """, (symbol,)).fetchone()
                
                if result and result[0]:
                    return result[0]
                return None
                
        except Exception as e:
            self.logger.error(f"ì¢…ëª© {symbol} ë§ˆì§€ë§‰ ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def collect_single_symbol_incremental(self, symbol: str, 
                                        start_date: Optional[date] = None) -> Dict:
        """
        ë‹¨ì¼ ì¢…ëª© ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼
            start_date: ì‹œì‘ ë‚ ì§œ (Noneì´ë©´ ìë™ ê³„ì‚°)
        
        Returns:
            ìˆ˜ì§‘ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì‹œì‘ ë‚ ì§œ ê²°ì •
            if start_date is None:
                last_date = self.get_last_data_date(symbol)
                if last_date:
                    # ë§ˆì§€ë§‰ ë‚ ì§œ ë‹¤ìŒë‚ ë¶€í„° ìˆ˜ì§‘
                    start_date = last_date + timedelta(days=1)
                else:
                    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìµœê·¼ 5ì¼
                    start_date = datetime.now().date() - timedelta(days=5)
            
            end_date = datetime.now().date()
            
            # ìˆ˜ì§‘í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            if start_date >= end_date:
                return {
                    'symbol': symbol,
                    'status': 'skipped',
                    'reason': 'no_new_data',
                    'records_added': 0,
                    'last_date': start_date - timedelta(days=1)
                }
            
            # yfinanceë¡œ ë°ì´í„° ìˆ˜ì§‘ (auto_adjust=True ê¸°ë³¸ê°’)
            ticker = yf.Ticker(symbol)
            
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (yfinanceëŠ” ë¬¸ìì—´ í˜•ì‹ ì„ í˜¸)
            start_str = start_date.strftime('%Y-%m-%d')
            end_str = (end_date + timedelta(days=1)).strftime('%Y-%m-%d')  # ì¢…ë£Œì¼ í¬í•¨
            
            hist = ticker.history(start=start_str, end=end_str, auto_adjust=True)
            
            if hist.empty:
                return {
                    'symbol': symbol,
                    'status': 'no_data',
                    'reason': 'empty_response',
                    'records_added': 0,
                    'last_date': None
                }
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            hist = hist.reset_index()
            hist['symbol'] = symbol
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬ (yfinance auto_adjust=True ì‚¬ìš© ì‹œ)
            # auto_adjust=Trueì´ë©´ ëª¨ë“  OHLCê°€ ì´ë¯¸ ì¡°ì •ëœ ê°’
            column_mapping = {
                'Date': 'date',
                'Open': 'open',
                'High': 'high',
                'Low': 'low',
                'Close': 'close',  # ì´ë¯¸ adjusted close
                'Volume': 'volume'
            }
            hist.rename(columns=column_mapping, inplace=True)
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            required_columns = ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']
            hist = hist[required_columns]
            
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            hist['date'] = pd.to_datetime(hist['date']).dt.date
            hist['volume'] = hist['volume'].fillna(0).astype('int64')
            
            # NaN ì œê±°
            hist = hist.dropna()
            
            if hist.empty:
                return {
                    'symbol': symbol,
                    'status': 'no_data',
                    'reason': 'empty_after_cleaning',
                    'records_added': 0,
                    'last_date': None
                }
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (UPSERT ë°©ì‹)
            records_added = self._save_incremental_data(hist)
            
            return {
                'symbol': symbol,
                'status': 'success',
                'records_added': records_added,
                'last_date': hist['date'].max(),
                'date_range': f"{hist['date'].min()} ~ {hist['date'].max()}"
            }
            
        except Exception as e:
            return {
                'symbol': symbol,
                'status': 'error',
                'reason': str(e),
                'records_added': 0,
                'last_date': None
            }
    
    def _save_incremental_data(self, df: pd.DataFrame) -> int:
        """
        ì¦ë¶„ ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
        
        Args:
            df: ì €ì¥í•  ë°ì´í„°í”„ë ˆì„
            
        Returns:
            ì‹¤ì œ ì¶”ê°€ëœ ë ˆì½”ë“œ ìˆ˜
        """
        try:
            with self._write_lock:
                with self._get_write_connection() as conn:
                    # ê¸°ì¡´ ë°ì´í„°ì™€ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ UPSERT
                    # 1. ì„ì‹œ í…Œì´ë¸”ì— ìƒˆ ë°ì´í„° ì €ì¥
                    conn.execute("DROP TABLE IF EXISTS temp_new_data")
                    conn.execute("""
                        CREATE TEMP TABLE temp_new_data AS 
                        SELECT * FROM df
                    """)
                    
                    # 2. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê°™ì€ symbol, date)
                    conn.execute("""
                        DELETE FROM stock_data 
                        WHERE (symbol, date) IN (
                            SELECT symbol, date FROM temp_new_data
                        )
                    """)
                    
                    # 3. ìƒˆ ë°ì´í„° ì‚½ì… (ì¤‘ë³µì‹œ ì—…ë°ì´íŠ¸)
                    conn.execute("""
                        INSERT INTO stock_data (symbol, date, open, high, low, close, volume, collected_at)
                        SELECT symbol, date, open, high, low, close, volume, CURRENT_TIMESTAMP
                        FROM temp_new_data
                        ON CONFLICT (symbol, date) DO UPDATE SET
                            open = EXCLUDED.open,
                            high = EXCLUDED.high,
                            low = EXCLUDED.low,
                            close = EXCLUDED.close,
                            volume = EXCLUDED.volume,
                            collected_at = EXCLUDED.collected_at
                    """)
                    
                    # 4. ì²´í¬í¬ì¸íŠ¸
                    conn.execute("CHECKPOINT")
                    
                    return len(df)
                    
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    def collect_daily_batch(self, target_date: Optional[date] = None) -> Dict:
        """
        ì¼ë³„ ë°°ì¹˜ ìˆ˜ì§‘ (ëª¨ë“  ì¢…ëª©)
        
        Args:
            target_date: ìˆ˜ì§‘í•  ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
            
        Returns:
            ìˆ˜ì§‘ ê²°ê³¼ í†µê³„
        """
        if target_date is None:
            target_date = datetime.now().date()
        
        collection_start = datetime.now()
        self.logger.info(f"ì¼ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹œì‘: {target_date}")
        
        # ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì¢…ëª© ì¡°íšŒ
        symbols = self.get_symbols_for_update()
        if not symbols:
            return {
                'status': 'failed',
                'reason': 'no_symbols',
                'collection_date': target_date,
                'symbols_attempted': 0,
                'symbols_success': 0,
                'symbols_failed': 0
            }
        
        total_symbols = len(symbols)
        success_count = 0
        failed_count = 0
        results = []
        
        try:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë³‘ë ¬ ì²˜ë¦¬
            for i in range(0, total_symbols, self.batch_size):
                batch_symbols = symbols[i:i + self.batch_size]
                self.logger.info(f"ë°°ì¹˜ {i//self.batch_size + 1} ì²˜ë¦¬ ì¤‘: {len(batch_symbols)}ê°œ ì¢…ëª©")
                
                batch_results = []
                
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # ì‘ì—… ì œì¶œ
                    future_to_symbol = {
                        executor.submit(
                            self.collect_single_symbol_incremental, 
                            symbol,
                            target_date  # íŠ¹ì • ë‚ ì§œë¶€í„° ìˆ˜ì§‘
                        ): symbol 
                        for symbol in batch_symbols
                    }
                    
                    # ê²°ê³¼ ìˆ˜ì§‘
                    for future in as_completed(future_to_symbol):
                        symbol = future_to_symbol[future]
                        try:
                            result = future.result(timeout=30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                            batch_results.append(result)
                            
                            if result['status'] == 'success':
                                success_count += 1
                                self.logger.info(f"âœ… {symbol}: {result['records_added']}ê°œ ë ˆì½”ë“œ ì¶”ê°€")
                            elif result['status'] == 'skipped':
                                self.logger.info(f"â­ï¸ {symbol}: {result['reason']}")
                            else:
                                failed_count += 1
                                self.logger.warning(f"âŒ {symbol}: {result['reason']}")
                                
                        except Exception as e:
                            failed_count += 1
                            self.logger.error(f"âŒ {symbol} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            batch_results.append({
                                'symbol': symbol,
                                'status': 'error',
                                'reason': f'timeout_or_exception: {e}',
                                'records_added': 0
                            })
                
                results.extend(batch_results)
                
                # ì§„í–‰ë¥  ì¶œë ¥
                progress = (i + len(batch_symbols)) / total_symbols * 100
                self.logger.info(f"ì§„í–‰ë¥ : {progress:.1f}% ({success_count} ì„±ê³µ, {failed_count} ì‹¤íŒ¨)")
                
                # API ì œí•œ ë°©ì§€ íœ´ì‹
                time.sleep(1)
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
            
            collection_end = datetime.now()
            duration = (collection_end - collection_start).total_seconds()
            
            # ìˆ˜ì§‘ ë¡œê·¸ ì €ì¥
            self._save_collection_log(target_date, total_symbols, success_count, failed_count, 
                                    collection_start, collection_end, f"Duration: {duration:.1f}s")
            
            summary = {
                'status': 'completed',
                'collection_date': target_date,
                'symbols_attempted': total_symbols,
                'symbols_success': success_count,
                'symbols_failed': failed_count,
                'duration_seconds': duration,
                'results': results
            }
            
            self.logger.info(f"ì¼ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_symbols} ì„±ê³µ ({duration:.1f}ì´ˆ)")
            return summary
            
        except Exception as e:
            self.logger.error(f"ì¼ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                'status': 'failed',
                'reason': str(e),
                'collection_date': target_date,
                'symbols_attempted': total_symbols,
                'symbols_success': success_count,
                'symbols_failed': failed_count
            }
    
    def _save_collection_log(self, collection_date: date, attempted: int, success: int, failed: int,
                           start_time: datetime, end_time: datetime, notes: str = ""):
        """ìˆ˜ì§‘ ë¡œê·¸ ì €ì¥"""
        try:
            with self._write_lock:
                with self._get_write_connection() as conn:
                    conn.execute("""
                        INSERT INTO daily_collection_log 
                        (collection_date, symbols_attempted, symbols_success, symbols_failed, 
                         last_data_date, collection_started_at, collection_completed_at, notes)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (collection_date, attempted, success, failed, collection_date, 
                          start_time, end_time, notes))
                    
                    conn.execute("CHECKPOINT")
                    
        except Exception as e:
            self.logger.error(f"ìˆ˜ì§‘ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_collection_stats(self) -> Dict:
        """ìˆ˜ì§‘ í†µê³„ ì¡°íšŒ"""
        try:
            with duckdb.connect(self.db_path) as conn:
                # ì „ì²´ ë°ì´í„° í†µê³„
                main_stats = conn.execute("""
                    SELECT 
                        COUNT(DISTINCT symbol) as symbol_count,
                        COUNT(*) as total_records,
                        MIN(date) as earliest_date,
                        MAX(date) as latest_date,
                        MAX(collected_at) as last_collection
                    FROM stock_data
                """).fetchone()
                
                # ìµœê·¼ ìˆ˜ì§‘ ë¡œê·¸
                recent_logs = conn.execute("""
                    SELECT * FROM daily_collection_log 
                    ORDER BY collection_date DESC 
                    LIMIT 5
                """).fetchdf()
                
                return {
                    'symbol_count': main_stats[0] if main_stats else 0,
                    'total_records': main_stats[1] if main_stats else 0,
                    'date_range': {
                        'start': main_stats[2] if main_stats else None,
                        'end': main_stats[3] if main_stats else None
                    },
                    'last_collection': main_stats[4] if main_stats else None,
                    'recent_collections': recent_logs.to_dict('records') if not recent_logs.empty else []
                }
                
        except Exception as e:
            self.logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    def close(self):
        """ì—°ê²° ì •ë¦¬"""
        if self._write_conn:
            self._write_conn.close()
            self._write_conn = None


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ - yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    collector = DailyIncrementalCollector(
        db_path="/tmp/test_daily_incremental.db",
        max_workers=3,
        batch_size=10
    )
    
    try:
        print("ğŸ”„ yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # ì¼ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤í–‰
        result = collector.collect_daily_batch()
        
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {result}")
        
        # í†µê³„ í™•ì¸
        stats = collector.get_collection_stats()
        print(f"ğŸ“ˆ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„: {stats}")
        
        print("âœ… yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    finally:
        collector.close()
