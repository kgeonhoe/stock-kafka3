FROM apache/airflow:2.7.3-python3.10

USER root

# 필수 시스템 패키지 설치
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# 데이터 디렉토리 생성 및 권한 설정
RUN mkdir -p /data/duckdb && chown -R airflow:root /data

USER airflow

# requirements 파일 복사
COPY ./requirements-airflow.txt /tmp/requirements.txt

# Java 설치 (PySpark 필요)
USER root
RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    && rm -rf /var/lib/apt/lists/*

USER airflow

# 환경 변수 설정 (Java)
ENV JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64"

# PySpark를 먼저 설치 (constraints 없이)
RUN pip install --no-cache-dir pyspark==3.5.0

# 나머지 패키지는 constraints와 함께 설치
RUN pip install --no-cache-dir -r /tmp/requirements.txt \
    --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.7.3/constraints-3.10.txt"

# 환경 변수 설정
ENV PYTHONPATH="/opt/airflow:/opt/airflow/common:/opt/airflow/config"
ENV AIRFLOW_HOME="/opt/airflow"

# Airflow가 정상 설치되었는지 확인
RUN airflow version

# 기본 entrypoint 유지
ENTRYPOINT ["/usr/bin/dumb-init", "--", "/entrypoint"]
CMD ["bash"]
