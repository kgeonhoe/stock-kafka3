#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ (Streamlit í˜ì´ì§€)

ê¸°ëŠ¥:
 - í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ì…ë ¥ (duration, workers ë“±)
 - í…ŒìŠ¤íŠ¸ ì‹œì‘/ì¤‘ë‹¨
 - ì‹¤ì‹œê°„ TPS / ëˆ„ì  ì¹´ìš´í„° / ë¼ì¸ì°¨íŠ¸
 - ì™„ë£Œ í›„ Summary ì¶œë ¥

ì£¼ì˜:
 - scripts.kafka_load_tester.KafkaLoadTester ì— ì˜ì¡´
 - ë‹¤ì¤‘ ì‹¤í–‰ ì‹œ ì´ì „ í…ŒìŠ¤íŠ¸ ê°ì²´ë¥¼ íê¸°í•˜ê³  ìƒˆë¡œ ìƒì„±
"""

import streamlit as st
import time
import threading
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import Optional
import sys
import os

# ---- Import fallback ì²˜ë¦¬ ----
_import_error = None
try:
    from scripts.kafka_load_tester import LoadTestConfig, KafkaLoadTester, SLOCriteria
except Exception as e:
    _import_error = e
    # ìƒìœ„ ë£¨íŠ¸ ì¶”ê°€ í›„ ì¬ì‹œë„
    ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if ROOT_DIR not in sys.path:
        sys.path.insert(0, ROOT_DIR)
    try:
        from scripts.kafka_load_tester import LoadTestConfig, KafkaLoadTester, SLOCriteria
        _import_error = None
    except Exception as e2:  # pragma: no cover
        _import_error = e2

if _import_error:
    st.error(f"kafka_load_tester import ì‹¤íŒ¨: {_import_error}")
    st.stop()

st.set_page_config(page_title="Kafka Load Test", page_icon="ğŸ“ˆ", layout="wide")
st.title("ğŸ“ˆ Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸")

# ---------------- ê°€ì´ë“œ / ì¶”ì²œ ì„¤ì • í‘œì‹œ ----------------
with st.expander("ğŸ“˜ ë¶€í•˜í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ / ì¶”ì²œê°’", expanded=False):
    st.markdown(
        """
### 1. ëª©ì 
Kafka ì¸í”„ë¼ê°€ (ì§€ì—°Â·ì—ëŸ¬Â·ìì›ì‚¬ìš©) SLOë¥¼ ë§Œì¡±í•˜ë©´ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì•ˆì • ì²˜ë¦¬ëŸ‰(capacity)ì„ ì°¾ê³  ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

### 2. í•µì‹¬ ì§€í‘œ (í…ŒìŠ¤íŠ¸ ì¤‘ ì¶”ì  ê¶Œì¥)
- Throughput: msgs/sec, MB/sec (Producer/Consumer ëª¨ë‘)
- End-to-End Latency: p50 / p95 / p99 (ì¶”ê°€ êµ¬í˜„ ì˜ˆì •)
- Producer Ack Latency / Error Rate
- Consumer Lag (íŒŒíŠ¸ì…˜ë³„ ëˆ„ì  ì§€ì—°)
- Error ìœ í˜•: timeout, NOT_ENOUGH_REPLICAS, ì¬ì‹œë„, rebalance ë¹ˆë„
- ìì› ì‚¬ìš©: Broker CPU / ë©”ëª¨ë¦¬ / ë„¤íŠ¸ì›Œí¬ / ë””ìŠ¤í¬ flush time
- íŒŒí‹°ì…˜ ìŠ¤í: (ìµœëŒ€ íŒŒí‹°ì…˜ ì²˜ë¦¬ëŸ‰) / (í‰ê·  ì²˜ë¦¬ëŸ‰)

### 3. ê¶Œì¥ ì ˆì°¨ (Ramp-Up)
1) ë©”ì‹œì§€ í¬ê¸°, í˜•ì‹ ê³ ì • (ì˜ˆ: 300~800B JSON)
2) ëª©í‘œì¹˜ì˜ 20% â†’ 40% â†’ 60% â†’ 80% â†’ 100% ìˆœì°¨ ìƒìŠ¹ (ë‹¨ê³„ë‹¹ 3~5ë¶„ ìœ ì§€)
3) ì„ê³„ ì§•í›„ (Latency ê¸‰ë“±, Lag ëˆ„ì , Error Rate ìƒìŠ¹) ê´€ì°° ì‹œì  ê¸°ë¡
4) ë³‘ëª© ìœ í˜•ë³„ íŠœë‹ (ë°°ì¹˜/linger/ì••ì¶•/íŒŒí‹°ì…˜/ìŠ¤ë ˆë“œ) í›„ ì¬ì‹¤í–‰ ë¹„êµ
5) ì•ˆì • êµ¬ê°„ê³¼ ë¶•ê´´(Degradation) êµ¬ê°„ ê²½ê³„ ëª…í™•í™”

### 4. í”„ë¡œë“€ì„œ ì¶”ì²œ ê¸°ë³¸ê°’ (ì†Œí˜• ë‹¨ì¼ ë¸Œë¡œì»¤, ë©”ì‹œì§€ <1KB ê¸°ì¤€)
| í•­ëª© | ê°’ | ë¹„ê³  |
|------|----|------|
| linger.ms | 5~10 (ì§€ì—° ëœ ì¤‘ìš”í•˜ë©´ 15~20) | ë°°ì¹˜ ê²°ì§‘ â†‘ Throughput â†‘ |
| batch.size | 65536 (64KB) â†’ í•„ìš”ì‹œ 131072 | ê³¼ë„í•˜ë©´ ë©”ëª¨ë¦¬ â†‘ |
| compression.type | lz4 (ê· í˜•) / zstd (CPU í—ˆìš©) | gzip ì€ CPU ë¹„ìš©â†‘ |
| acks | all | ë°ì´í„° ë‚´êµ¬ì„± í™•ë³´ |
| enable.idempotence | true | ì¤‘ë³µ ë°©ì§€ |
| max.in.flight.requests.per.connection | 5 | ì§€ì—° ë¯¼ê° ì‹œ 1 |
| buffer.memory | ê¸°ë³¸ 32MB â†’ 64MB | ì›Œì»¤ ë§ì„ ë•Œ í™•ì¥ |

### 5. í† í”½ / ì»¨ìŠˆë¨¸ ì„¤ì • ê°€ì´ë“œ
- íŒŒí‹°ì…˜: ì´ˆê¸° 6~12 (í™•ì¥ ì—¬ì§€ ê³ ë ¤). ì²˜ë¦¬ëŸ‰ ëª©í‘œ / (ë‹¨ì¼ íŒŒí‹°ì…˜ ì²˜ë¦¬ í•œê³„)ë¡œ ì—­ì‚°
- auto.offset.reset: ìµœì‹  run ì •ë°€ ì¸¡ì •ì€ latest ê¶Œì¥ (ë°±ë¡œê·¸ ì˜í–¥ ì œê±°)
- Consumer fetch.min.bytes: 8KB~32KB (Throughput ìš°ì„  ì‹œ â†‘, ì§€ì—° ìš°ì„  ì‹œ â†“)
- fetch.max.wait.ms: 20~50ms

### 6. ë‹¨ê³„ë³„ ë¶€í•˜ ì˜ˆì‹œ (ëª©í‘œ 10K msg/s)
| ë‹¨ê³„ | ëª©í‘œ TPS | ë¹„ê³  |
|------|----------|------|
| 1 | 2K | ì•ˆì • ë² ì´ìŠ¤ë¼ì¸ |
| 2 | 4K | ì§€ì—°/ì—ëŸ¬ ì¶”ì„¸ í™•ì¸ |
| 3 | 6K | Lag ë°œìƒ ì—¬ë¶€ |
| 4 | 8K | ìì› ì—¬ìœ  í•œê³„ ì ‘ê·¼ |
| 5 | 10K | ëª©í‘œ SLO ìœ ì§€ ì—¬ë¶€ |
| 6 | 11~12K | ë¶•ê´´ êµ¬ê°„ íƒì§€ |

### 7. íŠœë‹ ìš°ì„ ìˆœìœ„ ë¡œì§
1) p95 Latency ì¦ê°€ + CPU ì—¬ìœ  â†’ linger / batch í™•ì¥, ë” ê°•í•œ ì••ì¶•
2) CPU í¬í™” + Latency ì¦ê°€ â†’ ì••ì¶• ì™„í™”(lz4) / í”„ë¡œë“€ì„œ ì›Œì»¤Â·íŒŒí‹°ì…˜ ì¬ì¡°ì •
3) ë„¤íŠ¸ì›Œí¬ í¬í™” â†’ ì••ì¶• ê°•ë„ â†‘ ë˜ëŠ” ë©”ì‹œì§€ ë¬¶ê¸° (aggregation)
4) Lag ì¦ê°€ + Broker ì—¬ìœ  â†’ consumer fetch.min.bytes â†‘, polling íš¨ìœ¨í™”
5) Rebalance ì¦ìŒ â†’ consumer ìˆ˜/ì„¸ì…˜ ì„¤ì • ì¬ì¡°ì •

### 8. í˜„ì¬ í˜ì´ì§€ ê°œì„  ì˜ˆì •(ì„ íƒ êµ¬í˜„)
- run_id ê¸°ë°˜ í•„í„° (ë°±ë¡œê·¸/ê³¼ê±° ë©”ì‹œì§€ ì œì™¸)
- End-to-End Latency ì¸¡ì • (producer timestamp ì‚½ì… â†’ consumer ìˆ˜ì‹  ì‹œ ì°¨ì´)
- Latency p50/p95/p99 ê·¸ë˜í”„
- ê²°ê³¼ JSON/CSV ë‹¤ìš´ë¡œë“œ / ì‹œë‚˜ë¦¬ì˜¤ ìë™ ì‹¤í–‰ ë§¤íŠ¸ë¦­ìŠ¤
- earliest/latest ì„ íƒ UI

### 9. ë¹ ë¥¸ Baseline (ë‹¨ì¼ ë¸Œë¡œì»¤ 5K msg/s ëª©í‘œ)
- Topic: partitions=8, replication.factor=1
- Producer: linger.ms=8, batch.size=65536, compression=lz4, acks=all, idempotence=true
- Consumer: auto.offset.reset=latest, fetch.min.bytes=8KB, fetch.max.wait.ms=40
- Ramp: 1K â†’ 3K â†’ 5K â†’ 6K (ë¶•ê´´ ì§•í›„ ê¸°ë¡)

ìœ„ ì„¤ì •ì€ ì‹œì‘ì ì¼ ë¿ì´ë©°, ëª©í‘œ SLO(ì˜ˆ: p95 < 300ms, ì—ëŸ¬ìœ¨ <0.1%, Broker CPU <70%)ì„ ë§Œì¡±í•˜ëŠ” ìµœëŒ€ TPS ì§€ì ì´ "í˜„ì¬ ìµœì "ì…ë‹ˆë‹¤.
        """
    )

# ---------------- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ----------------

def _init_state():
    defaults = {
        'tester': None,
        'test_thread': None,
        'load_test_running': False,
        'load_test_summary': None,
        'last_metrics': None,
        'last_refresh': 0.0,
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v

_init_state()

# ---------------- ì‚¬ì´ë“œë°” ì„¤ì • í¼ ----------------
with st.sidebar:
    st.subheader("âš™ï¸ í…ŒìŠ¤íŠ¸ ì„¤ì •")
    duration_minutes = st.number_input("Duration (ë¶„)", 0.1, 120.0, 1.0, 0.5, help="ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„")
    kafka_producers = st.number_input("Kafka Producers", 1, 200, 5)
    kafka_consumers = st.number_input("Kafka Consumers", 0, 200, 2)
    api_workers = st.number_input("API Workers", 0, 100, 2)
    messages_per_worker = st.number_input("Messages per Producer Worker", 10, 1_000_000, 500, step=50)
    topics_raw = st.text_input("Topics (ê³µë°± êµ¬ë¶„)", "realtime-stock yfinance-stock-data")
    snapshot_interval = st.slider("ìŠ¤ëƒ…ìƒ· ì£¼ê¸°(ì´ˆ)", 0.2, 5.0, 1.0, 0.2)
    auto_refresh_sec = st.slider("í™”ë©´ ê°±ì‹  ì£¼ê¸°(ì´ˆ)", 0.5, 10.0, 2.0, 0.5)

    with st.expander("SLO ê¸°ì¤€ ì„¤ì •", expanded=False):
        col_slo1, col_slo2 = st.columns(2)
        with col_slo1:
            slo_min_success = st.number_input("ì„±ê³µë¥  â‰¥ (%)", 90.0, 100.0, 99.5, 0.1, help="ìµœì†Œ ì„±ê³µë¥ ")
            slo_max_cpu = st.number_input("í‰ê·  CPU â‰¤ (%)", 10.0, 100.0, 70.0, 1.0)
            slo_min_thr = st.number_input("ì²˜ë¦¬ëŸ‰ â‰¥ (msg/s)", 0.0, 1_000_000.0, 0.0, 100.0)
        with col_slo2:
            slo_max_errors = st.number_input("ì´ ì˜¤ë¥˜ â‰¤", 0, 1_000_000, 0, 1)
            slo_gap_pct = st.number_input("í¸ì°¨ â‰¤ (%)", 0.0, 100.0, 5.0, 0.5, help="Consumer/Producer ëˆ„ì  í¸ì°¨")
        use_slo = st.checkbox("SLO í‰ê°€ í™œì„±í™”", value=True)

    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        start_clicked = st.button("ğŸš€ í…ŒìŠ¤íŠ¸ ì‹œì‘", type="primary", disabled=st.session_state.load_test_running)
    with col_btn2:
        stop_clicked = st.button("ğŸ›‘ ì¤‘ë‹¨", disabled=not st.session_state.load_test_running)

# ---------------- í…ŒìŠ¤íŠ¸ ì‹œì‘ ë¡œì§ ----------------

def _start_test():
    # ê¸°ì¡´ ìŠ¤ë ˆë“œ ì •ë¦¬ (ê°€ëŠ¥í•˜ë©´ soft stop)
    if st.session_state.tester and st.session_state.load_test_running:
        try:
            st.session_state.tester.is_running = False
        except Exception:
            pass
    topics = [t.strip() for t in topics_raw.split() if t.strip()]
    config = LoadTestConfig(
        duration_minutes=float(duration_minutes),
        kafka_producer_workers=int(kafka_producers),
        kafka_consumer_workers=int(kafka_consumers),
        api_call_workers=int(api_workers),
        messages_per_worker=int(messages_per_worker),
        topics=topics,
    )
    slo_obj = None
    if 'SLOCriteria' in globals() and use_slo:
        try:
            slo_obj = SLOCriteria(
                min_success_rate=float(slo_min_success),
                max_error_count=int(slo_max_errors),
                max_avg_cpu_usage=float(slo_max_cpu),
                max_consumer_producer_gap_pct=float(slo_gap_pct),
                min_overall_throughput=float(slo_min_thr)
            )
        except Exception:
            slo_obj = None
    tester = KafkaLoadTester(config, slo_obj)
    tester.snapshot_interval_sec = float(snapshot_interval)
    st.session_state.tester = tester
    st.session_state.load_test_summary = None
    st.session_state.load_test_running = True
    st.session_state.last_metrics = None

    def _run():
        try:
            summary = tester.run_load_test()
            st.session_state.load_test_summary = summary
        finally:
            st.session_state.load_test_running = False

    th = threading.Thread(target=_run, daemon=True, name="load_test_runner")
    # Streamlit ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì»¨í…ìŠ¤íŠ¸ ë“±ë¡ (missing ScriptRunContext ê²½ê³  ë°©ì§€)
    try:  # Streamlit ë²„ì „ë³„ ì•ˆì „ ì²˜ë¦¬
        from streamlit.runtime.scriptrunner import add_script_run_ctx
        add_script_run_ctx(th)
    except Exception:
        pass
    st.session_state.test_thread = th
    th.start()


def _stop_test():
    if st.session_state.tester:
        st.session_state.tester.is_running = False


if start_clicked:
    _start_test()
if stop_clicked:
    _stop_test()

# ---------------- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í‘œì‹œ ----------------
placeholder_charts = st.empty()


def _render_live():
    tester: Optional[KafkaLoadTester] = st.session_state.tester
    if not tester:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì • í›„ 'í…ŒìŠ¤íŠ¸ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return
    metrics = tester.get_live_metrics()
    st.session_state.last_metrics = metrics

    # ìƒë‹¨ ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Producer ëˆ„ì ", f"{metrics['producer_sent_total']:,}", f"TPS {metrics['producer_tps']:.1f}")
    col2.metric("Consumer ëˆ„ì ", f"{metrics['consumer_received_total']:,}", f"TPS {metrics['consumer_tps']:.1f}")
    col3.metric("API ì„±ê³µ", f"{metrics['api_success_total']:,}", f"TPS {metrics['api_tps']:.1f}")
    elapsed = metrics['snapshots'][-1]['elapsed_sec'] if metrics['snapshots'] else 0
    col4.metric("ê²½ê³¼(ì´ˆ)", f"{elapsed:.1f}")
    # run_id í‘œì‹œ
    if hasattr(tester, 'run_id') and tester.run_id:
        st.caption(f"run_id: {tester.run_id}")

    # ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
    snaps = metrics['snapshots']
    if snaps:
        df = pd.DataFrame(snaps)
        # ëˆ„ì  ì¹´ìš´í„° ê·¸ë˜í”„ (go.Figure ì§ì ‘ êµ¬ì„± - RecursionError íšŒí”¼)
        cum_fig = go.Figure()
        cum_fig.add_trace(go.Scatter(x=df['elapsed_sec'], y=df['producer_sent'], name='producer_sent', mode='lines'))
        cum_fig.add_trace(go.Scatter(x=df['elapsed_sec'], y=df['consumer_received'], name='consumer_received', mode='lines'))
        cum_fig.add_trace(go.Scatter(x=df['elapsed_sec'], y=df['api_success'], name='api_success', mode='lines'))
        cum_fig.update_layout(title='ëˆ„ì  ì¹´ìš´í„°', xaxis_title='ê²½ê³¼(sec)', yaxis_title='ëˆ„ì ')
        placeholder_charts.plotly_chart(cum_fig, use_container_width=True)

        # TPS ê³„ì‚°
        if len(df) > 1:
            elapsed_diff = df['elapsed_sec'].diff()
            # 0 ë˜ëŠ” ìŒìˆ˜ ë°©ì§€
            elapsed_diff[elapsed_diff <= 0] = float('nan')
            tps_df = pd.DataFrame({
                'elapsed_sec': df['elapsed_sec'],
                'producer_tps': df['producer_sent'].diff() / elapsed_diff,
                'consumer_tps': df['consumer_received'].diff() / elapsed_diff,
                'api_tps': df['api_success'].diff() / elapsed_diff,
            }).dropna()
            if not tps_df.empty:
                tps_fig = go.Figure()
                tps_fig.add_trace(go.Scatter(x=tps_df['elapsed_sec'], y=tps_df['producer_tps'], name='producer_tps', mode='lines'))
                tps_fig.add_trace(go.Scatter(x=tps_df['elapsed_sec'], y=tps_df['consumer_tps'], name='consumer_tps', mode='lines'))
                tps_fig.add_trace(go.Scatter(x=tps_df['elapsed_sec'], y=tps_df['api_tps'], name='api_tps', mode='lines'))
                tps_fig.update_layout(title='ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰ (TPS)', xaxis_title='ê²½ê³¼(sec)', yaxis_title='TPS')
                st.plotly_chart(tps_fig, use_container_width=True)
            else:
                st.info("TPS ê³„ì‚° ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ")
        else:
            st.info("TPS ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        st.info("ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘...")

    # ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ í‘œì‹œ
    if metrics.get('recent_errors'):
        with st.expander("ìµœê·¼ ì—ëŸ¬ (ìµœê·¼ 50ê°œ)"):
            for e in metrics['recent_errors'][-50:]:
                st.code(e)


def _render_summary():
    summary = st.session_state.load_test_summary
    if not summary:
        return
    st.success("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    basic_cols = st.columns(5)
    basic_cols[0].metric("ì´ ë©”ì‹œì§€", f"{summary['total_messages']:,}")
    basic_cols[1].metric("ì„±ê³µ", f"{summary['total_success']:,}")
    basic_cols[2].metric("ì‹¤íŒ¨", f"{summary['total_errors']:,}")
    basic_cols[3].metric("ì„±ê³µë¥ ", f"{summary['success_rate']:.1f}%")
    basic_cols[4].metric("ì „ì²´ ì²˜ë¦¬ëŸ‰", f"{summary['overall_throughput']:.2f} msg/s")
    if 'run_id' in summary:
        st.caption(f"run_id: {summary['run_id']}")
    if 'latency_stats' in summary and summary['latency_stats']['count']:
        ls = summary['latency_stats']
        st.write(f"Latency ms: avg {ls['avg_ms']:.2f} | p50 {ls['p50_ms']:.2f} | p95 {ls['p95_ms']:.2f} | p99 {ls['p99_ms']:.2f} | max {ls['max_ms']:.2f}")
    if 'skipped_messages' in summary:
        st.write(f"(run_id ë¶ˆì¼ì¹˜ë¡œ ë¬´ì‹œëœ consumer ë©”ì‹œì§€: {summary['skipped_messages']:,})")

    with st.expander("Producer/Consumer/API ìš”ì•½", expanded=True):
        colp, colc, cola = st.columns(3)
        p = summary['producer_stats']; c = summary['consumer_stats']; a = summary['api_stats']
        colp.write(f"**Producer**\n\nì›Œì»¤: {p['worker_count']}\n\nì´ ì „ì†¡: {p['total_sent']:,}\n\ní‰ê·  TPS: {p['avg_throughput']:.2f}")
        colc.write(f"**Consumer**\n\nì›Œì»¤: {c['worker_count']}\n\nì´ ìˆ˜ì‹ : {c['total_consumed']:,}\n\ní‰ê·  TPS: {c['avg_throughput']:.2f}")
        cola.write(f"**API**\n\nì›Œì»¤: {a['worker_count']}\n\nì´ ì„±ê³µ: {a['total_calls']:,}\n\ní‰ê·  TPS: {a['avg_throughput']:.2f}")

    if summary.get('slo'):
        slo = summary['slo']
        st.subheader("SLO í‰ê°€")
        pass_badge = "âœ… PASS" if slo['passed'] else "âŒ FAIL"
        st.write(f"ì¢…í•© ê²°ê³¼: {pass_badge}")
        # ì²´í¬ í…Œì´ë¸”
        check_rows = []
        for chk in slo['checks']:
            check_rows.append({
                'í•­ëª©': chk['name'],
                'ê²°ê³¼': 'PASS' if chk['passed'] else 'FAIL',
                'Actual': chk['actual'],
                'Expected': chk['expected']
            })
        st.dataframe(pd.DataFrame(check_rows), use_container_width=True, hide_index=True)

    worker_df = pd.DataFrame(summary['worker_results'])
    st.dataframe(worker_df, use_container_width=True, hide_index=True)

    with st.expander("ì›ì‹œ Summary JSON"):
        st.json(summary, expanded=False)


# ---------------- ë Œë”ë§ íë¦„ ----------------
if st.session_state.load_test_running:
    _render_live()
    time.sleep(auto_refresh_sec)
    st.rerun()
else:
    if st.session_state.load_test_summary:
        _render_summary()
    else:
        st.info("í…ŒìŠ¤íŠ¸ ëŒ€ê¸° ìƒíƒœì…ë‹ˆë‹¤. ì¢Œì¸¡ì—ì„œ ì„¤ì • í›„ ì‹œì‘í•˜ì„¸ìš”.")
