#!/bin/bash
# -*- coding: utf-8 -*-

"""
ë‚˜ìŠ¤ë‹¥ ìµœì í™” ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (5ë…„ì¹˜)
- HDD í™˜ê²½ ìµœì í™”
- ë³‘ëª© í•´ê²°
"""

set -e  # ì—ëŸ¬ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# ë¡œê·¸ í•¨ìˆ˜
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

log_step() {
    echo -e "${BLUE}[STEP]${NC} $1"
}

# ê¸°ë³¸ ì„¤ì •
PROJECT_ROOT="/home/grey1/stock-kafka3"
VENV_PATH="${PROJECT_ROOT}/venv"
DATA_DIR="/data"
DUCKDB_DIR="${DATA_DIR}/duckdb"
PARQUET_DIR="${DATA_DIR}/parquet_storage"

# í•¨ìˆ˜: ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
check_system_requirements() {
    log_step "ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘..."
    
    # ë©”ëª¨ë¦¬ í™•ì¸
    available_memory=$(free -g | awk '/^Mem:/{print $7}')
    if [ "$available_memory" -lt 2 ]; then
        log_error "ë©”ëª¨ë¦¬ ë¶€ì¡±: ${available_memory}GB (ìµœì†Œ 2GB í•„ìš”)"
        exit 1
    fi
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ 
    available_disk=$(df -BG "$DATA_DIR" 2>/dev/null | awk 'NR==2{print $4}' | sed 's/G//' || echo "50")
    if [ "$available_disk" -lt 50 ]; then
        log_error "ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: ${available_disk}GB (ìµœì†Œ 50GB í•„ìš”)"
        exit 1
    fi
    
    log_info "ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± âœ“"
    log_info "ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: ${available_memory}GB"
    log_info "ì‚¬ìš© ê°€ëŠ¥ ë””ìŠ¤í¬: ${available_disk}GB"
}

# í•¨ìˆ˜: ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
setup_directories() {
    log_step "ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì¤‘..."
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p "$DUCKDB_DIR"
    mkdir -p "$PARQUET_DIR"/{staging,archive,temp}
    mkdir -p "$DATA_DIR"/{logs,temp}
    
    # ê¶Œí•œ ì„¤ì •
    chmod 755 "$DATA_DIR"
    chmod 755 "$DUCKDB_DIR"
    chmod 755 "$PARQUET_DIR"
    
    log_info "ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ âœ“"
}

# í•¨ìˆ˜: Python í™˜ê²½ ì„¤ì •
setup_python_environment() {
    log_step "Python í™˜ê²½ ì„¤ì • ì¤‘..."
    
    cd "$PROJECT_ROOT"
    
    # ê°€ìƒí™˜ê²½ ìƒì„± (ì—†ëŠ” ê²½ìš°)
    if [ ! -d "$VENV_PATH" ]; then
        log_info "Python ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
        python3 -m venv "$VENV_PATH"
    fi
    
    # ê°€ìƒí™˜ê²½ í™œì„±í™”
    source "$VENV_PATH/bin/activate"
    
    # pip ì—…ê·¸ë ˆì´ë“œ
    pip install --upgrade pip
    
    # í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
    log_info "í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
    pip install -r requirements.txt || {
        log_warn "requirements.txtê°€ ì—†ìŠµë‹ˆë‹¤. í•„ìˆ˜ íŒ¨í‚¤ì§€ë¥¼ ì§ì ‘ ì„¤ì¹˜í•©ë‹ˆë‹¤."
        pip install \
            FinanceDataReader \
            duckdb \
            pandas \
            pyarrow \
            psutil \
            apache-airflow \
            redis
    }
    
    log_info "Python í™˜ê²½ ì„¤ì • ì™„ë£Œ âœ“"
}

# í•¨ìˆ˜: DuckDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
initialize_database() {
    log_step "DuckDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘..."
    
    cd "$PROJECT_ROOT"
    source "$VENV_PATH/bin/activate"
    
    # Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    python3 -c "
import sys
sys.path.insert(0, '$PROJECT_ROOT/common')

from parquet_loader import ParquetDataLoader
from batch_scheduler import DuckDBBatchScheduler, CheckpointConfig
import os

# Parquet ë¡œë”ë¡œ Append-only êµ¬ì¡° ìƒì„±
loader = ParquetDataLoader('$PARQUET_DIR')
success = loader.create_append_only_structure('$DUCKDB_DIR/stock_data.db', 'stock_data')

if success:
    print('âœ… DuckDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ')
else:
    print('âŒ DuckDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨')
    exit(1)
"
    
    log_info "DuckDB ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ âœ“"
}

# í•¨ìˆ˜: ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
test_sample_collection() {
    log_step "ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì¤‘..."
    
    cd "$PROJECT_ROOT"
    source "$VENV_PATH/bin/activate"
    
    # ì†ŒëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
    python3 -c "
import sys
sys.path.insert(0, '$PROJECT_ROOT/common')

from bulk_data_collector import BulkDataCollector

# í…ŒìŠ¤íŠ¸ìš© ì†ŒëŸ‰ ìˆ˜ì§‘
collector = BulkDataCollector(
    db_path='$DUCKDB_DIR/stock_data.db',
    batch_size=100,
    max_workers=2
)

try:
    # ìƒìœ„ 5ê°œ ì¢…ëª©ë§Œ í…ŒìŠ¤íŠ¸
    symbols = collector.get_nasdaq_symbols(limit=5)
    if symbols:
        print(f'í…ŒìŠ¤íŠ¸ ì¢…ëª©: {symbols}')
        success = collector.collect_bulk_data(
            symbols=symbols,
            start_date='2024-01-01',
            batch_symbols=2
        )
        
        if success:
            stats = collector.get_collection_stats()
            print(f'âœ… ìƒ˜í”Œ ìˆ˜ì§‘ ì™„ë£Œ: {stats}')
        else:
            print('âŒ ìƒ˜í”Œ ìˆ˜ì§‘ ì‹¤íŒ¨')
            exit(1)
    else:
        print('âŒ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨')
        exit(1)
        
finally:
    collector.close()
"
    
    log_info "ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ âœ“"
}

# í•¨ìˆ˜: Airflow DAG ë°°í¬
deploy_airflow_dags() {
    log_step "Airflow DAG ë°°í¬ ì¤‘..."
    
    # Airflow DAG ë””ë ‰í† ë¦¬ í™•ì¸
    AIRFLOW_DAGS_DIR="${PROJECT_ROOT}/airflow/dags"
    AIRFLOW_HOME="${AIRFLOW_HOME:-/opt/airflow}"
    
    if [ ! -d "$AIRFLOW_HOME" ]; then
        log_warn "Airflowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DAG íŒŒì¼ë§Œ ì¤€ë¹„ë©ë‹ˆë‹¤."
        return 0
    fi
    
    # DAG íŒŒì¼ ë³µì‚¬
    if [ -d "$AIRFLOW_DAGS_DIR" ]; then
        cp "$AIRFLOW_DAGS_DIR"/*.py "$AIRFLOW_HOME/dags/" 2>/dev/null || true
        log_info "Airflow DAG ë°°í¬ ì™„ë£Œ âœ“"
    else
        log_warn "DAG ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $AIRFLOW_DAGS_DIR"
    fi
}

# í•¨ìˆ˜: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •
setup_monitoring() {
    log_step "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì¤‘..."
    
    # ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    cat > "$DATA_DIR/monitor_performance.sh" << 'EOF'
#!/bin/bash
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

while true; do
    timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    memory_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    
    # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
    disk_usage=$(df /data | tail -1 | awk '{print $5}' | sed 's/%//')
    
    # DuckDB íŒŒì¼ í¬ê¸°
    db_size=0
    wal_size=0
    if [ -f "/data/duckdb/stock_data.db" ]; then
        db_size=$(stat -c%s "/data/duckdb/stock_data.db" 2>/dev/null || echo 0)
        db_size=$((db_size / 1024 / 1024))  # MB
    fi
    
    if [ -f "/data/duckdb/stock_data.db.wal" ]; then
        wal_size=$(stat -c%s "/data/duckdb/stock_data.db.wal" 2>/dev/null || echo 0)
        wal_size=$((wal_size / 1024 / 1024))  # MB
    fi
    
    # ë¡œê·¸ ì¶œë ¥
    echo "[$timestamp] ë©”ëª¨ë¦¬: ${memory_usage}% | ë””ìŠ¤í¬: ${disk_usage}% | DB: ${db_size}MB | WAL: ${wal_size}MB"
    
    sleep 60  # 1ë¶„ë§ˆë‹¤
done
EOF
    
    chmod +x "$DATA_DIR/monitor_performance.sh"
    
    log_info "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì™„ë£Œ âœ“"
    log_info "ëª¨ë‹ˆí„°ë§ ì‹¤í–‰: $DATA_DIR/monitor_performance.sh"
}

# í•¨ìˆ˜: ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
run_full_collection() {
    log_step "ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘..."
    
    cd "$PROJECT_ROOT"
    source "$VENV_PATH/bin/activate"
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
    if [ -f "$DATA_DIR/monitor_performance.sh" ]; then
        "$DATA_DIR/monitor_performance.sh" > "$DATA_DIR/logs/performance.log" 2>&1 &
        MONITOR_PID=$!
        log_info "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨ (PID: $MONITOR_PID)"
    fi
    
    # ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
    python3 -c "
import sys
sys.path.insert(0, '$PROJECT_ROOT/common')

from bulk_data_collector import BulkDataCollector
from parquet_loader import ParquetDataLoader
from batch_scheduler import DuckDBBatchScheduler, CheckpointConfig
import time

print('ğŸš€ ë‚˜ìŠ¤ë‹¥ 5ë…„ì¹˜ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...')

# ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
config = CheckpointConfig(
    interval_seconds=300,
    wal_size_threshold_mb=200,
    memory_threshold_percent=75,
    force_checkpoint_interval=1800
)

scheduler = DuckDBBatchScheduler('$DUCKDB_DIR/stock_data.db', config)
scheduler.start()

# ëŒ€ëŸ‰ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
collector = BulkDataCollector(
    db_path='$DUCKDB_DIR/stock_data.db',
    batch_size=2000,
    max_workers=3
)

try:
    # ì „ì²´ ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ìˆ˜ì§‘
    print('ğŸ“Š ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...')
    symbols = collector.get_nasdaq_symbols()
    
    if not symbols:
        raise Exception('ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨')
    
    print(f'âœ… ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ: {len(symbols)}ê°œ')
    
    # 5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘
    print('ğŸ“ˆ 5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...')
    success = collector.collect_bulk_data(
        symbols=symbols,
        start_date='2020-01-01',
        batch_symbols=50
    )
    
    if not success:
        raise Exception('ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨')
    
    # í†µê³„ ì¶œë ¥
    stats = collector.get_collection_stats()
    print(f'âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ:')
    print(f'   ì¢…ëª© ìˆ˜: {stats.get(\"symbol_count\", 0):,}')
    print(f'   ì´ ë ˆì½”ë“œ: {stats.get(\"total_records\", 0):,}')
    print(f'   ë‚ ì§œ ë²”ìœ„: {stats.get(\"date_range\", {})}')
    
    # Parquet ë¡œë”ë¡œ ìµœì í™”
    print('ğŸ”§ ë°ì´í„° ìµœì í™” ì¤‘...')
    parquet_loader = ParquetDataLoader('$PARQUET_DIR')
    
    # ë°°ì¹˜ ë³‘í•©
    parquet_loader.merge_batches_to_main('stock_data', '$DUCKDB_DIR/stock_data.db')
    
    # ì €ì¥ì†Œ ìµœì í™”
    parquet_loader.optimize_storage('stock_data', '$DUCKDB_DIR/stock_data.db')
    
    # ì™„ë£Œ í”Œë˜ê·¸ ìƒì„±
    import json
    from datetime import datetime
    
    completion_info = {
        'completion_time': datetime.now().isoformat(),
        'collection_type': 'bulk_5year_data',
        'final_stats': stats,
        'success': True
    }
    
    with open('/tmp/nasdaq_bulk_collection_complete.flag', 'w') as f:
        json.dump(completion_info, f, indent=2, default=str)
    
    print('ğŸ‰ ë‚˜ìŠ¤ë‹¥ 5ë…„ì¹˜ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!')
    
except Exception as e:
    print(f'âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}')
    exit(1)
    
finally:
    collector.close()
    scheduler.stop()
"
    
    # ëª¨ë‹ˆí„°ë§ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
    if [ -n "$MONITOR_PID" ]; then
        kill $MONITOR_PID 2>/dev/null || true
        log_info "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"
    fi
    
    log_info "ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ âœ“"
}

# í•¨ìˆ˜: ë„ì›€ë§ ì¶œë ¥
show_help() {
    echo "ë‚˜ìŠ¤ë‹¥ ìµœì í™” ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ"
    echo ""
    echo "ì‚¬ìš©ë²•: $0 [ì˜µì…˜]"
    echo ""
    echo "ì˜µì…˜:"
    echo "  setup     - ì „ì²´ í™˜ê²½ ì„¤ì • (ë””ë ‰í† ë¦¬, Python, DB ì´ˆê¸°í™”)"
    echo "  test      - ìƒ˜í”Œ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"
    echo "  full      - ì „ì²´ 5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"
    echo "  monitor   - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ë§Œ ì‹¤í–‰"
    echo "  clean     - ì„ì‹œ íŒŒì¼ ë° ìºì‹œ ì •ë¦¬"
    echo "  status    - í˜„ì¬ ìƒíƒœ í™•ì¸"
    echo "  help      - ì´ ë„ì›€ë§ ì¶œë ¥"
    echo ""
    echo "ì˜ˆì‹œ:"
    echo "  $0 setup    # ì²˜ìŒ ì„¤ì •"
    echo "  $0 test     # í…ŒìŠ¤íŠ¸ ì‹¤í–‰"
    echo "  $0 full     # ì „ì²´ ìˆ˜ì§‘"
}

# í•¨ìˆ˜: ìƒíƒœ í™•ì¸
check_status() {
    log_step "ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘..."
    
    # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ í™•ì¸
    if [ -f "$DUCKDB_DIR/stock_data.db" ]; then
        db_size=$(stat -c%s "$DUCKDB_DIR/stock_data.db" | awk '{printf "%.1f", $1/1024/1024}')
        log_info "DuckDB ë°ì´í„°ë² ì´ìŠ¤: ${db_size}MB"
    else
        log_warn "DuckDB ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤"
    fi
    
    # WAL íŒŒì¼ í™•ì¸
    if [ -f "$DUCKDB_DIR/stock_data.db.wal" ]; then
        wal_size=$(stat -c%s "$DUCKDB_DIR/stock_data.db.wal" | awk '{printf "%.1f", $1/1024/1024}')
        log_info "WAL íŒŒì¼: ${wal_size}MB"
    fi
    
    # Parquet íŒŒì¼ í™•ì¸
    parquet_count=$(find "$PARQUET_DIR" -name "*.parquet" 2>/dev/null | wc -l)
    log_info "Parquet íŒŒì¼ ìˆ˜: $parquet_count"
    
    # ì™„ë£Œ í”Œë˜ê·¸ í™•ì¸
    if [ -f "/tmp/nasdaq_bulk_collection_complete.flag" ]; then
        log_info "ëŒ€ëŸ‰ ìˆ˜ì§‘ ì™„ë£Œ í”Œë˜ê·¸ ì¡´ì¬ âœ“"
    else
        log_warn "ëŒ€ëŸ‰ ìˆ˜ì§‘ ì™„ë£Œ í”Œë˜ê·¸ ì—†ìŒ"
    fi
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
    memory_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    disk_usage=$(df "$DATA_DIR" | tail -1 | awk '{print $5}')
    log_info "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ${memory_usage}%"
    log_info "ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : $disk_usage"
}

# í•¨ìˆ˜: ì •ë¦¬
clean_temp_files() {
    log_step "ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘..."
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
    rm -rf "$PARQUET_DIR/temp"/*
    rm -rf "$DATA_DIR/temp"/*
    
    # ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ (7ì¼ ì´ìƒ)
    find "$DATA_DIR/logs" -name "*.log" -mtime +7 -delete 2>/dev/null || true
    
    # ì™„ë£Œ í”Œë˜ê·¸ ì •ë¦¬ (ì„ íƒì )
    # rm -f /tmp/nasdaq_*_complete.flag
    
    log_info "ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ âœ“"
}

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
main() {
    echo "================================================"
    echo "ë‚˜ìŠ¤ë‹¥ ìµœì í™” ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ"
    echo "HDD í™˜ê²½ ìµœì í™” | 5ë…„ì¹˜ ëŒ€ëŸ‰ ìˆ˜ì§‘"
    echo "================================================"
    echo ""
    
    case "${1:-help}" in
        "setup")
            check_system_requirements
            setup_directories
            setup_python_environment
            initialize_database
            deploy_airflow_dags
            setup_monitoring
            log_info "ğŸ‰ ì „ì²´ í™˜ê²½ ì„¤ì • ì™„ë£Œ!"
            ;;
        "test")
            check_system_requirements
            test_sample_collection
            log_info "ğŸ‰ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!"
            ;;
        "full")
            check_system_requirements
            run_full_collection
            log_info "ğŸ‰ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
            ;;
        "monitor")
            if [ -f "$DATA_DIR/monitor_performance.sh" ]; then
                log_info "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)"
                "$DATA_DIR/monitor_performance.sh"
            else
                log_error "ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'setup'ì„ ì‹¤í–‰í•˜ì„¸ìš”."
            fi
            ;;
        "status")
            check_status
            ;;
        "clean")
            clean_temp_files
            ;;
        "help"|*)
            show_help
            ;;
    esac
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
