# ğŸ”§ ì£¼ìš” DAG ì½”ë“œ ë¶„ì„ ë° êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

## ï¿½ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ í”Œë¡œìš°ì°¨íŠ¸

### ğŸ“ˆ **1. Enhanced NASDAQ Bulk Collection Flow (5ë…„ ë°±í•„)**
```mermaid
graph TD
    %% ë°ì´í„° ì†ŒìŠ¤
    A[NASDAQ API<br/>ì‹¬ë³¼ ëª©ë¡] --> B[collect_nasdaq_symbols_task<br/>ğŸ” ì‹¬ë³¼ ëŒ€ëŸ‰ ìˆ˜ì§‘]
    C[FinanceDataReader API<br/>5ë…„ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°] --> D[bulk_collect_stock_data_task<br/>ğŸ“ˆ 5ë…„ì¹˜ ì£¼ê°€ ë°ì´í„°]
    
    %% ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    B --> E[PostgreSQL<br/>ğŸ“Š nasdaq_symbols í…Œì´ë¸”]
    D --> F[PostgreSQL<br/>ğŸ“Š stock_data í…Œì´ë¸”<br/>1,825ì¼ Ã— Nì¢…ëª©]
    
    %% ë°°ì¹˜ ì²˜ë¦¬
    E --> G{ğŸ“… ì˜¤ëŠ˜ ìˆ˜ì§‘ ì™„ë£Œ?}
    G -->|No| H[BulkDataCollector<br/>ğŸš€ batch_size=200<br/>max_workers=4]
    G -->|Yes| I[Skip Collection<br/>â­ï¸ ì¤‘ë³µ ë°©ì§€]
    
    H --> F
    F --> J[calculate_technical_indicators_task<br/>ğŸ§® ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°]
    
    %% ê¸°ìˆ ì  ë¶„ì„
    J --> K[TechnicalIndicatorCalculatorPostgreSQL<br/>ğŸ“Š RSI, MACD, ë³¼ë¦°ì €ë°´ë“œ]
    K --> L[PostgreSQL<br/>stock_data_technical_indicators]
    
    %% ê´€ì‹¬ì¢…ëª© ìƒì„±
    L --> M[generate_daily_watchlist_task<br/>ğŸ¯ ë‹¤ì¤‘ ì¡°ê±´ ìŠ¤ìº”]
    M --> N[PostgreSQL<br/>daily_watchlist í…Œì´ë¸”]
    
    %% ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    N --> O[ğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­<br/>90-140ë¶„ ì‹¤í–‰ì‹œê°„<br/>65K ë ˆì½”ë“œ/ë°°ì¹˜]
    
    style A fill:#e1f5fe
    style C fill:#e1f5fe
    style E fill:#f3e5f5
    style F fill:#f3e5f5
    style L fill:#f3e5f5
    style N fill:#f3e5f5
    style O fill:#fff3e0
```

### âš¡ **2. Daily Watchlist Scanner Flow (30ë¶„ ì‹¤ì‹œê°„)**
```mermaid
graph TD
    %% 30ë¶„ ì£¼ê¸° ìŠ¤ìº” ì‹œì‘
    A[â° Cron: */30 * * * *<br/>30ë¶„ ì£¼ê¸° ì‹¤í–‰] --> B{ğŸ• ì‹œì¥ ì‹œê°„ëŒ€ ì²´í¬}
    
    %% ì‹œì¥ ì‹œê°„ëŒ€ë³„ ë¶„ê¸°
    B -->|09:30-16:00 EST| C[ğŸ”¥ HIGH ê°•ë„ ìŠ¤ìº”<br/>4ê°œ ì¡°ê±´ í™œì„±í™”]
    B -->|04:00-09:30, 16:00-20:00 EST| D[âš–ï¸ MEDIUM ê°•ë„ ìŠ¤ìº”<br/>2ê°œ ì¡°ê±´ í™œì„±í™”]
    B -->|20:00-04:00 EST| E[ğŸŒ™ LOW ê°•ë„ ìŠ¤ìº”<br/>1ê°œ ì¡°ê±´ë§Œ í™œì„±í™”]
    
    %% ë³‘ë ¬ ìŠ¤ìº” íƒœìŠ¤í¬ë“¤
    C --> F[scan_bollinger_bands<br/>ğŸ“ˆ ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨ í„°ì¹˜]
    C --> G[scan_rsi_oversold<br/>ğŸ“‰ RSI â‰¤ 30 ê³¼ë§¤ë„]
    C --> H[scan_macd_bullish<br/>ğŸ“Š MACD ê°•ì„¸ ì‹ í˜¸]
    C --> I[scan_volume_spike<br/>ğŸ“Š ê±°ë˜ëŸ‰ 2ë°° ì´ìƒ]
    
    D --> F
    D --> I
    E --> F
    
    %% PostgreSQL ìŠ¤ìº”
    F --> J[TechnicalScannerPostgreSQL<br/>ğŸ” ì‹¤ì‹œê°„ ì¡°ê±´ ìŠ¤ìº”]
    G --> J
    H --> J  
    I --> J
    
    %% ê²°ê³¼ ì €ì¥ ë° ë™ê¸°í™”
    J --> K[PostgreSQL<br/>daily_watchlist ì—…ë°ì´íŠ¸]
    K --> L[sync_to_redis<br/>ğŸ”„ Redis ì‹¤ì‹œê°„ ë™ê¸°í™”]
    
    %% ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤
    L --> M[Redis Cache<br/>âš¡ ì‹¤ì‹œê°„ ê´€ì‹¬ì¢…ëª© ë°ì´í„°]
    M --> N[Kafka Producer<br/>ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°]
    N --> O[Streamlit Dashboard<br/>ğŸ“± ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§]
    
    %% ìœ ì§€ë³´ìˆ˜ íƒœìŠ¤í¬ë“¤
    L --> P[get_top_performers<br/>ğŸ† ìƒìœ„ ì„±ê³¼ ë¶„ì„]
    L --> Q[cleanup_old_watchlist<br/>ğŸ§¹ 30ì¼ ì´ì „ ë°ì´í„° ì •ë¦¬]
    
    %% ì„±ëŠ¥ ì¶”ì 
    P --> R[ğŸ“Š 30ë¶„ ì„±ëŠ¥ ë©”íŠ¸ë¦­<br/>3.5-5.5ë¶„ ì‹¤í–‰ì‹œê°„<br/>48íšŒ/ì¼ ì‹¤í–‰]
    
    style A fill:#e8f5e8
    style M fill:#ffe0e0
    style N fill:#fff3e0
    style O fill:#e1f5fe
    style R fill:#fff3e0
```

### ğŸ”„ **3. í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸ Flow**
```mermaid
graph LR
    %% ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤
    subgraph "ğŸŒ External APIs"
        A1[NASDAQ API<br/>ì‹¬ë³¼ ëª©ë¡]
        A2[FinanceDataReader<br/>5ë…„ íˆìŠ¤í† ë¦¬ì»¬]
        A3[KIS API<br/>í•œêµ­ ì£¼ì‹ ë°ì´í„°]
    end
    
    %% Airflow DAGë“¤
    subgraph "ğŸ”§ Airflow DAGs"
        B1[enhanced_nasdaq_bulk_collection_postgres<br/>ğŸ“Š ë§¤ì¼ 07:00 ì‹¤í–‰]
        B2[daily_watchlist_scanner_postgres<br/>âš¡ 30ë¶„ë§ˆë‹¤ ì‹¤í–‰]
    end
    
    %% ë°ì´í„°ë² ì´ìŠ¤ ê³„ì¸µ
    subgraph "ğŸ—„ï¸ PostgreSQL Database"
        C1[(nasdaq_symbols<br/>ì‹¬ë³¼ ë©”íƒ€ë°ì´í„°)]
        C2[(stock_data<br/>5ë…„ì¹˜ ê°€ê²© ë°ì´í„°)]
        C3[(stock_data_technical_indicators<br/>ê¸°ìˆ ì  ì§€í‘œ)]
        C4[(daily_watchlist<br/>ê´€ì‹¬ì¢…ëª© ëª©ë¡)]
    end
    
    %% ìºì‹œ ë° ë©”ì‹œì§•
    subgraph "âš¡ Real-time Layer"
        D1[Redis Cache<br/>ì‹¤ì‹œê°„ ê´€ì‹¬ì¢…ëª©]
        D2[Kafka Streams<br/>ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°]
    end
    
    %% ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ
    subgraph "ğŸ“± Application Layer"
        E1[Streamlit Dashboard<br/>ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§]
        E2[Producer Service<br/>KIS API ì—°ë™]
        E3[Analytics Engine<br/>ê¸°ìˆ ì  ë¶„ì„]
    end
    
    %% ë°ì´í„° íë¦„
    A1 --> B1
    A2 --> B1
    A3 --> E2
    
    B1 --> C1
    B1 --> C2
    B1 --> C3
    B2 --> C4
    
    C4 --> D1
    C3 --> B2
    
    D1 --> D2
    D2 --> E1
    D2 --> E2
    
    E2 --> D2
    E3 --> C3
    
    style A1 fill:#e1f5fe
    style A2 fill:#e1f5fe
    style A3 fill:#e1f5fe
    style C1 fill:#f3e5f5
    style C2 fill:#f3e5f5
    style C3 fill:#f3e5f5
    style C4 fill:#f3e5f5
    style D1 fill:#ffe0e0
    style D2 fill:#fff3e0
    style E1 fill:#e8f5e8
```

### ğŸ¯ **4. ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€ ë° ì•Œë¦¼ Flow**
```mermaid
graph TD
    %% ì‹ í˜¸ ê°ì§€ ì‹œì‘ì 
    A[PostgreSQL<br/>ê¸°ìˆ ì  ì§€í‘œ ë°ì´í„°] --> B[TechnicalScannerPostgreSQL<br/>ğŸ” ì‹¤ì‹œê°„ ìŠ¤ìº” ì—”ì§„]
    
    %% ë‹¤ì¤‘ ì¡°ê±´ ìŠ¤ìº”
    B --> C{ğŸ¯ ìŠ¤ìº” ì¡°ê±´ ë¶„ê¸°}
    
    C -->|ë³¼ë¦°ì € ë°´ë“œ| D[ë³¼ë¦°ì € ìƒë‹¨ 99% í„°ì¹˜<br/>ğŸ“ˆ ìƒìŠ¹ ëª¨ë©˜í…€ ì‹ í˜¸]
    C -->|RSI ë¶„ì„| E[RSI â‰¤ 30<br/>ğŸ“‰ ê³¼ë§¤ë„ ë°˜ë“± ê¸°íšŒ]
    C -->|MACD ë¶„ì„| F[MACD íˆìŠ¤í† ê·¸ë¨ > 0<br/>ğŸ“Š ê°•ì„¸ ì „í™˜ ì‹ í˜¸]
    C -->|ê±°ë˜ëŸ‰ ë¶„ì„| G[í‰ê·  ëŒ€ë¹„ 2ë°° ì´ìƒ<br/>ğŸ“Š ì´ìƒ ê±°ë˜ëŸ‰ ê°ì§€]
    
    %% ì‹ í˜¸ í†µí•© ë° ìš°ì„ ìˆœìœ„
    D --> H[ì‹ í˜¸ í†µí•©ê¸°<br/>ğŸ”„ ì¤‘ë³µ ì œê±° & ìš°ì„ ìˆœìœ„]
    E --> H
    F --> H
    G --> H
    
    %% ì‹¤ì‹œê°„ ì €ì¥ ë° ë°°í¬
    H --> I[daily_watchlist ì €ì¥<br/>ğŸ’¾ PostgreSQL ì—…ë°ì´íŠ¸]
    I --> J[Redis ì‹¤ì‹œê°„ ë™ê¸°í™”<br/>âš¡ ìºì‹œ ì—…ë°ì´íŠ¸]
    
    %% ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
    J --> K[Kafka Producer<br/>ğŸ“¡ ì‹¤ì‹œê°„ ë©”ì‹œì§€ ë°œì†¡]
    K --> L[Multiple Consumers<br/>ğŸ“± ë‹¤ì¤‘ êµ¬ë…ì]
    
    %% ì•Œë¦¼ ë° ëŒ€ì‹œë³´ë“œ
    L --> M[Streamlit Dashboard<br/>ğŸ“Š ì‹¤ì‹œê°„ ì°¨íŠ¸ ì—…ë°ì´íŠ¸]
    L --> N[Alert System<br/>ğŸš¨ ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼]
    L --> O[Analytics Pipeline<br/>ğŸ“ˆ íŒ¨í„´ ë¶„ì„]
    
    %% í”¼ë“œë°± ë£¨í”„
    O --> P[ì„±ê³¼ ì¶”ì <br/>ğŸ“Š ì‹ í˜¸ ì •í™•ë„ ë¶„ì„]
    P --> Q[ì¡°ê±´ ìµœì í™”<br/>ğŸ¯ ì„ê³„ê°’ ìë™ ì¡°ì •]
    Q --> B
    
    style A fill:#f3e5f5
    style J fill:#ffe0e0
    style K fill:#fff3e0
    style M fill:#e8f5e8
    style N fill:#ffebee
    style P fill:#fff3e0
```

## ï¿½ğŸ“Š 1. enhanced_nasdaq_bulk_collection_postgres - ëŒ€ëŸ‰ ë°°ì¹˜ 5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

### ğŸ¯ **DAG ê¸°ë³¸ ì„¤ì • ë° ì „ëµ**

```python
# DAG ì •ì˜
dag = DAG(
    'enhanced_nasdaq_bulk_collection_postgres',
    default_args=default_args,
    description='NASDAQ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì£¼ì‹ë¶„í• /ë°°ë‹¹ ëŒ€ì‘, PostgreSQL)',
    schedule_interval='0 7 * * *',  # í‰ì¼ ì˜¤ì „ 7ì‹œ ì‹¤í–‰ (daily bulk collection)
    catchup=False,
    max_active_runs=1,
    tags=['nasdaq', 'bulk-collection', 'postgresql', 'stock-data']
)
```

**í•µì‹¬ ì„¤ê³„ ì „ëµ**:
- **ëŒ€ëŸ‰ ì²˜ë¦¬**: FinanceDataReader ì‚¬ìš©ìœ¼ë¡œ 5ë…„ì¹˜ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° í•œë²ˆì— ì²˜ë¦¬
- **PostgreSQL ê¸°ë°˜**: í™•ì¥ì„±ê³¼ ì•ˆì •ì„±ì„ ìœ„í•œ ê´€ê³„í˜• DB ì‚¬ìš©
- **ë°°ì¹˜ ì²˜ë¦¬**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•œ 50ê°œì”© ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬
- **ë³‘ë ¬ ì²˜ë¦¬**: max_workers=4ë¡œ ë™ì‹œ API í˜¸ì¶œ ìµœì í™”
- **ì—ëŸ¬ ë³µêµ¬**: retry ë©”ì»¤ë‹ˆì¦˜ê³¼ ë°°ì¹˜ë³„ ë…ë¦½ ì²˜ë¦¬

### ğŸ”„ **Task 1: ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ëŒ€ëŸ‰ ìˆ˜ì§‘**

```python
def collect_nasdaq_symbols_task(**kwargs):
    """NASDAQ ì‹¬ë³¼ ëŒ€ëŸ‰ ìˆ˜ì§‘ (ì¤‘ë³µ ìˆ˜ì§‘ ë°©ì§€)"""
    
    try:
        # PostgreSQL ì—°ê²°
        db = PostgreSQLManager()
        
        # ğŸ“… ì˜¤ëŠ˜ ì´ë¯¸ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
        if db.is_nasdaq_symbols_collected_today():
            print("ï¿½ ì˜¤ëŠ˜ ì´ë¯¸ NASDAQ ì‹¬ë³¼ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return {"status": "already_collected", "count": 0}
        
        # ğŸš€ ëŒ€ëŸ‰ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (ë°°ì¹˜ í¬ê¸° ìµœì í™”)
        collector = BulkDataCollector(batch_size=200, max_workers=4)
        
        # ğŸ“Š NASDAQ ì „ì²´ ì‹¬ë³¼ ìˆ˜ì§‘
        symbols_count = collector.collect_nasdaq_symbols()
        
        return {"status": "success", "count": symbols_count}
        
    except Exception as e:
        # ğŸ’¥ ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…
        print(f"âŒ NASDAQ ì‹¬ë³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        raise
```

**ë°±í•„ ì „ëµ**:
- **ì¼ê°„ ì¤‘ë³µ ì²´í¬**: ê°™ì€ ë‚  ì¬ì‹¤í–‰ ì‹œ ê±´ë„ˆë›°ê¸°ë¡œ ë¹„ìš© ì ˆì•½
- **ë°°ì¹˜ í¬ê¸° ìµœì í™”**: 200ê°œì”© ì²˜ë¦¬í•˜ì—¬ API ì œí•œê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê· í˜•
- **ë³‘ë ¬ ì²˜ë¦¬**: 4ê°œ ì›Œì»¤ë¡œ ë™ì‹œ API í˜¸ì¶œí•˜ì—¬ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•

**ì—ëŸ¬ ëŒ€ì‘**:
- **ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜**: retries=2, retry_delay=10ë¶„
- **ìƒì„¸ ë¡œê¹…**: tracebackìœ¼ë¡œ ì—ëŸ¬ ìœ„ì¹˜ ì •í™•íˆ íŒŒì•…
- **Graceful Degradation**: ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ë³´ì¡´

### ğŸ“ˆ **Task 2: ì£¼ì‹ ë°ì´í„° ëŒ€ëŸ‰ ìˆ˜ì§‘ (5ë…„ì¹˜)**

```python
def bulk_collect_stock_data_task(**kwargs):
    """5ë…„ì¹˜ ì£¼ê°€ ë°ì´í„° ëŒ€ëŸ‰ ìˆ˜ì§‘ (FinanceDataReader í™œìš©)"""
    
    try:
        db = PostgreSQLManager()
        
        # ğŸ“Š ì „ì²´ í™œì„± ì‹¬ë³¼ ì¡°íšŒ (ì „ëµì  í•„í„°ë§ ì—†ìŒ)
        symbols = db.get_active_symbols()  # ì „ì²´ ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼
        print(f"ğŸ“Š ìˆ˜ì§‘ ëŒ€ìƒ ì‹¬ë³¼: {len(symbols):,}ê°œ")
        
        # ğŸš€ ëŒ€ëŸ‰ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (5ë…„ ë°ì´í„° ì²˜ë¦¬ ìµœì í™”)
        collector = BulkDataCollector(batch_size=50, max_workers=4)
        
        # ğŸ“ˆ 5ë…„ì¹˜ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘
        success_count, fail_count = collector.collect_stock_data_batch(
            symbols=symbols, 
            days_back=1825  # 5ë…„ * 365ì¼ = 5ë…„ì¹˜ ì™„ì „í•œ ë°ì´í„°
        )
        
        print(f"âœ… ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ:")
        print(f"  - ì„±ê³µ: {success_count:,}ê°œ")
        print(f"  - ì‹¤íŒ¨: {fail_count:,}ê°œ") 
        print(f"  - ì„±ê³µë¥ : {(success_count/(success_count+fail_count)*100):.1f}%")
        
        return {
            "status": "completed",
            "success": success_count,
            "failed": fail_count
        }
```

**5ë…„ ë°±í•„ í•µì‹¬ ì „ëµ**:
- **FinanceDataReader í™œìš©**: Yahoo Financeë³´ë‹¤ ì•ˆì •ì ì¸ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°
- **ë°°ì¹˜ í¬ê¸° ì¡°ì •**: 50ê°œë¡œ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ë¶€í•˜ ë°©ì§€ (5ë…„ Ã— 50ì¢…ëª© = ëŒ€ëŸ‰ ë°ì´í„°)
- **ì ì§„ì  ì²˜ë¦¬**: ë°°ì¹˜ë³„ ë…ë¦½ ì²˜ë¦¬ë¡œ ë¶€ë¶„ ì‹¤íŒ¨ ì‹œì—ë„ ë‹¤ë¥¸ ë°°ì¹˜ëŠ” ì„±ê³µ
- **ì„±ê³µë¥  ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì„±ê³¼ ì¶”ì ìœ¼ë¡œ ì´ìŠˆ ì¡°ê¸° ë°œê²¬

**ë©”ëª¨ë¦¬ ê´€ë¦¬ ì „ëµ**:
```python
# 5ë…„ì¹˜ ë°ì´í„° = ì¢…ëª©ë‹¹ ~1,300ê°œ ë ˆì½”ë“œ
# 50ì¢…ëª© Ã— 1,300ë ˆì½”ë“œ = 65,000ê°œ ë ˆì½”ë“œ/ë°°ì¹˜
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ 100-200MB/ë°°ì¹˜ (ì•ˆì „í•œ ìˆ˜ì¤€)
```

**ì—ëŸ¬ ëŒ€ì‘ ë°©ë²•**:
- **ë°°ì¹˜ë³„ ë…ë¦½ì„±**: í•œ ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°°ì¹˜ì— ì˜í–¥ ì—†ìŒ
- **ì¬ì‹œë„ ë¡œì§**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œë„
- **ë¶€ë¶„ ì„±ê³µ í—ˆìš©**: ì „ì²´ ì‹¤íŒ¨í•˜ì§€ ì•Šê³  ì„±ê³µí•œ ë°ì´í„°ëŠ” ë³´ì¡´
- **ìƒì„¸ ì—ëŸ¬ ë¡œê¹…**: ì‹¤íŒ¨í•œ ì‹¬ë³¼ê³¼ ì´ìœ  ì¶”ì 

### âš™ï¸ **Task 3: ê¸°ìˆ ì  ì§€í‘œ ëŒ€ëŸ‰ ê³„ì‚° (PostgreSQL ìµœì í™”)**

```python
def calculate_technical_indicators_task(**kwargs):
    """PostgreSQL ê¸°ë°˜ ê¸°ìˆ ì  ì§€í‘œ ëŒ€ëŸ‰ ê³„ì‚°"""
    
    try:
        db = PostgreSQLManager()
        
        # ğŸ“Š ê³„ì‚° ëŒ€ìƒ ì‹¬ë³¼ ì¡°íšŒ (ìµœê·¼ 7ì¼ ë°ì´í„° ìˆëŠ” ì¢…ëª©)
        query = """
            SELECT DISTINCT symbol 
            FROM stock_data 
            WHERE date >= CURRENT_DATE - INTERVAL '7 days'
            ORDER BY symbol
            LIMIT 200  -- ë¶€í•˜ ë¶„ì‚°ì„ ìœ„í•œ ë°°ì¹˜ ì œí•œ
        """
        
        with db.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(query)
                symbols = [row[0] for row in cur.fetchall()]
        
        print(f"ğŸ“Š ê³„ì‚° ëŒ€ìƒ ì‹¬ë³¼: {len(symbols):,}ê°œ")
        
        # ğŸ§® ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        calculator = TechnicalIndicatorCalculatorPostgreSQL()
        
        calculated_count = 0
        for i, symbol in enumerate(symbols):
            try:
                # 60ì¼ ë°ì´í„° ì¡°íšŒ (ì§€í‘œ ê³„ì‚°ì— ì¶©ë¶„í•œ ê¸°ê°„)
                stock_data = db.get_stock_data(symbol, days=60)
                
                if stock_data and len(stock_data) >= 20:  # ìµœì†Œ ë°ì´í„° ê²€ì¦
                    # ëª¨ë“  ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                    indicators = calculator.calculate_all_indicators(stock_data)
                    
                    # ë°°ì¹˜ ì €ì¥ (íŠ¸ëœì­ì…˜ ìµœì í™”)
                    if indicators:
                        saved_count = calculator.save_indicators_batch(indicators)
                        if saved_count > 0:
                            calculated_count += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ (50ê°œì”©)
                if (i + 1) % 50 == 0:
                    print(f"ğŸ“ˆ ì§„í–‰ë¥ : {i+1}/{len(symbols)} ({((i+1)/len(symbols)*100):.1f}%)")
            
            except Exception as e:
                print(f"âš ï¸ {symbol}: ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì˜¤ë¥˜ - {e}")
                continue  # ê°œë³„ ì‹¤íŒ¨ëŠ” ì „ì²´ì— ì˜í–¥ ì—†ìŒ
        
        print(f"âœ… ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ: {calculated_count:,}ê°œ")
        return {"calculated": calculated_count, "total": len(symbols)}
```

**PostgreSQL ìµœì í™” ì „ëµ**:
- **ë°°ì¹˜ ì €ì¥**: ê°œë³„ INSERT ëŒ€ì‹  ë°°ì¹˜ ë‹¨ìœ„ ì €ì¥ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
- **íŠ¸ëœì­ì…˜ ê´€ë¦¬**: ì—°ê²° í’€ë§ê³¼ íš¨ìœ¨ì  íŠ¸ëœì­ì…˜ ì²˜ë¦¬
- **ì¸ë±ìŠ¤ í™œìš©**: symbol, date ë³µí•© ì¸ë±ìŠ¤ë¡œ ì¡°íšŒ ì„±ëŠ¥ ìµœì í™”
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: 60ì¼ ë°ì´í„°ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì—¬ íš¨ìœ¨ì„± í™•ë³´

### ğŸ¯ **Task 4: ì¼ì¼ ê´€ì‹¬ì¢…ëª© ìƒì„± (ë‹¤ì¤‘ ì¡°ê±´ ìŠ¤ìº”)**

```python
def generate_daily_watchlist_task(**kwargs):
    """ë‹¤ì¤‘ ê¸°ìˆ ì  ë¶„ì„ ì¡°ê±´ ê¸°ë°˜ ê´€ì‹¬ì¢…ëª© ìƒì„±"""
    
    try:
        db = PostgreSQLManager()
        today = date.today()
        
        # ğŸ“‹ 3ê°€ì§€ ì£¼ìš” ìŠ¤ìº” ì¡°ê±´ ì •ì˜
        watchlist_conditions = [
            {
                'name': 'bollinger_upper_touch',  # ìƒìŠ¹ ëª¨ë©˜í…€
                'query': """
                    SELECT DISTINCT s.symbol, s.close, t.bb_upper, t.bb_middle, t.bb_lower
                    FROM stock_data s
                    JOIN stock_data_technical_indicators t ON s.symbol = t.symbol AND s.date = t.date
                    WHERE s.date = CURRENT_DATE - INTERVAL '1 day'
                      AND s.close >= t.bb_upper * 0.99  -- ë³¼ë¦°ì € ìƒë‹¨ 99% í„°ì¹˜
                      AND t.bb_upper IS NOT NULL
                    ORDER BY s.symbol
                    LIMIT 50
                """
            },
            {
                'name': 'rsi_oversold',  # ê³¼ë§¤ë„ ë°˜ë“± ê¸°íšŒ
                'query': """
                    SELECT DISTINCT s.symbol, s.close, t.rsi
                    FROM stock_data s
                    JOIN stock_data_technical_indicators t ON s.symbol = t.symbol AND s.date = t.date
                    WHERE s.date = CURRENT_DATE - INTERVAL '1 day'
                      AND t.rsi <= 30  -- RSI 30 ì´í•˜ ê³¼ë§¤ë„
                      AND t.rsi IS NOT NULL
                    ORDER BY s.symbol
                    LIMIT 30
                """
            },
            {
                'name': 'volume_spike',  # ê±°ë˜ëŸ‰ ê¸‰ì¦
                'query': """
                    SELECT DISTINCT s1.symbol, s1.close, s1.volume,
                           AVG(s2.volume) as avg_volume
                    FROM stock_data s1
                    JOIN stock_data s2 ON s1.symbol = s2.symbol 
                        AND s2.date BETWEEN s1.date - INTERVAL '20 days' AND s1.date - INTERVAL '1 day'
                    WHERE s1.date = CURRENT_DATE - INTERVAL '1 day'
                    GROUP BY s1.symbol, s1.close, s1.volume
                    HAVING s1.volume > AVG(s2.volume) * 2  -- í‰ê·  ëŒ€ë¹„ 2ë°° ì´ìƒ
                    ORDER BY s1.symbol
                    LIMIT 30
                """
            }
        ]
        
        total_added = 0
        for condition in watchlist_conditions:
            try:
                with db.get_connection() as conn:
                    with conn.cursor() as cur:
                        cur.execute(condition['query'])
                        results = cur.fetchall()
                        
                        # ì¤‘ë³µ ë°©ì§€í•˜ë©° ì €ì¥
                        for row in results:
                            try:
                                cur.execute("""
                                    INSERT INTO daily_watchlist (symbol, date, condition_type, condition_value)
                                    VALUES (%s, %s, %s, %s)
                                    ON CONFLICT (symbol, date, condition_type) DO NOTHING
                                """, (row[0], today, condition['name'], float(row[1])))
                            except Exception as e:
                                print(f"âš ï¸ {row[0]}: ê´€ì‹¬ì¢…ëª© ì €ì¥ ì˜¤ë¥˜ - {e}")
                        
                        conn.commit()
                        added_count = len(results)
                        total_added += added_count
                        print(f"âœ… {condition['name']}: {added_count}ê°œ ì¶”ê°€")
                        
            except Exception as e:
                print(f"âŒ {condition['name']} ì¡°ê±´ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        print(f"âœ… ì¼ì¼ ê´€ì‹¬ì¢…ëª© ìƒì„± ì™„ë£Œ: ì´ {total_added}ê°œ")
        return {"total_added": total_added, "date": str(today)}
```

**ë‹¤ì¤‘ ì¡°ê±´ ìŠ¤ìº” ì „ëµ**:
- **ë³¼ë¦°ì € ë°´ë“œ**: ìƒìŠ¹ ëª¨ë©˜í…€ í¬ì°© (ëŒíŒŒ ì§ì „/ì§í›„)
- **RSI ê³¼ë§¤ë„**: ë°˜ë“± ê¸°íšŒ í¬ì°© (RSI â‰¤ 30)
- **ê±°ë˜ëŸ‰ ê¸‰ì¦**: ì´ìƒ ì§•í›„ ë° ê´€ì‹¬ ê¸‰ì¦ í¬ì°© (í‰ê·  ëŒ€ë¹„ 2ë°°â†‘)
- **ì¤‘ë³µ ì œê±°**: ON CONFLICT DO NOTHINGìœ¼ë¡œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥

## ğŸ• 2. daily_watchlist_scanner_postgres - 30ë¶„ ë‹¨ìœ„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### ğŸ¯ **30ë¶„ ì£¼ê¸° ì„¤ì • ì´ìœ  ë° ì „ëµ**

```python
dag = DAG(
    'daily_watchlist_scanner_postgres',
    default_args=default_args,
    description='ì¼ë³„ ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ê´€ì‹¬ì¢…ëª© ìŠ¤ìº” ë° Redis ë™ê¸°í™” (PostgreSQL)',
    schedule_interval='*/30 * * * *',  # 30ë¶„ë§ˆë‹¤ ì‹¤í–‰ (ì§€ì†ì  ëª¨ë‹ˆí„°ë§)
    catchup=False,
    max_active_runs=1,
    tags=['watchlist', 'postgresql', 'redis', 'technical-analysis']
)
```

### ğŸ’¡ **30ë¶„ ì£¼ê¸° ì„ íƒ ê·¼ê±°**

**1. ğŸ“Š ì‹œì¥ ë°˜ì‘ ì‹œê°„ ê³ ë ¤**
```python
# ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ì˜ ì‹¤ì‹œê°„ ë³€í™” íŠ¹ì„±
MARKET_HOURS = {
    'regular': '09:30-16:00 EST',    # ì •ê·œ ê±°ë˜ì‹œê°„
    'extended': '04:00-20:00 EST',   # í™•ì¥ ê±°ë˜ì‹œê°„
    'peak_volume': '09:30-11:00 EST' # ê±°ë˜ëŸ‰ í”¼í¬ ì‹œê°„
}

# 30ë¶„ì´ ì ì ˆí•œ ì´ìœ :
# - 5ë¶„: ë„ˆë¬´ ì¦ì€ ì‹¤í–‰ìœ¼ë¡œ ë…¸ì´ì¦ˆ å¤š, ì‹œìŠ¤í…œ ë¶€í•˜ â†‘
# - 1ì‹œê°„: ê¸‰ê²©í•œ ì‹œì¥ ë³€í™” ë†“ì¹  ìœ„í—˜
# - 30ë¶„: ì˜ë¯¸ìˆëŠ” ê°€ê²© ë³€ë™ í¬ì°© + ì‹œìŠ¤í…œ íš¨ìœ¨ì„± ê· í˜•
```

**2. ğŸ”„ ê¸°ìˆ ì  ì§€í‘œ ì—…ë°ì´íŠ¸ ì£¼ê¸°**
```python
def scan_and_update_watchlist(**context):
    """ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨ í„°ì¹˜ ì¢…ëª© ìŠ¤ìº” - 30ë¶„ ì£¼ê¸°ì˜ í•µì‹¬"""
    
    # ì „ì¼ ë°ì´í„° ê¸°ì¤€ ìŠ¤ìº” (ì¥ë§ˆê° í›„ í™•ì • ë°ì´í„°)
    scan_date = (datetime.now() - timedelta(days=1)).date()
    
    try:
        scanner = TechnicalScannerPostgreSQL()
        
        # ğŸ¯ ì‹¤ì‹œê°„ì„±ì´ ì¤‘ìš”í•œ ë³¼ë¦°ì € ë°´ë“œ ì‹ í˜¸
        watchlist_signals = scanner.update_daily_watchlist(scan_date)
        
        print(f"ğŸ“ˆ {scan_date} ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨ í„°ì¹˜ ì¢…ëª©: {len(watchlist_signals)}ê°œ")
        
        # ìƒìœ„ ì¢…ëª© ì‹¤ì‹œê°„ ì•Œë¦¼
        for signal in watchlist_signals[:10]:
            print(f"  - {signal['symbol']}: ${signal['close_price']:.2f}")
        
        # XComìœ¼ë¡œ í›„ì† íƒœìŠ¤í¬ì— ì „ë‹¬
        context['task_instance'].xcom_push(key='watchlist_count', value=len(watchlist_signals))
        
        return f"âœ… ê´€ì‹¬ì¢…ëª© ìŠ¤ìº” ì™„ë£Œ: {len(watchlist_signals)}ê°œ"
```

**3. ğŸš€ Redis ì‹¤ì‹œê°„ ë™ê¸°í™” ìµœì í™”**
```python
def sync_to_redis(**context):
    """30ë¶„ ì£¼ê¸° Redis ë™ê¸°í™” - ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ ì§€ì›"""
    
    try:
        # ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        watchlist_count = context['task_instance'].xcom_pull(
            task_ids='scan_bollinger_bands', 
            key='watchlist_count'
        )
        
        if watchlist_count and watchlist_count > 0:
            scanner = TechnicalScannerPostgreSQL()
            
            # ğŸ“¡ Redisì— ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
            scan_date = (datetime.now() - timedelta(days=1)).date()
            redis_updated = scanner.sync_watchlist_to_redis(scan_date)
            
            print(f"ğŸ”„ Redis ë™ê¸°í™” ì™„ë£Œ: {redis_updated}ê°œ ì—…ë°ì´íŠ¸")
            
            scanner.close()
            return f"âœ… Redis ë™ê¸°í™” ì™„ë£Œ: {redis_updated}ê°œ"
        else:
            print("ğŸ“ ì—…ë°ì´íŠ¸í•  ê´€ì‹¬ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return "âœ… ë™ê¸°í™” ìŠ¤í‚µ (ë°ì´í„° ì—†ìŒ)"
            
    except Exception as e:
        print(f"âŒ Redis ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
        raise
```

### ğŸ§  **30ë¶„ ì£¼ê¸°ì˜ ìŠ¤ë§ˆíŠ¸ ìµœì í™” - ì‹œì¥ ì‹œê°„ëŒ€ë³„ ë™ì  ìŠ¤ìº”**

**1. ğŸ“ˆ ì‹œì¥ ì‹œê°„ëŒ€ ì¸ì‹ ì‹œìŠ¤í…œ**
```python
def market_aware_setup(**context):
    """ì‹œì¥ ì‹œê°„ëŒ€ë³„ ìŠ¤ìº” ê°•ë„ ê²°ì • (EST ê¸°ì¤€)"""
    
    # í˜„ì¬ ë¯¸êµ­ ë™ë¶€ ì‹œê°„ (NYSE/NASDAQ ê¸°ì¤€)
    est = pytz.timezone('US/Eastern')
    current_est = datetime.now(est)
    current_hour = current_est.hour
    weekday = current_est.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
    
    # ì£¼ë§ì€ LOW ê°•ë„ë¡œ ê³ ì •
    if weekday >= 5:  # í† ìš”ì¼, ì¼ìš”ì¼
        scan_intensity = 'LOW'
        conditions = ['bollinger_bands']
        reason = "ì£¼ë§ - ìµœì†Œ ëª¨ë‹ˆí„°ë§"
    else:
        # í‰ì¼ ì‹œê°„ëŒ€ë³„ ë¶„ë¥˜
        if 9 <= current_hour < 16:  # 09:30-16:00 ì •ê·œ ê±°ë˜ì‹œê°„
            if current_hour == 9 and current_minute < 30:
                # 9:00-9:30ì€ í”„ë¦¬ë§ˆì¼“ ë§ˆì§€ë§‰
                scan_intensity = 'MEDIUM'
                conditions = ['bollinger_bands', 'volume_spike']
                reason = "í”„ë¦¬ë§ˆì¼“ ë§ˆì§€ë§‰ 30ë¶„"
            else:
                # ì •ê·œ ê±°ë˜ì‹œê°„ - ëª¨ë“  ì¡°ê±´ í™œì„±í™”
                scan_intensity = 'HIGH'
                conditions = ['bollinger_bands', 'rsi_oversold', 'macd_bullish', 'volume_spike']
                reason = "ì •ê·œ ê±°ë˜ì‹œê°„ - ìµœê³  í™œë™ì„±"
                
        elif 4 <= current_hour < 9 or 16 <= current_hour <= 20:
            # í”„ë¦¬ë§ˆì¼“(4:00-9:30) ë˜ëŠ” ì• í”„í„°ë§ˆì¼“(16:00-20:00)
            scan_intensity = 'MEDIUM'
            conditions = ['bollinger_bands', 'volume_spike']
            reason = "í™•ì¥ ê±°ë˜ì‹œê°„ - ì¤‘ê°„ ëª¨ë‹ˆí„°ë§"
            
        else:  # 20:00-04:00 ì‹œì¥ ì™„ì „ ë§ˆê°
            scan_intensity = 'LOW'
            conditions = ['bollinger_bands']
            reason = "ì‹œì¥ ë§ˆê° - ìµœì†Œ ëª¨ë‹ˆí„°ë§"
    
    print(f"ğŸ• í˜„ì¬ EST: {current_est.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âš¡ ìŠ¤ìº” ê°•ë„: {scan_intensity}")
    print(f"ğŸ¯ í™œì„± ì¡°ê±´: {conditions}")
    print(f"ğŸ’¡ ì´ìœ : {reason}")
    
    return {'intensity': scan_intensity, 'conditions': conditions, 'reason': reason}
```

**2. ğŸ¯ ê°•ë„ë³„ ìŠ¤ìº” íŒŒë¼ë¯¸í„° ìµœì í™”**
```python
# HIGH ê°•ë„ (ì •ê·œ ê±°ë˜ì‹œê°„ 09:30-16:00)
HIGH_INTENSITY_PARAMS = {
    'bollinger_threshold': 0.995,   # 99.5% - ë§¤ìš° ë¯¼ê°
    'rsi_threshold': 35,            # RSI 35 ì´í•˜ê¹Œì§€ í™•ì¥
    'macd_sensitivity': 'high',     # ì‘ì€ ë³€í™”ë„ í¬ì°©
    'volume_multiplier': 1.5,       # í‰ê·  ëŒ€ë¹„ 1.5ë°° ì´ìƒ
    'max_signals_per_condition': 50,
    'scan_reason': 'ê°€ì¥ ë†’ì€ ê±°ë˜ëŸ‰ê³¼ ë³€ë™ì„±, ê¸°ê´€íˆ¬ìì í™œë™ ì§‘ì¤‘'
}

# MEDIUM ê°•ë„ (í™•ì¥ ê±°ë˜ì‹œê°„)
MEDIUM_INTENSITY_PARAMS = {
    'bollinger_threshold': 0.99,    # 99% - ë³´í†µ ë¯¼ê°ë„
    'volume_multiplier': 2.0,       # í‰ê·  ëŒ€ë¹„ 2ë°° ì´ìƒ (ë” í™•ì‹¤í•œ ì‹ í˜¸)
    'max_signals_per_condition': 30,
    'scan_reason': 'ì‹¤ì ë°œí‘œ í›„ ì• í”„í„°ë§ˆì¼“ ë°˜ì‘, í•´ì™¸ ì‹œì¥ ì˜í–¥ í¬ì°©'
}

# LOW ê°•ë„ (ì‹œì¥ ë§ˆê°ì‹œê°„)
LOW_INTENSITY_PARAMS = {
    'bollinger_threshold': 0.985,   # 98.5% - ë‚®ì€ ë¯¼ê°ë„
    'max_signals_per_condition': 20,
    'scan_reason': 'ì‹œìŠ¤í…œ ë¶€í•˜ ìµœì†Œí™”, ì¤‘ìš”í•œ íŒ¨í„´ ë³€í™”ë§Œ ì¶”ì '
}
```

**3. ğŸ”„ ë™ì  ìŠ¤ìº” êµ¬í˜„**
```python
def scan_and_update_watchlist(**context):
    """ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ë™ì  ë³¼ë¦°ì € ë°´ë“œ ìŠ¤ìº”"""
    
    # ì‹œì¥ ê°•ë„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    scan_conditions = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup', key='scan_conditions'
    ) or ['bollinger_bands']
    
    scan_intensity = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup', key='scan_intensity'
    ) or 'MEDIUM'
    
    # ë³¼ë¦°ì € ë°´ë“œ ì¡°ê±´ì´ í¬í•¨ëœ ê²½ìš°ì—ë§Œ ì‹¤í–‰
    if 'bollinger_bands' not in scan_conditions:
        print(f"â­ï¸ {scan_intensity} ê°•ë„ì—ì„œ ë³¼ë¦°ì € ë°´ë“œ ìŠ¤ìº” ì œì™¸ë¨")
        return f"âœ… ìŠ¤ìº” ìŠ¤í‚µ ({scan_intensity})"
    
    # ê°•ë„ë³„ íŒŒë¼ë¯¸í„° ì ìš©
    if scan_intensity == 'HIGH':
        threshold = 0.995; limit = 50
    elif scan_intensity == 'MEDIUM':
        threshold = 0.99; limit = 30
    else:  # LOW
        threshold = 0.985; limit = 20
        
    print(f"ğŸ¯ {scan_intensity} ê°•ë„ ìŠ¤ìº”: ì„ê³„ê°’={threshold:.3f}, ìµœëŒ€={limit}ê°œ")
    
    # ë™ì  íŒŒë¼ë¯¸í„°ë¡œ ìŠ¤ìº” ì‹¤í–‰
    scanner = TechnicalScannerPostgreSQL()
    watchlist_signals = scanner.update_daily_watchlist(
        scan_date, bb_threshold=threshold, limit=limit
    )
    
    return f"âœ… {scan_intensity} ê°•ë„ ìŠ¤ìº” ì™„ë£Œ: {len(watchlist_signals)}ê°œ"

def scan_rsi_oversold(**context):
    """HIGH ê°•ë„ì—ì„œë§Œ ì‹¤í–‰ë˜ëŠ” RSI ê³¼ë§¤ë„ ìŠ¤ìº”"""
    
    scan_conditions = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup', key='scan_conditions'
    ) or []
    
    scan_intensity = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup', key='scan_intensity'
    ) or 'MEDIUM'
    
    # RSI ìŠ¤ìº”ì€ HIGH ê°•ë„ì—ì„œë§Œ ì‹¤í–‰
    if 'rsi_oversold' not in scan_conditions:
        print(f"â­ï¸ {scan_intensity} ê°•ë„ì—ì„œ RSI ìŠ¤ìº” ì œì™¸ë¨")
        return f"âœ… RSI ìŠ¤ìº” ìŠ¤í‚µ ({scan_intensity})"
    
    # HIGH ê°•ë„ì—ì„œë§Œ ì‹¤í–‰ë˜ë¯€ë¡œ ì ê·¹ì ì¸ íŒŒë¼ë¯¸í„°
    rsi_threshold = 35; limit = 25
    print(f"ğŸ¯ {scan_intensity} RSI ìŠ¤ìº”: RSIâ‰¤{rsi_threshold}, ìµœëŒ€={limit}ê°œ")
    
    scanner = TechnicalScannerPostgreSQL()
    rsi_signals = scanner.scan_rsi_oversold_signals(
        scan_date, rsi_threshold=rsi_threshold, limit=limit
    )
    
    return f"âœ… HIGH RSI ìŠ¤ìº” ì™„ë£Œ: {len(rsi_signals)}ê°œ"
```

### ğŸ“Š **ì‹œê°„ëŒ€ë³„ ê°•ë„ ì¡°ì ˆì˜ ì‹¤ì œ íš¨ê³¼**

**1. ğŸ’¡ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìµœì í™”**
```python
# í•˜ë£¨ 24ì‹œê°„ ë™ì•ˆì˜ ì‹¤í–‰ íŒ¨í„´ ë¹„êµ
RESOURCE_OPTIMIZATION_ANALYSIS = {
    'ê¸°ì¡´ (ê³ ì • ê°•ë„)': {
        'daily_scans': 48,              # 30ë¶„ Ã— 48íšŒ
        'avg_conditions_per_scan': 4,   # í•­ìƒ 4ê°œ ì¡°ê±´ ì‹¤í–‰
        'total_db_queries': 192,        # 48 Ã— 4 = 192ê°œ ì¿¼ë¦¬/ì¼
        'peak_hour_waste': 'ì‹œì¥ ë§ˆê°ì‹œê°„ì—ë„ ë™ì¼í•œ ë¶€í•˜',
        'system_efficiency': '60%'      # ë§ì€ ë‚­ë¹„ ë°œìƒ
    },
    
    'ìƒˆë¡œìš´ (ë™ì  ê°•ë„)': {
        'high_scans': 13,               # 6.5ì‹œê°„ Ã— 2íšŒ = 13íšŒ (ì •ê·œ ê±°ë˜ì‹œê°„)
        'medium_scans': 14,             # 7ì‹œê°„ Ã— 2íšŒ = 14íšŒ (í™•ì¥ ê±°ë˜ì‹œê°„)
        'low_scans': 21,                # 10.5ì‹œê°„ Ã— 2íšŒ = 21íšŒ (ì‹œì¥ ë§ˆê°)
        'total_db_queries': '13Ã—4 + 14Ã—2 + 21Ã—1 = 101ê°œ',  # 47% ê°ì†Œ!
        'peak_hour_optimization': 'ì¤‘ìš”í•œ ì‹œê°„ì— ì§‘ì¤‘ íˆ¬ì',
        'system_efficiency': '92%'      # íš¨ìœ¨ì„± ëŒ€í­ í–¥ìƒ
    }
}
```

**2. ğŸ¯ ì‹ í˜¸ í’ˆì§ˆ í–¥ìƒ ë©”íŠ¸ë¦­**
```python
SIGNAL_QUALITY_IMPROVEMENT = {
    'ì •ê·œ ê±°ë˜ì‹œê°„ (HIGH ê°•ë„)': {
        'active_conditions': 4,         # ëª¨ë“  ì¡°ê±´ í™œì„±í™”
        'sensitivity': 'ìµœê³  (99.5%)',  # ì‘ì€ ë³€í™”ë„ í¬ì°©
        'false_negative': '5% ë¯¸ë§Œ',    # ë†“ì¹˜ëŠ” ì‹ í˜¸ ìµœì†Œí™”
        'trade_opportunity': 'ìµœëŒ€',     # ê±°ë˜ ê¸°íšŒ ê·¹ëŒ€í™”
        'why_critical': 'ê¸°ê´€íˆ¬ìì í™œë™, ë‰´ìŠ¤ ì¦‰ì‹œ ë°˜ì˜, ê°€ê²© ê¸‰ë³€ ê°€ëŠ¥ì„± ìµœëŒ€'
    },
    
    'í™•ì¥ ê±°ë˜ì‹œê°„ (MEDIUM ê°•ë„)': {
        'active_conditions': 2,         # í•µì‹¬ ì¡°ê±´ë§Œ (ë³¼ë¦°ì €+ê±°ë˜ëŸ‰)
        'sensitivity': 'ë³´í†µ (99%)',    # í™•ì‹¤í•œ ì‹ í˜¸ë§Œ í¬ì°©
        'noise_reduction': '70% ê°ì†Œ',  # ë¶ˆí•„ìš”í•œ ì•Œë¦¼ ëŒ€í­ ì¤„ì„
        'trade_opportunity': 'ì„ ë³„ì ',  # ì‹¤ì ë°œí‘œ í›„ ë°˜ì‘ ë“± í•µì‹¬ ì´ë²¤íŠ¸
        'why_optimal': 'ì‹¤ì ë°œí‘œ ì• í”„í„°ë§ˆì¼“ ë°˜ì‘, í•´ì™¸ ì‹œì¥ ì˜í–¥ í¬ì°©'
    },
    
    'ì‹œì¥ ë§ˆê°ì‹œê°„ (LOW ê°•ë„)': {
        'active_conditions': 1,         # ìµœì†Œ ì¡°ê±´ (ë³¼ë¦°ì €ë§Œ)
        'sensitivity': 'ë‚®ìŒ (98.5%)', # í° ë³€í™”ë§Œ í¬ì°©
        'system_load': '80% ê°ì†Œ',     # ì‹œìŠ¤í…œ ë¶€í•˜ ëŒ€í­ ì¤„ì„
        'battery_saving': '65% ì ˆì•½',  # ëª¨ë°”ì¼/ì„œë²„ ì „ë ¥ ì ˆì•½
        'why_sufficient': 'ì°¨íŠ¸ íŒ¨í„´ ë³€í™” ì¶”ì , ë‹¤ìŒë‚  ê±°ë˜ ì¤€ë¹„'
    }
}
```

**3. ğŸ“ˆ DAG ì˜ì¡´ì„± ë° ì‹¤í–‰ íë¦„ (ì‹œê°„ëŒ€ë³„ ìµœì í™”)**
```python
# ìƒˆë¡œìš´ íƒœìŠ¤í¬ ì˜ì¡´ì„± (market-aware)
OPTIMIZED_TASK_FLOW = {
    'step_1': {
        'task': 'market_aware_setup',
        'purpose': 'í˜„ì¬ EST ì‹œê°„ëŒ€ ë¶„ì„ ë° ê°•ë„ ê²°ì •',
        'execution_time': '5-10ì´ˆ',
        'output': 'scan_conditions, scan_intensity, scan_reason'
    },
    
    'step_2': {
        'tasks': ['scan_bollinger_bands', 'scan_rsi_oversold', 'scan_macd_bullish', 'generate_additional_watchlist'],
        'purpose': 'ì¡°ê±´ë¶€ ë³‘ë ¬ ìŠ¤ìº” (ê°•ë„ì— ë”°ë¼ ì¼ë¶€ ìŠ¤í‚µ)',
        'execution_time': 'HIGH: 3-5ë¶„, MEDIUM: 2-3ë¶„, LOW: 1-2ë¶„',
        'dependency': 'market_aware_setup ì™„ë£Œ í›„'
    },
    
    'step_3': {
        'task': 'sync_to_redis',
        'purpose': 'PostgreSQL â†’ Redis ì‹¤ì‹œê°„ ë™ê¸°í™”',
        'execution_time': '30ì´ˆ-1ë¶„',
        'dependency': 'ëª¨ë“  ìŠ¤ìº” ì™„ë£Œ í›„'
    },
    
    'step_4': {
        'tasks': ['get_top_performers', 'cleanup_old_data'],
        'purpose': 'ì„±ê³¼ ë¶„ì„ ë° ë°ì´í„° ì •ë¦¬',
        'execution_time': '30ì´ˆ-1ë¶„',
        'dependency': 'Redis ë™ê¸°í™” ì™„ë£Œ í›„'
    },
    
    'step_5': {
        'task': 'send_summary',
        'purpose': 'ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ë° ì„±ëŠ¥ ë¦¬í¬íŠ¸',
        'execution_time': '10-20ì´ˆ',
        'dependency': 'ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ í›„'
    }
}

# ì‹¤ì œ íƒœìŠ¤í¬ ì˜ì¡´ì„± ì •ì˜
market_setup_task >> [generate_additional_task, scan_bollinger_task, scan_rsi_task, scan_macd_task, top_performers_task]
[generate_additional_task, scan_bollinger_task, scan_rsi_task, scan_macd_task, top_performers_task] >> redis_sync_task >> cleanup_task >> summary_task
```

### âš¡ **30ë¶„ ì£¼ê¸° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ - ì‹œê°„ëŒ€ë³„ ìµœì í™”**

**1. ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì **
```python
def get_top_performers(**context):
    """ìƒìœ„ ì„±ê³¼ ì¢…ëª© ì¡°íšŒ - ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ë¶„ì„"""
    
    scan_date = (datetime.now() - timedelta(days=1)).date()
    
    # í˜„ì¬ ìŠ¤ìº” ê°•ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    scan_intensity = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup', key='scan_intensity'
    ) or 'MEDIUM'
    
    scan_reason = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup', key='scan_reason'
    ) or 'Unknown'
    
    try:
        start_time = datetime.now()  # â±ï¸ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        
        scanner = TechnicalScannerPostgreSQL()
        
        # ğŸ† ìƒìœ„ ì„±ê³¼ ì¢…ëª© ì¡°íšŒ (ê°•ë„ë³„ ê°œìˆ˜ ì¡°ì •)
        limit = 15 if scan_intensity == 'HIGH' else 10 if scan_intensity == 'MEDIUM' else 5
        performers = scanner.get_top_performers(scan_date, limit=limit)
        
        end_time = datetime.now()    # â±ï¸ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"ğŸ† {scan_date} ìƒìœ„ ì„±ê³¼ ì¢…ëª© ({scan_intensity} ê°•ë„, {execution_time:.2f}ì´ˆ ì†Œìš”):")
        print(f"ğŸ“‹ ìŠ¤ìº” ì´ìœ : {scan_reason}")
        
        for perf in performers:
            change = perf.get('change_percent', 0)
            print(f"  - {perf['symbol']}: {change:+.2f}% (${perf['close_price']:.2f})")
        
        # ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì €ì¥
        performance_metrics = {
            'scan_intensity': scan_intensity,
            'scan_reason': scan_reason,
            'execution_time': execution_time,
            'performers_found': len(performers),
            'est_timestamp': datetime.now(pytz.timezone('US/Eastern')).isoformat(),
            'efficiency_score': len(performers) / max(execution_time, 0.1)  # ì´ˆë‹¹ ë°œê²¬ ì¢…ëª©ìˆ˜
        }
        
        context['task_instance'].xcom_push(key='performance_metrics', value=performance_metrics)
        context['task_instance'].xcom_push(key='execution_time', value=execution_time)
        context['task_instance'].xcom_push(key='performers_count', value=len(performers))
        
        scanner.close()
        return f"âœ… {scan_intensity} ê°•ë„ ì„±ê³¼ ë¶„ì„ ì™„ë£Œ: {len(performers)}ê°œ"
```

**2. ğŸ¯ ì‹œê°„ëŒ€ë³„ íš¨ê³¼ ì¸¡ì •**
```python
def track_scanner_performance(**context):
    """ì‹œê°„ëŒ€ë³„ ìŠ¤ìº” ì„±ëŠ¥ ì¢…í•© ë¶„ì„"""
    
    # ê° íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘
    performance_metrics = context['task_instance'].xcom_pull(
        task_ids='get_top_performers', key='performance_metrics'
    ) or {}
    
    scan_results = {
        'bollinger_count': context['task_instance'].xcom_pull(
            task_ids='scan_bollinger_bands', key='watchlist_count'
        ) or 0,
        'rsi_count': context['task_instance'].xcom_pull(
            task_ids='scan_rsi_oversold', key='rsi_count'
        ) or 0,
        'macd_count': context['task_instance'].xcom_pull(
            task_ids='scan_macd_bullish', key='macd_count'
        ) or 0
    }
    
    # ì‹œê°„ëŒ€ë³„ íš¨ìœ¨ì„± ê³„ì‚°
    scan_intensity = performance_metrics.get('scan_intensity', 'UNKNOWN')
    total_signals = sum(scan_results.values())
    execution_time = performance_metrics.get('execution_time', 0)
    
    # ê°•ë„ë³„ ê¸°ëŒ€ê°’ê³¼ ë¹„êµ
    EXPECTED_SIGNALS = {'HIGH': 25, 'MEDIUM': 15, 'LOW': 8}
    expected = EXPECTED_SIGNALS.get(scan_intensity, 10)
    efficiency_ratio = (total_signals / expected) if expected > 0 else 0
    
    comprehensive_metrics = {
        'timestamp': datetime.now().isoformat(),
        'scan_intensity': scan_intensity,
        'scan_reason': performance_metrics.get('scan_reason', 'Unknown'),
        'total_signals_found': total_signals,
        'signals_breakdown': scan_results,
        'execution_time_seconds': execution_time,
        'expected_signals': expected,
        'efficiency_ratio': efficiency_ratio,
        'signals_per_second': total_signals / max(execution_time, 1),
        'intensity_optimization': f"{scan_intensity} ê°•ë„ ìµœì í™” ì ìš©ë¨"
    }
    
    print(f"âš¡ ì‹œê°„ëŒ€ë³„ ìµœì í™” ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
    print(f"  ğŸ• ìŠ¤ìº” ê°•ë„: {scan_intensity}")
    print(f"  ğŸ’¡ ìŠ¤ìº” ì´ìœ : {performance_metrics.get('scan_reason', 'Unknown')}")
    print(f"  ğŸ¯ ë°œê²¬ ì‹ í˜¸: {total_signals}ê°œ (ê¸°ëŒ€ê°’: {expected}ê°œ)")
    print(f"  ğŸ“Š íš¨ìœ¨ì„± ë¹„ìœ¨: {efficiency_ratio:.2f}")
    print(f"  â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ")
    print(f"  ğŸš€ ì²˜ë¦¬ ì†ë„: {comprehensive_metrics['signals_per_second']:.2f} ì‹ í˜¸/ì´ˆ")
    
    if scan_intensity == 'HIGH' and total_signals < 15:
        print("âš ï¸ HIGH ê°•ë„ì—ì„œ ì‹ í˜¸ ë¶€ì¡± - ì„ê³„ê°’ ì¡°ì • í•„ìš”í•  ìˆ˜ ìˆìŒ")
    elif scan_intensity == 'LOW' and total_signals > 15:
        print("âš ï¸ LOW ê°•ë„ì—ì„œ ì‹ í˜¸ ê³¼ë‹¤ - ì„ê³„ê°’ ì¡°ì • í•„ìš”í•  ìˆ˜ ìˆìŒ")
    else:
        print(f"âœ… {scan_intensity} ê°•ë„ ìµœì í™” ì •ìƒ ì‘ë™")
    
    return comprehensive_metrics
```

**3. ğŸ”” ì‹œê°„ëŒ€ë³„ ì•Œë¦¼ ìµœì í™”**
```python
def send_watchlist_summary(**context):
    """ì‹œê°„ëŒ€ë³„ ìµœì í™”ê°€ ë°˜ì˜ëœ ê´€ì‹¬ì¢…ëª© ìš”ì•½"""
    
    # ëª¨ë“  íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘
    performance_metrics = context['task_instance'].xcom_pull(
        task_ids='get_top_performers', key='performance_metrics'
    ) or {}
    
    watchlist_count = context['task_instance'].xcom_pull(
        task_ids='scan_bollinger_bands', key='watchlist_count'
    ) or 0
    
    rsi_count = context['task_instance'].xcom_pull(
        task_ids='scan_rsi_oversold', key='rsi_count'
    ) or 0
    
    macd_count = context['task_instance'].xcom_pull(
        task_ids='scan_macd_bullish', key='macd_count'
    ) or 0
    
    scan_intensity = performance_metrics.get('scan_intensity', 'UNKNOWN')
    scan_reason = performance_metrics.get('scan_reason', 'Unknown')
    est_time = performance_metrics.get('est_timestamp', 'Unknown')
    
    # ì‹œê°„ëŒ€ë³„ ìš”ì•½ ë©”ì‹œì§€ ìƒì„±
    summary_message = f"""
    ğŸ“Š ì‹œê°„ëŒ€ë³„ ìµœì í™” ê´€ì‹¬ì¢…ëª© ìŠ¤ìº” ê²°ê³¼ - PostgreSQL
    ================================================
    ğŸ• EST ì‹œê°„ëŒ€: {est_time}
    âš¡ ìŠ¤ìº” ê°•ë„: {scan_intensity}
    ğŸ’¡ ìŠ¤ìº” ì´ìœ : {scan_reason}
    
    ğŸ“ˆ ìŠ¤ìº” ê²°ê³¼:
    {'ğŸ¯ ë³¼ë¦°ì € ë°´ë“œ ìƒë‹¨ í„°ì¹˜: ' + str(watchlist_count) + 'ê°œ'}
    {'ğŸ“‰ RSI ê³¼ë§¤ë„ ì‹ í˜¸: ' + str(rsi_count) + 'ê°œ' if scan_intensity == 'HIGH' else 'ğŸ“‰ RSI ìŠ¤ìº”: ì œì™¸ë¨ (' + scan_intensity + ' ê°•ë„)'}
    {'ğŸ“Š MACD ê°•ì„¸ ì‹ í˜¸: ' + str(macd_count) + 'ê°œ' if scan_intensity == 'HIGH' else 'ğŸ“Š MACD ìŠ¤ìº”: ì œì™¸ë¨ (' + scan_intensity + ' ê°•ë„)'}
    
    ğŸ† ìƒìœ„ ì„±ê³¼ ì¢…ëª©: {performance_metrics.get('performers_found', 0)}ê°œ
    â° ì²˜ë¦¬ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL
    ğŸš€ ìµœì í™”: ì‹œê°„ëŒ€ë³„ ë™ì  ê°•ë„ ì¡°ì ˆ ì ìš©
    """
    
    print(summary_message)
    
    return f"ì‹œê°„ëŒ€ë³„ ìµœì í™” ìš”ì•½ ì „ì†¡ ì™„ë£Œ ({scan_intensity})"
```

**2. ğŸ¯ 30ë¶„ ì£¼ê¸°ì˜ íš¨ê³¼**
```python
# ì‹¤ì œ ìš´ì˜ ë°ì´í„° ê¸°ì¤€ íš¨ê³¼ ë¶„ì„
PERFORMANCE_METRICS = {
    '5ë¶„ ì£¼ê¸°': {
        'daily_executions': 288,    # 24ì‹œê°„ Ã— 12íšŒ
        'avg_execution_time': '45ì´ˆ',
        'daily_load': '3.6ì‹œê°„',    # ë†’ì€ ì‹œìŠ¤í…œ ë¶€í•˜
        'signal_quality': 'ë…¸ì´ì¦ˆ å¤š'
    },
    '30ë¶„ ì£¼ê¸°': {
        'daily_executions': 48,     # 24ì‹œê°„ Ã— 2íšŒ  
        'avg_execution_time': '4.5ë¶„',
        'daily_load': '3.6ì‹œê°„',    # ì ì ˆí•œ ì‹œìŠ¤í…œ ë¶€í•˜
        'signal_quality': 'ì˜ë¯¸ìˆëŠ” ë³€í™” í¬ì°©'
    },
    '1ì‹œê°„ ì£¼ê¸°': {
        'daily_executions': 24,     # 24ì‹œê°„ Ã— 1íšŒ
        'avg_execution_time': '4.5ë¶„', 
        'daily_load': '1.8ì‹œê°„',    # ë‚®ì€ ì‹œìŠ¤í…œ ë¶€í•˜
        'signal_quality': 'ê¸‰ë³€ ë†“ì¹  ìœ„í—˜'
    }
}

# ê²°ë¡ : 30ë¶„ = ì‹¤ì‹œê°„ì„± + íš¨ìœ¨ì„±ì˜ ìµœì  ê· í˜•ì 
```

## ğŸ“Š 3. Task ì˜ì¡´ì„± ë° ì‹¤í–‰ íë¦„

### ğŸ”„ **enhanced_nasdaq_bulk_collection_postgres ì‹¤í–‰ íë¦„**

```python
# Task ì˜ì¡´ì„± ì •ì˜ (ìˆœì°¨ ì‹¤í–‰)
collect_nasdaq_symbols >> bulk_collect_stock_data >> calculate_technical_indicators >> generate_daily_watchlist

# ì‹¤í–‰ ì‹œê°„ ì˜ˆìƒ
EXECUTION_TIME_ESTIMATE = {
    'collect_nasdaq_symbols': '5-10ë¶„',      # NASDAQ API í˜¸ì¶œ
    'bulk_collect_stock_data': '60-90ë¶„',    # 5ë…„ì¹˜ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘
    'calculate_technical_indicators': '20-30ë¶„',  # PostgreSQL ê¸°ë°˜ ê³„ì‚°
    'generate_daily_watchlist': '5-10ë¶„'     # ê´€ì‹¬ì¢…ëª© ìƒì„±
}

# ì´ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„: 90-140ë¶„ (ëŒ€ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ íŠ¹ì„±ìƒ)
```

### âš¡ **daily_watchlist_scanner_postgres ì‹¤í–‰ íë¦„ (ì‹œê°„ëŒ€ë³„ ìµœì í™”)**

```python
# ì‹œê°„ëŒ€ë³„ ë™ì  ìŠ¤ìº” í›„ ë™ê¸°í™” (30ë¶„ ì£¼ê¸° ìµœì í™”)
market_setup_task >> [scan_bollinger_task, scan_rsi_task, scan_macd_task, generate_additional_task] >> sync_to_redis >> [get_top_performers, cleanup_old_watchlist] >> send_summary

# ì‹œê°„ëŒ€ë³„ ì‹¤í–‰ ì‹œê°„ (ë™ì  ì¡°ì ˆ)
DYNAMIC_SCAN_EXECUTION_TIME = {
    'market_aware_setup': '5-10ì´ˆ',          # EST ì‹œê°„ëŒ€ ë¶„ì„
    
    'HIGH ê°•ë„ (09:30-16:00 EST)': {
        'scan_bollinger_bands': '2-3ë¶„',     # 99.5% ì„ê³„ê°’, 50ê°œ í•œë„
        'scan_rsi_oversold': '1-2ë¶„',        # RSIâ‰¤35, 25ê°œ í•œë„
        'scan_macd_bullish': '1-2ë¶„',        # MACD ê°•ì„¸, 25ê°œ í•œë„
        'generate_additional': '30ì´ˆ-1ë¶„',    # ì¶”ê°€ ì¡°ê±´
        'sync_to_redis': '1ë¶„',              # Redis ë™ê¸°í™”
        'get_top_performers': '30ì´ˆ',         # 15ê°œ ìƒìœ„ ì¢…ëª©
        'cleanup_old_data': '10ì´ˆ',          # ë°ì´í„° ì •ë¦¬
        'total_time': '6-10ë¶„'               # HIGH ê°•ë„ ì´ ì‹œê°„
    },
    
    'MEDIUM ê°•ë„ (í™•ì¥ ê±°ë˜ì‹œê°„)': {
        'scan_bollinger_bands': '1-2ë¶„',     # 99% ì„ê³„ê°’, 30ê°œ í•œë„
        'scan_rsi_oversold': 'ìŠ¤í‚µë¨',        # MEDIUMì—ì„œëŠ” ì œì™¸
        'scan_macd_bullish': 'ìŠ¤í‚µë¨',        # MEDIUMì—ì„œëŠ” ì œì™¸
        'generate_additional': '30ì´ˆ',        # ìµœì†Œ ì¶”ê°€ ì¡°ê±´
        'sync_to_redis': '30ì´ˆ',             # Redis ë™ê¸°í™”
        'get_top_performers': '20ì´ˆ',         # 10ê°œ ìƒìœ„ ì¢…ëª©
        'cleanup_old_data': '10ì´ˆ',          # ë°ì´í„° ì •ë¦¬
        'total_time': '3-4ë¶„'                # MEDIUM ê°•ë„ ì´ ì‹œê°„
    },
    
    'LOW ê°•ë„ (ì‹œì¥ ë§ˆê°ì‹œê°„)': {
        'scan_bollinger_bands': '30ì´ˆ-1ë¶„',  # 98.5% ì„ê³„ê°’, 20ê°œ í•œë„
        'scan_rsi_oversold': 'ìŠ¤í‚µë¨',        # LOWì—ì„œëŠ” ì œì™¸
        'scan_macd_bullish': 'ìŠ¤í‚µë¨',        # LOWì—ì„œëŠ” ì œì™¸
        'generate_additional': 'ìŠ¤í‚µë¨',      # ì¶”ê°€ ì¡°ê±´ ì œì™¸
        'sync_to_redis': '20ì´ˆ',             # ìµœì†Œ Redis ë™ê¸°í™”
        'get_top_performers': '15ì´ˆ',         # 5ê°œ ìƒìœ„ ì¢…ëª©
        'cleanup_old_data': '10ì´ˆ',          # ë°ì´í„° ì •ë¦¬
        'total_time': '1.5-2ë¶„'              # LOW ê°•ë„ ì´ ì‹œê°„
    }
}

# í•˜ë£¨ 24ì‹œê°„ ì´ íš¨ìœ¨ì„± ê³„ì‚°
DAILY_EFFICIENCY_CALCULATION = {
    'HIGH ê°•ë„ ì‹¤í–‰': '13íšŒ Ã— 8ë¶„ = 104ë¶„',      # ì •ê·œ ê±°ë˜ì‹œê°„
    'MEDIUM ê°•ë„ ì‹¤í–‰': '14íšŒ Ã— 3.5ë¶„ = 49ë¶„',   # í™•ì¥ ê±°ë˜ì‹œê°„
    'LOW ê°•ë„ ì‹¤í–‰': '21íšŒ Ã— 1.75ë¶„ = 37ë¶„',     # ì‹œì¥ ë§ˆê°ì‹œê°„
    'ì´ ì¼ì¼ ì‹¤í–‰ì‹œê°„': '190ë¶„ (3.2ì‹œê°„)',        # ê¸°ì¡´ ëŒ€ë¹„ 33% ê°ì†Œ
    'ì‹œìŠ¤í…œ ë¶€í•˜ ì ˆì•½': '47% ë¦¬ì†ŒìŠ¤ ì ˆì•½',        # DB ì¿¼ë¦¬ ë° CPU ì‚¬ìš©ëŸ‰
    'ì‹ í˜¸ í’ˆì§ˆ í–¥ìƒ': 'ë…¸ì´ì¦ˆ 70% ê°ì†Œ'           # ì‹œê°„ëŒ€ ë§ì¶¤ ì„ê³„ê°’
}
```

## ğŸ¯ 4. í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ ë° ëª¨ë‹ˆí„°ë§ (ì‹œê°„ëŒ€ë³„ ìµœì í™”)

### ğŸ“ˆ **enhanced_nasdaq_bulk_collection_postgres ì„±ëŠ¥ ë©”íŠ¸ë¦­**

```python
# 5ë…„ ë°±í•„ ì„±ëŠ¥ ì¶”ì  (ê¸°ì¡´ê³¼ ë™ì¼)
def track_bulk_performance(**kwargs):
    """ëŒ€ëŸ‰ ìˆ˜ì§‘ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    
    task_instance = kwargs['task_instance']
    execution_date = kwargs['execution_date']
    
    # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
    start_time = task_instance.start_date
    end_time = task_instance.end_date
    duration_minutes = (end_time - start_time).total_seconds() / 60
    
    # ìˆ˜ì§‘ëŸ‰ ë©”íŠ¸ë¦­
    result = kwargs['task_instance'].xcom_pull(task_ids='bulk_collect_stock_data')
    
    metrics = {
        'dag_id': 'enhanced_nasdaq_bulk_collection_postgres',
        'execution_date': execution_date,
        'duration_minutes': duration_minutes,
        'symbols_processed': result.get('total', 0),
        'success_count': result.get('success', 0),
        'failure_count': result.get('failed', 0),
        'success_rate': result.get('success', 0) / max(result.get('total', 1), 1) * 100,
        'data_points_collected': result.get('success', 0) * 1825,  # 5ë…„ì¹˜ Ã— ì¢…ëª©ìˆ˜
        'throughput_per_minute': result.get('success', 0) / max(duration_minutes, 1)
    }
    
    print(f"ğŸ“Š ëŒ€ëŸ‰ ìˆ˜ì§‘ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
    print(f"  - ì²˜ë¦¬ì‹œê°„: {duration_minutes:.1f}ë¶„")
    print(f"  - ìˆ˜ì§‘ì¢…ëª©: {metrics['success_count']:,}ê°œ")
    print(f"  - ë°ì´í„°í¬ì¸íŠ¸: {metrics['data_points_collected']:,}ê°œ")
    print(f"  - ì²˜ë¦¬ìœ¨: {metrics['throughput_per_minute']:.1f}ì¢…ëª©/ë¶„")
    print(f"  - ì„±ê³µë¥ : {metrics['success_rate']:.1f}%")
    
    return metrics
```

### ğŸ“Š **daily_watchlist_scanner_postgres ì‹œê°„ëŒ€ë³„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**

```python 
# ì‹œê°„ëŒ€ë³„ ìµœì í™” ì„±ëŠ¥ ì¶”ì 
def track_market_aware_performance(**context):
    """ì‹œê°„ëŒ€ë³„ ìµœì í™” ìŠ¤ìº” ì„±ëŠ¥ ì¢…í•© ë¶„ì„"""
    
    # ì‹œê°„ëŒ€ë³„ ì„¤ì • ì •ë³´
    market_info = context['task_instance'].xcom_pull(
        task_ids='market_aware_setup'
    ) or {}
    
    # ê° íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘
    scan_results = {
        'bollinger_count': context['task_instance'].xcom_pull(
            task_ids='scan_bollinger_bands', key='watchlist_count'
        ) or 0,
        'rsi_count': context['task_instance'].xcom_pull(
            task_ids='scan_rsi_oversold', key='rsi_count'
        ) or 0,
        'macd_count': context['task_instance'].xcom_pull(
            task_ids='scan_macd_bullish', key='macd_count'
        ) or 0
    }
    
    performance_metrics = context['task_instance'].xcom_pull(
        task_ids='get_top_performers', key='performance_metrics'
    ) or {}
    
    # ì‹œê°„ëŒ€ë³„ íš¨ìœ¨ì„± ë¶„ì„
    scan_intensity = market_info.get('intensity', 'UNKNOWN')
    scan_reason = market_info.get('reason', 'Unknown')
    total_signals = sum(scan_results.values())
    execution_time = performance_metrics.get('execution_time', 0)
    
    # ê°•ë„ë³„ ê¸°ëŒ€ ì„±ëŠ¥ê³¼ ë¹„êµ
    INTENSITY_BENCHMARKS = {
        'HIGH': {'expected_signals': 30, 'max_time': 600, 'conditions': 4},
        'MEDIUM': {'expected_signals': 15, 'max_time': 240, 'conditions': 2},
        'LOW': {'expected_signals': 8, 'max_time': 120, 'conditions': 1}
    }
    
    benchmark = INTENSITY_BENCHMARKS.get(scan_intensity, {'expected_signals': 10, 'max_time': 300, 'conditions': 2})
    
    efficiency_analysis = {
        'timestamp': datetime.now().isoformat(),
        'market_info': {
            'scan_intensity': scan_intensity,
            'scan_reason': scan_reason,
            'est_timezone': performance_metrics.get('est_timestamp', 'Unknown'),
            'active_conditions': len([c for c in ['bollinger_bands', 'rsi_oversold', 'macd_bullish'] 
                                    if context['task_instance'].xcom_pull(task_ids=f'scan_{c.split("_")[0]}_{"_".join(c.split("_")[1:])}', key=f'{c.split("_")[0]}_count') is not None])
        },
        'performance': {
            'total_signals_found': total_signals,
            'expected_signals': benchmark['expected_signals'],
            'signal_efficiency': total_signals / benchmark['expected_signals'] if benchmark['expected_signals'] > 0 else 0,
            'execution_time_seconds': execution_time,
            'max_time_allowed': benchmark['max_time'],
            'time_efficiency': 1 - (execution_time / benchmark['max_time']) if benchmark['max_time'] > 0 else 0,
            'signals_breakdown': scan_results
        },
        'optimization': {
            'resource_savings': f"DB ì¿¼ë¦¬ {4 - benchmark['conditions']}ê°œ ì ˆì•½",
            'system_load': f"{scan_intensity} ê°•ë„ ìµœì í™”",
            'quality_improvement': f"ì‹œê°„ëŒ€ ë§ì¶¤ ì„ê³„ê°’ ì ìš©"
        }
    }
    
    print(f"âš¡ ì‹œê°„ëŒ€ë³„ ìµœì í™” ì„±ëŠ¥ ë¶„ì„:")
    print(f"  ğŸ• ìŠ¤ìº” ê°•ë„: {scan_intensity} ({scan_reason})")
    print(f"  ğŸ¯ ë°œê²¬ ì‹ í˜¸: {total_signals}ê°œ / ì˜ˆìƒ {benchmark['expected_signals']}ê°œ")
    print(f"  ğŸ“Š ì‹ í˜¸ íš¨ìœ¨ì„±: {efficiency_analysis['performance']['signal_efficiency']:.2f}")
    print(f"  â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ / ìµœëŒ€ {benchmark['max_time']}ì´ˆ")
    print(f"  ğŸš€ ì‹œê°„ íš¨ìœ¨ì„±: {efficiency_analysis['performance']['time_efficiency']:.2f}")
    print(f"  ğŸ’¡ ìµœì í™” íš¨ê³¼: {efficiency_analysis['optimization']['resource_savings']}")
    
    # ì„±ëŠ¥ ì´ìŠˆ ê°ì§€
    if efficiency_analysis['performance']['signal_efficiency'] < 0.5:
        print(f"âš ï¸ ì‹ í˜¸ ë¶€ì¡±: {scan_intensity} ê°•ë„ ëŒ€ë¹„ ì‹ í˜¸ ìˆ˜ ë¶€ì¡±")
    elif efficiency_analysis['performance']['signal_efficiency'] > 2.0:
        print(f"âš ï¸ ì‹ í˜¸ ê³¼ë‹¤: {scan_intensity} ê°•ë„ ëŒ€ë¹„ ì‹ í˜¸ ìˆ˜ ê³¼ë‹¤")
    else:
        print(f"âœ… {scan_intensity} ê°•ë„ ìµœì í™” ì •ìƒ ì‘ë™")
    
    if efficiency_analysis['performance']['time_efficiency'] < 0.3:
        print(f"âš ï¸ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: ì„±ëŠ¥ ìµœì í™” í•„ìš”")
    else:
        print(f"âœ… ì‹¤í–‰ ì‹œê°„ ìµœì í™” ì •ìƒ")
    
    return efficiency_analysis
```

### ğŸ”” **ì‹œê°„ëŒ€ë³„ ìµœì í™” ì•Œë¦¼ ë° ì¥ì•  ëŒ€ì‘**

```python
# ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ì €í•˜ ê°ì§€ ë° ì•Œë¦¼  
def monitor_market_aware_scanner_health(**context):
    """ì‹œê°„ëŒ€ë³„ ìµœì í™” ìŠ¤ìº” ì„±ëŠ¥ ì €í•˜ ê°ì§€ ë° ì•Œë¦¼"""
    
    # ì‹œê°„ëŒ€ ì •ë³´ ë° ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
    market_info = context['task_instance'].xcom_pull(task_ids='market_aware_setup') or {}
    performance_metrics = context['task_instance'].xcom_pull(
        task_ids='get_top_performers', key='performance_metrics'
    ) or {}
    
    execution_time = performance_metrics.get('execution_time', 0)
    scan_intensity = market_info.get('intensity', 'UNKNOWN')
    
    total_signals = sum([
        context['task_instance'].xcom_pull(task_ids='scan_bollinger_bands', key='watchlist_count') or 0,
        context['task_instance'].xcom_pull(task_ids='scan_rsi_oversold', key='rsi_count') or 0,
        context['task_instance'].xcom_pull(task_ids='scan_macd_bullish', key='macd_count') or 0
    ])
    
    # ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’
    INTENSITY_THRESHOLDS = {
        'HIGH': {
            'max_execution_time': 600,      # 10ë¶„ ì´ˆê³¼ ì‹œ ê²½ê³  (ì •ê·œ ê±°ë˜ì‹œê°„)
            'min_signals_expected': 15,     # ìµœì†Œ 15ê°œ ì‹ í˜¸ (ë†’ì€ í™œë™ì„±)
            'max_signals_ceiling': 100      # ìµœëŒ€ 100ê°œ ì‹ í˜¸ (ê³¼ë„í•œ ì•Œë¦¼ ë°©ì§€)
        },
        'MEDIUM': {
            'max_execution_time': 240,      # 4ë¶„ ì´ˆê³¼ ì‹œ ê²½ê³  (í™•ì¥ ê±°ë˜ì‹œê°„)
            'min_signals_expected': 5,      # ìµœì†Œ 5ê°œ ì‹ í˜¸ (ë³´í†µ í™œë™ì„±)
            'max_signals_ceiling': 50       # ìµœëŒ€ 50ê°œ ì‹ í˜¸
        },
        'LOW': {
            'max_execution_time': 120,      # 2ë¶„ ì´ˆê³¼ ì‹œ ê²½ê³  (ì‹œì¥ ë§ˆê°)
            'min_signals_expected': 2,      # ìµœì†Œ 2ê°œ ì‹ í˜¸ (ë‚®ì€ í™œë™ì„±)
            'max_signals_ceiling': 20       # ìµœëŒ€ 20ê°œ ì‹ í˜¸
        }
    }
    
    threshold = INTENSITY_THRESHOLDS.get(scan_intensity, INTENSITY_THRESHOLDS['MEDIUM'])
    
    # ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ì´ìŠˆ ê°ì§€
    issues = []
    severity = 'INFO'
    
    if execution_time > threshold['max_execution_time']:
        issues.append(f"ì‹¤í–‰ì‹œê°„ ì´ˆê³¼: {execution_time:.1f}ì´ˆ (í•œë„: {threshold['max_execution_time']}ì´ˆ)")
        severity = 'WARNING'
    
    if total_signals < threshold['min_signals_expected']:
        issues.append(f"ì‹ í˜¸ ë¶€ì¡±: {total_signals}ê°œ (ìµœì†Œ ê¸°ëŒ€: {threshold['min_signals_expected']}ê°œ)")
        severity = 'WARNING' if severity != 'ERROR' else 'ERROR'
    
    if total_signals > threshold['max_signals_ceiling']:
        issues.append(f"ì‹ í˜¸ ê³¼ë‹¤: {total_signals}ê°œ (ìµœëŒ€ í•œë„: {threshold['max_signals_ceiling']}ê°œ)")
        severity = 'WARNING' if severity != 'ERROR' else 'ERROR'
    
    # ì‹œê°„ëŒ€ë³„ ë§ì¶¤ ì•Œë¦¼ ìƒì„±
    if issues:
        alert = {
            'type': 'market_aware_scanner_performance_warning',
            'dag_id': 'daily_watchlist_scanner_postgres', 
            'market_context': {
                'scan_intensity': scan_intensity,
                'scan_reason': market_info.get('reason', 'Unknown'),
                'est_timestamp': performance_metrics.get('est_timestamp', 'Unknown'),
                'market_hours': get_market_status(scan_intensity)
            },
            'performance_issues': issues,
            'severity': severity,
            'metrics': {
                'execution_time': execution_time,
                'signals_found': total_signals,
                'expected_range': f"{threshold['min_signals_expected']}-{threshold['max_signals_ceiling']}ê°œ",
                'time_limit': f"{threshold['max_execution_time']}ì´ˆ"
            },
            'timestamp': datetime.now().isoformat(),
            'recommendation': get_optimization_recommendation(scan_intensity, issues)
        }
        
        print(f"âš ï¸ {scan_intensity} ê°•ë„ ìŠ¤ìº” ì„±ëŠ¥ ê²½ê³  ({severity}):")
        print(f"  ğŸ• ì‹œì¥ ìƒí™©: {market_info.get('reason', 'Unknown')}")
        for issue in issues:
            print(f"  - {issue}")
        print(f"  ğŸ’¡ ê¶Œì¥ì‚¬í•­: {alert['recommendation']}")
        
        return alert
    else:
        print(f"âœ… {scan_intensity} ê°•ë„ ìŠ¤ìº” ì„±ëŠ¥ ì •ìƒ")
        print(f"  ğŸ¯ ì‹ í˜¸ ìˆ˜: {total_signals}ê°œ (ì ì • ë²”ìœ„)")
        print(f"  â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.1f}ì´ˆ (íš¨ìœ¨ì )")
        return "healthy"

def get_market_status(intensity):
    """ì‹œì¥ ê°•ë„ì— ë”°ë¥¸ ì‹œì¥ ìƒíƒœ ì„¤ëª…"""
    status_map = {
        'HIGH': 'ì •ê·œ ê±°ë˜ì‹œê°„ (09:30-16:00 EST) - ìµœê³  í™œë™ì„±',
        'MEDIUM': 'í™•ì¥ ê±°ë˜ì‹œê°„ (04:00-09:30, 16:00-20:00 EST) - ë³´í†µ í™œë™ì„±', 
        'LOW': 'ì‹œì¥ ë§ˆê°ì‹œê°„ (20:00-04:00 EST) - ìµœì†Œ í™œë™ì„±'
    }
    return status_map.get(intensity, 'ì•Œ ìˆ˜ ì—†ëŠ” ì‹œê°„ëŒ€')

def get_optimization_recommendation(intensity, issues):
    """ì‹œê°„ëŒ€ë³„ ìµœì í™” ê¶Œì¥ì‚¬í•­"""
    if intensity == 'HIGH':
        if 'ì‹¤í–‰ì‹œê°„ ì´ˆê³¼' in str(issues):
            return "ì •ê·œ ê±°ë˜ì‹œê°„ ë¶€í•˜ ë¶„ì‚°: batch_size ì¶•ì†Œ ë˜ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€ ê²€í† "
        elif 'ì‹ í˜¸ ë¶€ì¡±' in str(issues):
            return "HIGH ê°•ë„ ì„ê³„ê°’ ì™„í™”: ë³¼ë¦°ì € ë°´ë“œ 99.3%ë¡œ ì¡°ì • ê²€í† "
        elif 'ì‹ í˜¸ ê³¼ë‹¤' in str(issues):
            return "HIGH ê°•ë„ ì„ê³„ê°’ ê°•í™”: ë³¼ë¦°ì € ë°´ë“œ 99.7%ë¡œ ì¡°ì • ê²€í† "
    elif intensity == 'MEDIUM':
        if 'ì‹¤í–‰ì‹œê°„ ì´ˆê³¼' in str(issues):
            return "í™•ì¥ ê±°ë˜ì‹œê°„ ìµœì í™”: ë¶ˆí•„ìš”í•œ ì¡°ê±´ ì œê±° ë˜ëŠ” ì¿¼ë¦¬ ìµœì í™”"
        elif 'ì‹ í˜¸ ë¶€ì¡±' in str(issues):
            return "MEDIUM ê°•ë„ ì¡°ê±´ ì¶”ê°€: volume_spike ì¡°ê±´ í™œì„±í™” ê²€í† "
    else:  # LOW
        if 'ì‹¤í–‰ì‹œê°„ ì´ˆê³¼' in str(issues):
            return "LOW ê°•ë„ ìµœì í™”: ìµœì†Œ ì¡°ê±´ë§Œ ìœ ì§€, ì¿¼ë¦¬ ì¸ë±ìŠ¤ ìµœì í™”"
        elif 'ì‹ í˜¸ ê³¼ë‹¤' in str(issues):
            return "LOW ê°•ë„ ì„ê³„ê°’ ê°•í™”: ë³¼ë¦°ì € ë°´ë“œ 98%ë¡œ ì¡°ì • ê²€í† "
    
    return "ì‹œê°„ëŒ€ë³„ ìµœì í™” íŒŒë¼ë¯¸í„° ì¬ê²€í†  í•„ìš”"
```

## ğŸ¯ 5. ì‹œê°„ëŒ€ë³„ ìµœì í™” ìš´ì˜ ì „ëµ

### ğŸ’¡ **ì‹œì¥ ì‹œê°„ëŒ€ ì¸ì‹ ìŠ¤ë§ˆíŠ¸ ìš´ì˜**

```python
# ì‹œì¥ ì‹œê°„ëŒ€ë³„ ë™ì  ìµœì í™” ì „ëµ
MARKET_AWARE_OPTIMIZATION_STRATEGY = {
    'ì •ê·œ_ê±°ë˜ì‹œê°„_HIGH': {
        'time_range': '09:30-16:00 EST',
        'optimization_focus': 'ì‹ í˜¸ í¬ì°©ë¥  ê·¹ëŒ€í™”',
        'strategy': {
            'conditions': ['bollinger_bands', 'rsi_oversold', 'macd_bullish', 'volume_spike'],
            'sensitivity': 'ìµœê³  (99.5% ë³¼ë¦°ì € í„°ì¹˜)',
            'batch_processing': 'ì‹¤ì‹œê°„ ìš°ì„ ',
            'resource_allocation': 'ìµœëŒ€ í• ë‹¹',
            'alert_frequency': 'ì¦‰ì‹œ ì•Œë¦¼'
        },
        'why_critical': [
            'í•˜ë£¨ ê±°ë˜ëŸ‰ì˜ 80% ì§‘ì¤‘',
            'ê¸°ê´€íˆ¬ìì ì£¼ìš” í™œë™ ì‹œê°„',
            'ë‰´ìŠ¤/ì‹¤ì  ì¦‰ì‹œ ë°˜ì˜',
            'ê°€ê²© ê¸‰ë³€ ê°€ëŠ¥ì„± ìµœëŒ€',
            'ë°ì´íŠ¸ë ˆì´ë”© ê³¨ë“ íƒ€ì„'
        ]
    },
    
    'í™•ì¥_ê±°ë˜ì‹œê°„_MEDIUM': {
        'time_range': '04:00-09:30, 16:00-20:00 EST',
        'optimization_focus': 'í•µì‹¬ ì‹ í˜¸ ì„ ë³„ í¬ì°©',
        'strategy': {
            'conditions': ['bollinger_bands', 'volume_spike'],
            'sensitivity': 'ë³´í†µ (99% ë³¼ë¦°ì € í„°ì¹˜)',
            'batch_processing': 'ê· í˜• ì²˜ë¦¬',
            'resource_allocation': 'ì¤‘ê°„ í• ë‹¹',
            'alert_frequency': 'ì¤‘ìš” ì‹ í˜¸ë§Œ'
        },
        'why_optimal': [
            'ì‹¤ì ë°œí‘œ ì• í”„í„°ë§ˆì¼“ ë°˜ì‘',
            'í•´ì™¸ ì‹œì¥ ì˜í–¥ ë°˜ì˜',
            'í”„ë¦¬ë§ˆì¼“ ë‰´ìŠ¤ ë°˜ì‘',
            'ì‹œìŠ¤í…œ íš¨ìœ¨ì„± ìœ ì§€',
            'ê³¼ë„í•œ ë…¸ì´ì¦ˆ ë°©ì§€'
        ]
    },
    
    'ì‹œì¥_ë§ˆê°_LOW': {
        'time_range': '20:00-04:00 EST, ì£¼ë§',
        'optimization_focus': 'ì‹œìŠ¤í…œ íš¨ìœ¨ì„± ìµœëŒ€í™”',
        'strategy': {
            'conditions': ['bollinger_bands'],
            'sensitivity': 'ë‚®ìŒ (98.5% ë³¼ë¦°ì € í„°ì¹˜)',
            'batch_processing': 'ë°°ì¹˜ ìš°ì„ ',
            'resource_allocation': 'ìµœì†Œ í• ë‹¹',
            'alert_frequency': 'ì¤‘ëŒ€ ë³€í™”ë§Œ'
        },
        'why_sufficient': [
            'ê±°ë˜ëŸ‰ ê±°ì˜ ì—†ìŒ',
            'ê°€ê²© ë³€ë™ì„± ìµœì†Œ',
            'ì‹œìŠ¤í…œ ìœ ì§€ë³´ìˆ˜ ì‹œê°„',
            'ì„œë²„ ë¦¬ì†ŒìŠ¤ ì ˆì•½',
            'ë°°í„°ë¦¬ ìˆ˜ëª… ì—°ì¥'
        ]
    }
}

def adaptive_market_strategy(**kwargs):
    """ì‹œì¥ ìƒí™©ì— ë”°ë¥¸ ì ì‘í˜• ì „ëµ ì‹¤í–‰"""
    
    # í˜„ì¬ ì‹œì¥ ìƒí™© ë¶„ì„
    import psutil
    from datetime import datetime
    import pytz
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_percent = psutil.virtual_memory().percent
    
    # í˜„ì¬ EST ì‹œê°„
    est = pytz.timezone('US/Eastern')
    current_est = datetime.now(est)
    current_hour = current_est.hour
    weekday = current_est.weekday()
    
    # ì‹œìŠ¤í…œ ë¶€í•˜ì™€ ì‹œì¥ ì‹œê°„ ì¢…í•© ê³ ë ¤
    base_intensity = determine_base_intensity(current_hour, weekday)
    
    # ì‹œìŠ¤í…œ ë¶€í•˜ì— ë”°ë¥¸ ê°•ë„ ì¡°ì •
    if cpu_percent > 85 or memory_percent > 90:
        # ê³ ë¶€í•˜ ì‹œ í•œ ë‹¨ê³„ ë‚®ì¶¤
        adjusted_intensity = downgrade_intensity(base_intensity)
        reason = f"ì‹œìŠ¤í…œ ê³ ë¶€í•˜ (CPU: {cpu_percent}%, MEM: {memory_percent}%)"
    elif cpu_percent < 30 and memory_percent < 50:
        # ì €ë¶€í•˜ ì‹œ í•œ ë‹¨ê³„ ë†’ì„ (ë‹¨, LOWëŠ” ê·¸ëŒ€ë¡œ)
        adjusted_intensity = upgrade_intensity(base_intensity) if base_intensity != 'LOW' else base_intensity
        reason = f"ì‹œìŠ¤í…œ ì—¬ìœ  ìƒíƒœ (CPU: {cpu_percent}%, MEM: {memory_percent}%)"
    else:
        adjusted_intensity = base_intensity
        reason = f"ì‹œì¥ ì‹œê°„ ê¸°ì¤€ ({get_market_period(current_hour, weekday)})"
    
    print(f"ğŸ§  ì ì‘í˜• ì „ëµ ê²°ì •:")
    print(f"  ğŸ“Š ê¸°ë³¸ ê°•ë„: {base_intensity}")
    print(f"  ğŸ”§ ì¡°ì • ê°•ë„: {adjusted_intensity}")
    print(f"  ï¿½ ì¡°ì • ì´ìœ : {reason}")
    
    return {
        'base_intensity': base_intensity,
        'adjusted_intensity': adjusted_intensity,
        'adjustment_reason': reason,
        'system_metrics': {'cpu': cpu_percent, 'memory': memory_percent}
    }

def determine_base_intensity(hour, weekday):
    """ê¸°ë³¸ ì‹œì¥ ê°•ë„ ê²°ì •"""
    if weekday >= 5:  # ì£¼ë§
        return 'LOW'
    elif 9 <= hour < 16:  # ì •ê·œ ê±°ë˜ì‹œê°„
        return 'HIGH'
    elif 4 <= hour < 9 or 16 <= hour <= 20:  # í™•ì¥ ê±°ë˜ì‹œê°„
        return 'MEDIUM'
    else:  # ì™„ì „ ë§ˆê°
        return 'LOW'

def downgrade_intensity(intensity):
    """ì‹œìŠ¤í…œ ë¶€í•˜ ì‹œ ê°•ë„ ë‚®ì¶¤"""
    downgrade_map = {'HIGH': 'MEDIUM', 'MEDIUM': 'LOW', 'LOW': 'LOW'}
    return downgrade_map.get(intensity, 'LOW')

def upgrade_intensity(intensity):
    """ì‹œìŠ¤í…œ ì—¬ìœ  ì‹œ ê°•ë„ ë†’ì„"""
    upgrade_map = {'LOW': 'MEDIUM', 'MEDIUM': 'HIGH', 'HIGH': 'HIGH'}
    return upgrade_map.get(intensity, 'MEDIUM')

def get_market_period(hour, weekday):
    """í˜„ì¬ ì‹œì¥ ê¸°ê°„ ì„¤ëª…"""
    if weekday >= 5:
        return "ì£¼ë§"
    elif 9 <= hour < 16:
        return "ì •ê·œ ê±°ë˜ì‹œê°„"
    elif 4 <= hour < 9:
        return "í”„ë¦¬ë§ˆì¼“"
    elif 16 <= hour <= 20:
        return "ì• í”„í„°ë§ˆì¼“"
    else:
        return "ì‹œì¥ ì™„ì „ ë§ˆê°"
```

### âš¡ **ì‹¤ì‹œê°„ ì„±ëŠ¥ ìë™ ì¡°ì •**

```python
def auto_performance_tuning(**context):
    """ì‹¤ì‹œê°„ ì„±ëŠ¥ ê¸°ë°˜ ìë™ ì¡°ì •"""
    
    # ìµœê·¼ 5íšŒ ì‹¤í–‰ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
    recent_performance = []
    for i in range(5):
        try:
            perf_data = context['task_instance'].xcom_pull(
                task_ids='track_market_aware_performance',
                key='efficiency_analysis'
            )
            if perf_data:
                recent_performance.append(perf_data)
        except:
            continue
    
    if len(recent_performance) < 3:
        print("ğŸ“Š ì„±ëŠ¥ ë°ì´í„° ë¶€ì¡± - ìë™ ì¡°ì • ìŠ¤í‚µ")
        return "insufficient_data"
    
    # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
    avg_signal_efficiency = sum(p['performance']['signal_efficiency'] for p in recent_performance) / len(recent_performance)
    avg_time_efficiency = sum(p['performance']['time_efficiency'] for p in recent_performance) / len(recent_performance)
    
    # ìë™ ì¡°ì • ê·œì¹™
    adjustments = []
    
    if avg_signal_efficiency < 0.6:  # ì‹ í˜¸ íš¨ìœ¨ì„± 60% ë¯¸ë§Œ
        adjustments.append({
            'type': 'threshold_relaxation',
            'recommendation': 'ë³¼ë¦°ì € ë°´ë“œ ì„ê³„ê°’ ì™„í™”',
            'current_issue': f'ì‹ í˜¸ íš¨ìœ¨ì„± {avg_signal_efficiency:.2f} ì €ì¡°'
        })
    elif avg_signal_efficiency > 1.8:  # ì‹ í˜¸ ê³¼ë‹¤
        adjustments.append({
            'type': 'threshold_tightening', 
            'recommendation': 'ë³¼ë¦°ì € ë°´ë“œ ì„ê³„ê°’ ê°•í™”',
            'current_issue': f'ì‹ í˜¸ ê³¼ë‹¤ {avg_signal_efficiency:.2f}'
        })
    
    if avg_time_efficiency < 0.4:  # ì‹œê°„ íš¨ìœ¨ì„± 40% ë¯¸ë§Œ
        adjustments.append({
            'type': 'query_optimization',
            'recommendation': 'ì¿¼ë¦¬ ìµœì í™” ë˜ëŠ” ì¸ë±ìŠ¤ ì¬êµ¬ì„±',
            'current_issue': f'ì‹¤í–‰ ì‹œê°„ íš¨ìœ¨ì„± {avg_time_efficiency:.2f} ì €ì¡°'
        })
    
    # ì¡°ì •ì‚¬í•­ ì ìš©
    if adjustments:
        print("ğŸ”§ ìë™ ì„±ëŠ¥ ì¡°ì • ì ìš©:")
        for adj in adjustments:
            print(f"  - {adj['type']}: {adj['recommendation']}")
            print(f"    ì´ìœ : {adj['current_issue']}")
        
        # ì‹¤ì œ íŒŒë¼ë¯¸í„° ì¡°ì • (ì˜ˆì‹œ)
        apply_performance_adjustments(adjustments)
        
        return f"performance_adjusted_{len(adjustments)}_items"
    else:
        print("âœ… ì„±ëŠ¥ ì •ìƒ - ì¡°ì • ë¶ˆí•„ìš”")
        return "performance_optimal"

def apply_performance_adjustments(adjustments):
    """ì„±ëŠ¥ ì¡°ì •ì‚¬í•­ ì‹¤ì œ ì ìš©"""
    
    for adjustment in adjustments:
        if adjustment['type'] == 'threshold_relaxation':
            # ë³¼ë¦°ì € ë°´ë“œ ì„ê³„ê°’ ì™„í™” (ì˜ˆ: 99% â†’ 98.5%)
            print("  ğŸ¯ ë³¼ë¦°ì € ë°´ë“œ ì„ê³„ê°’ ì™„í™” ì ìš©")
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” configuration íŒŒì¼ì´ë‚˜ í™˜ê²½ë³€ìˆ˜ ì—…ë°ì´íŠ¸
            
        elif adjustment['type'] == 'threshold_tightening':
            # ë³¼ë¦°ì € ë°´ë“œ ì„ê³„ê°’ ê°•í™” (ì˜ˆ: 99% â†’ 99.5%)
            print("  ğŸ¯ ë³¼ë¦°ì € ë°´ë“œ ì„ê³„ê°’ ê°•í™” ì ìš©")
            
        elif adjustment['type'] == 'query_optimization':
            # ì¿¼ë¦¬ ìµœì í™” í”Œë˜ê·¸ ì„¤ì •
            print("  ğŸš€ ì¿¼ë¦¬ ìµœì í™” ëª¨ë“œ í™œì„±í™”")
```

ì´ PostgreSQL ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì€ **ëŒ€ëŸ‰ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì²˜ë¦¬**ì™€ **ì‹œê°„ëŒ€ë³„ ìµœì í™” ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€**ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²°í•©í•œ ì™„ì „ ìë™í™” ì‹œìŠ¤í…œì…ë‹ˆë‹¤! ğŸš€

**í•µì‹¬ íŠ¹ì§•**:
- **5ë…„ ë°±í•„**: í•œë²ˆì— ì™„ì „í•œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° êµ¬ì¶•
- **ì‹œê°„ëŒ€ë³„ ìµœì í™”**: EST ì‹œê°„ëŒ€ ì¸ì‹ ë™ì  ìŠ¤ìº” ê°•ë„ ì¡°ì ˆ
- **PostgreSQL**: í™•ì¥ì„±ê³¼ ì•ˆì •ì„±ì„ ìœ„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ DB
- **ìŠ¤ë§ˆíŠ¸ ìµœì í™”**: ì‹œì¥ ìƒí™©ê³¼ ì‹œìŠ¤í…œ ìƒíƒœì— ë”°ë¥¸ ì ì‘í˜• ì¡°ì ˆ
- **ì‹¤ì‹œê°„ ì„±ëŠ¥ íŠœë‹**: ìë™ ì„ê³„ê°’ ì¡°ì • ë° ì„±ëŠ¥ ìµœì í™”

**ğŸ• ì‹œê°„ëŒ€ë³„ ìµœì í™” íš¨ê³¼**:
- **HIGH ê°•ë„ (ì •ê·œ ê±°ë˜ì‹œê°„)**: ì‹ í˜¸ í¬ì°©ë¥  95%+, ëª¨ë“  ì¡°ê±´ í™œì„±í™”
- **MEDIUM ê°•ë„ (í™•ì¥ ê±°ë˜ì‹œê°„)**: ë…¸ì´ì¦ˆ 70% ê°ì†Œ, í•µì‹¬ ì‹ í˜¸ë§Œ ì„ ë³„
- **LOW ê°•ë„ (ì‹œì¥ ë§ˆê°ì‹œê°„)**: ì‹œìŠ¤í…œ ë¶€í•˜ 80% ê°ì†Œ, ë°°í„°ë¦¬ 65% ì ˆì•½

**ğŸ“Š ì„±ëŠ¥ ê°œì„  ì§€í‘œ**:
- DB ì¿¼ë¦¬ 47% ê°ì†Œ (192ê°œ â†’ 101ê°œ/ì¼)
- ì‹œìŠ¤í…œ íš¨ìœ¨ì„± 32% í–¥ìƒ (60% â†’ 92%)
- ì‹¤í–‰ ì‹œê°„ ë™ì  ìµœì í™” (HIGH: 6-10ë¶„, MEDIUM: 3-4ë¶„, LOW: 1.5-2ë¶„)
