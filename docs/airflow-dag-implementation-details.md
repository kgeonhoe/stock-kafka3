# ğŸ”§ ì£¼ìš” DAG ì½”ë“œ ë¶„ì„ ë° êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

## ğŸ“Š 1. nasdaq_daily_pipeline.py - ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸

### ğŸ¯ **DAG ê¸°ë³¸ ì„¤ì •**

```python
# DAG ì •ì˜
dag = DAG(
    'nasdaq_daily_pipeline',
    default_args=default_args,
    description='ğŸš€ ë‚˜ìŠ¤ë‹¥ ì™„ì „ íŒŒì´í”„ë¼ì¸: ìˆ˜ì§‘â†’ë¶„ì„â†’ìŠ¤ìº”â†’ë³µì œ (Kafka Ready)',
    schedule_interval='0 7 * * *',  # í•œêµ­ì‹œê°„ ì˜¤ì „ 7ì‹œ (ë¯¸êµ­ ì¥ë§ˆê° í›„)
    catchup=False,
    max_active_runs=1,
    tags=['nasdaq', 'stock', 'spark', 'technical-analysis', 'watchlist', 'kafka']
)
```

**í•µì‹¬ ì„¤ì •**:
- **ìŠ¤ì¼€ì¤„**: ë§¤ì¼ ì˜¤ì „ 7ì‹œ (ë¯¸êµ­ ì¥ë§ˆê° í›„)
- **ë™ì‹œ ì‹¤í–‰ ì œí•œ**: 1ê°œ (ë°ì´í„° ì¼ê´€ì„± ë³´ì¥)
- **Catchup ë¹„í™œì„±í™”**: ê³¼ê±° ì‹¤í–‰ ê±´ë„ˆë›°ê¸°

### ğŸ”„ **Task 1: ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘**

```python
def collect_nasdaq_symbols_func(**kwargs):
    """ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘ í•¨ìˆ˜"""
    import sys
    sys.path.append('/opt/airflow/plugins')
    sys.path.append('/opt/airflow/common')
    
    from collect_nasdaq_symbols_api import NasdaqSymbolCollector
    from database import DuckDBManager
    
    print("ğŸš€ ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘ ì‹œì‘...")
    
    # 1. ì‹¬ë³¼ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = NasdaqSymbolCollector()
    
    # 2. NASDAQ APIì—ì„œ ì‹¬ë³¼ ìˆ˜ì§‘
    collected_count = collector.collect_and_save_symbols()
    
    # 3. ê²°ê³¼ ê²€ì¦
    if collected_count < 100:
        raise ValueError(f"ìˆ˜ì§‘ëœ ì‹¬ë³¼ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤: {collected_count}ê°œ")
    
    print(f"âœ… ì´ {collected_count}ê°œ ì‹¬ë³¼ ìˆ˜ì§‘ ì™„ë£Œ")
    return collected_count
```

**í•µì‹¬ ê¸°ëŠ¥**:
- NASDAQ API í˜¸ì¶œ ë° ì‹¬ë³¼ ëª©ë¡ ìˆ˜ì§‘
- ì‹œê°€ì´ì•¡ ê¸°ì¤€ í•„í„°ë§
- DuckDBì— ì¤‘ë³µ ì œê±°í•˜ì—¬ ì €ì¥
- ìµœì†Œ ìˆ˜ì§‘ ê°œìˆ˜ ê²€ì¦

### ğŸ“ˆ **Task 2: ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘**

```python
def collect_stock_data_yfinance_func(**kwargs):
    """Yahoo Financeì—ì„œ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘"""
    from collect_stock_data_yfinance import collect_stock_data_yfinance_task
    from database import DuckDBManager
    
    db = DuckDBManager(DB_PATH)
    
    try:
        # 1. ìˆ˜ì§‘ ëŒ€ìƒ ì‹¬ë³¼ ì¡°íšŒ
        symbols_query = """
            SELECT DISTINCT symbol 
            FROM nasdaq_symbols 
            WHERE is_active = true 
            AND market_cap > 100000000  -- 1ì–µ ë‹¬ëŸ¬ ì´ìƒ
            ORDER BY market_cap DESC
        """
        
        symbols_df = db.execute_query(symbols_query)
        symbols = symbols_df['symbol'].tolist()
        
        print(f"ğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ: {len(symbols)}ê°œ ì‹¬ë³¼")
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë°ì´í„° ìˆ˜ì§‘
        batch_size = 100
        total_success = 0
        
        for i in range(0, len(symbols), batch_size):
            batch = symbols[i:i + batch_size]
            print(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ)")
            
            # Yahoo Finance API í˜¸ì¶œ
            success_count = collect_stock_data_yfinance_task(
                symbols=batch,
                db_path=DB_PATH,
                days_back=5  # ìµœê·¼ 5ì¼ ë°ì´í„°
            )
            
            total_success += success_count
            print(f"âœ… ë°°ì¹˜ ì™„ë£Œ: {success_count}/{len(batch)}ê°œ ì„±ê³µ")
        
        print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {total_success}/{len(symbols)}ê°œ")
        return total_success
        
    except Exception as e:
        print(f"âŒ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        db.close()
```

**ì„±ëŠ¥ ìµœì í™”**:
- ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
- ë³‘ë ¬ API í˜¸ì¶œ
- ì—°ê²° í’€ë§ ë° ì¬ì‚¬ìš©
- ì‹¤íŒ¨í•œ ì‹¬ë³¼ ì¬ì‹œë„

### âš™ï¸ **Task 3: ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (Spark)**

```python
def calculate_technical_indicators_func(**kwargs):
    """Sparkë¥¼ ì‚¬ìš©í•œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    from technical_indicators import calculate_technical_indicators_task
    
    print("ğŸ“Š Spark ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œì‘...")
    
    # Spark ì„¤ì •
    spark_config = {
        'spark.executor.memory': '2g',
        'spark.executor.cores': '2',
        'spark.sql.adaptive.enabled': 'true',
        'spark.sql.adaptive.coalescePartitions.enabled': 'true'
    }
    
    try:
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤í–‰
        processed_symbols = calculate_technical_indicators_task(
            db_path=DB_PATH,
            spark_config=spark_config,
            indicators=['rsi_14', 'macd', 'bollinger_bands', 'sma_20', 'sma_50']
        )
        
        print(f"âœ… {processed_symbols}ê°œ ì‹¬ë³¼ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
        return processed_symbols
        
    except Exception as e:
        print(f"âŒ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        raise
```

**ê³„ì‚° ì§€í‘œ**:
- **RSI (14ì¼)**: ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ì‹ í˜¸
- **MACD**: ì¶”ì„¸ ë³€í™” ê°ì§€
- **ë³¼ë¦°ì € ë°´ë“œ**: ë³€ë™ì„± ê¸°ë°˜ ì‹ í˜¸
- **ì´ë™í‰ê· ì„ **: ì¶”ì„¸ í™•ì¸

### ğŸ¯ **Task 4: ê´€ì‹¬ì¢…ëª© ìŠ¤ìº”**

```python
def watchlist_scan_func(**kwargs):
    """ê¸°ìˆ ì  ë¶„ì„ ê¸°ë°˜ ê´€ì‹¬ì¢…ëª© ìŠ¤ìº”"""
    from database import DuckDBManager
    from datetime import date
    
    db = DuckDBManager(DB_PATH)
    
    try:
        # ìŠ¤ìº” ì¡°ê±´ë³„ ì¿¼ë¦¬ ì‹¤í–‰
        scan_results = []
        
        # 1. RSI ê³¼ë§¤ë„ ì¢…ëª© (RSI < 30)
        rsi_oversold_query = """
            SELECT 
                s.symbol, s.name, s.sector, s.market_cap,
                sd.close, sd.volume,
                ti.rsi_14, ti.bb_position,
                'rsi_oversold' as scan_reason,
                1 as priority
            FROM nasdaq_symbols s
            JOIN stock_data sd ON s.symbol = sd.symbol
            JOIN technical_indicators ti ON s.symbol = ti.symbol AND sd.date = ti.date
            WHERE sd.date = ?
            AND ti.rsi_14 < 30
            AND s.market_cap > 500000000  -- 5ì–µ ë‹¬ëŸ¬ ì´ìƒ
            AND sd.volume > s.avg_volume_30 * 1.5  -- í‰ì†Œë³´ë‹¤ 50% ì´ìƒ ê±°ë˜ëŸ‰
            ORDER BY s.market_cap DESC
            LIMIT 20
        """
        
        # 2. ë³¼ë¦°ì € ë°´ë“œ í•˜ë‹¨ í„°ì¹˜
        bollinger_touch_query = """
            SELECT 
                s.symbol, s.name, s.sector, s.market_cap,
                sd.close, sd.volume,
                ti.bb_lower, ti.bb_upper, ti.bb_position,
                'bollinger_lower_touch' as scan_reason,
                2 as priority
            FROM nasdaq_symbols s
            JOIN stock_data sd ON s.symbol = sd.symbol
            JOIN technical_indicators ti ON s.symbol = ti.symbol AND sd.date = ti.date
            WHERE sd.date = ?
            AND sd.close <= ti.bb_lower * 1.02  -- í•˜ë‹¨ ë°´ë“œ 2% ë‚´
            AND s.market_cap > 1000000000  -- 10ì–µ ë‹¬ëŸ¬ ì´ìƒ
            ORDER BY (ti.bb_lower - sd.close) DESC
            LIMIT 15
        """
        
        # 3. ëŒ€ëŸ‰ ê±°ë˜ ëŒíŒŒ
        high_volume_query = """
            SELECT 
                s.symbol, s.name, s.sector, s.market_cap,
                sd.close, sd.volume,
                (sd.volume / s.avg_volume_30) as volume_ratio,
                'high_volume_breakout' as scan_reason,
                3 as priority
            FROM nasdaq_symbols s
            JOIN stock_data sd ON s.symbol = sd.symbol
            WHERE sd.date = ?
            AND sd.volume > s.avg_volume_30 * 3  -- í‰ì†Œì˜ 3ë°° ì´ìƒ
            AND sd.close > sd.open * 1.02  -- 2% ì´ìƒ ìƒìŠ¹
            ORDER BY (sd.volume / s.avg_volume_30) DESC
            LIMIT 10
        """
        
        today = date.today()
        
        # ê° ì¡°ê±´ë³„ ìŠ¤ìº” ì‹¤í–‰
        for query_name, query in [
            ('rsi_oversold', rsi_oversold_query),
            ('bollinger_touch', bollinger_touch_query),
            ('high_volume', high_volume_query)
        ]:
            print(f"ğŸ” {query_name} ì¡°ê±´ ìŠ¤ìº” ì¤‘...")
            
            results = db.execute_query(query, (today,))
            scan_results.extend(results.to_dict('records'))
            
            print(f"âœ… {query_name}: {len(results)}ê°œ ì¢…ëª© ë°œê²¬")
        
        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_results = {}
        for result in scan_results:
            symbol = result['symbol']
            if symbol not in unique_results or result['priority'] < unique_results[symbol]['priority']:
                unique_results[symbol] = result
        
        final_watchlist = list(unique_results.values())
        
        # daily_watchlist í…Œì´ë¸”ì— ì €ì¥
        if final_watchlist:
            save_daily_watchlist(db, final_watchlist, today)
            print(f"ğŸ’¾ {len(final_watchlist)}ê°œ ê´€ì‹¬ì¢…ëª© ì €ì¥ ì™„ë£Œ")
        else:
            print("âš ï¸ ì¡°ê±´ì— ë§ëŠ” ê´€ì‹¬ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤")
        
        return len(final_watchlist)
        
    finally:
        db.close()

def save_daily_watchlist(db, watchlist_data, scan_date):
    """ì¼ì¼ ê´€ì‹¬ì¢…ëª©ì„ DBì— ì €ì¥"""
    
    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê°™ì€ ë‚ ì§œ)
    delete_query = "DELETE FROM daily_watchlist WHERE scan_date = ?"
    db.execute_query(delete_query, (scan_date,))
    
    # ìƒˆ ë°ì´í„° ì‚½ì…
    insert_query = """
        INSERT INTO daily_watchlist 
        (symbol, name, sector, market_cap, scan_date, scan_reason, priority,
         close_price, volume, rsi_14, bb_position, volume_ratio, market_cap_tier)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """
    
    for item in watchlist_data:
        # ì‹œê°€ì´ì•¡ êµ¬ê°„ ë¶„ë¥˜
        market_cap = item.get('market_cap', 0)
        if market_cap >= 50_000_000_000:  # 500ì–µ ì´ìƒ
            tier = 1  # ëŒ€í˜•ì£¼
        elif market_cap >= 10_000_000_000:  # 100ì–µ ì´ìƒ
            tier = 2  # ì¤‘í˜•ì£¼
        else:
            tier = 3  # ì†Œí˜•ì£¼
        
        db.execute_query(insert_query, (
            item['symbol'], item['name'], item['sector'], item['market_cap'],
            scan_date, item['scan_reason'], item['priority'],
            item['close'], item['volume'],
            item.get('rsi_14'), item.get('bb_position'), item.get('volume_ratio'),
            tier
        ))
```

**ìŠ¤ìº” ì „ëµ**:
- **ë‹¤ì¤‘ ì¡°ê±´ ìŠ¤ìº”**: RSI, ë³¼ë¦°ì € ë°´ë“œ, ê±°ë˜ëŸ‰
- **ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì„ ë³„**: ì¤‘ë³µ ì‹œ ë” ê°•í•œ ì‹ í˜¸ ìš°ì„ 
- **ì‹œê°€ì´ì•¡ í•„í„°ë§**: ì•ˆì •ì„± í™•ë³´
- **ê±°ë˜ëŸ‰ ê²€ì¦**: ìœ ë™ì„± í™•ë³´

### ğŸ”„ **Task 5: ë°ì´í„° ë³µì œ (Read Replica)**

```python
def database_replication_func(**kwargs):
    """DuckDB ì½ê¸° ì „ìš© ë³µì œë³¸ ìƒì„±"""
    import shutil
    from pathlib import Path
    
    source_db = Path(DB_PATH)
    replica_db = Path("/data/duckdb/stock_data_replica.db")
    
    try:
        print("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ë³µì œ ì‹œì‘...")
        
        # 1. ì†ŒìŠ¤ DB ë°±ì—…
        backup_db = Path("/data/duckdb/backup/stock_data_backup.db")
        backup_db.parent.mkdir(exist_ok=True)
        shutil.copy2(source_db, backup_db)
        
        # 2. ì½ê¸° ì „ìš© ë³µì œë³¸ ìƒì„±
        shutil.copy2(source_db, replica_db)
        
        # 3. ê¶Œí•œ ì„¤ì • (ì½ê¸° ì „ìš©)
        replica_db.chmod(0o444)  # r--r--r--
        
        # 4. ë³µì œ ê²€ì¦
        from database import DuckDBManager
        replica_manager = DuckDBManager(str(replica_db))
        
        # í…Œì´ë¸” ìˆ˜ í™•ì¸
        table_count_query = """
            SELECT COUNT(*) as table_count
            FROM information_schema.tables 
            WHERE table_schema = 'main'
        """
        result = replica_manager.execute_query(table_count_query)
        table_count = result.iloc[0]['table_count']
        
        replica_manager.close()
        
        if table_count < 5:  # ìµœì†Œ í…Œì´ë¸” ìˆ˜ ê²€ì¦
            raise ValueError(f"ë³µì œ DB í…Œì´ë¸” ìˆ˜ ë¶€ì¡±: {table_count}ê°œ")
        
        print(f"âœ… ë³µì œ ì™„ë£Œ: {table_count}ê°œ í…Œì´ë¸”")
        return "success"
        
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ë³µì œ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì´ì „ ë³µì œë³¸ ìœ ì§€
        if backup_db.exists() and not replica_db.exists():
            shutil.copy2(backup_db, replica_db)
        raise
```

**ë³µì œ ì „ëµ**:
- **íŒŒì¼ ê¸°ë°˜ ë³µì œ**: ë¹ ë¥¸ ë³µì‚¬
- **ì½ê¸° ì „ìš© ë³´ì¥**: ê¶Œí•œ ì œí•œ
- **ë°±ì—… ë³´ê´€**: ì‹¤íŒ¨ ì‹œ ë³µêµ¬
- **ë¬´ê²°ì„± ê²€ì¦**: í…Œì´ë¸” ìˆ˜ í™•ì¸

## ğŸ”„ 2. redis_watchlist_sync.py - Redis ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™”

### ğŸ§  **ìŠ¤ë§ˆíŠ¸ ì—…ë°ì´íŠ¸ ë¡œì§**

```python
def redis_smart_sync_task(**kwargs):
    """ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰"""
    try:
        from load_watchlist_to_redis import WatchlistDataLoader
        
        # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
        execution_date = kwargs['execution_date']
        is_weekend = execution_date.weekday() in [5, 6]  # í† , ì¼
        force_full = kwargs.get('dag_run').conf.get('force_full', False) if kwargs.get('dag_run') and kwargs.get('dag_run').conf else False
        
        loader = WatchlistDataLoader()
        
        if force_full or is_weekend:
            print("ğŸ”„ ì£¼ë§ ì „ì²´ ì¬ë™ê¸°í™” ëª¨ë“œ")
            success = loader.load_watchlist_to_redis(days_back=30)
            loader.set_last_update_info(1, 1, "weekly_full_sync")
        else:
            print("âš¡ í‰ì¼ ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸")
            success = loader.smart_incremental_update()
        
        return "success" if success else raise Exception("Redis ë™ê¸°í™” ì‹¤íŒ¨")
```

**ì—…ë°ì´íŠ¸ ì „ëµ**:
- **í‰ì¼**: ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸ (2-5ë¶„)
- **ì£¼ë§**: ì „ì²´ ì¬ë™ê¸°í™” (10-15ë¶„)
- **ìˆ˜ë™**: force_full íŒŒë¼ë¯¸í„°ë¡œ ê°•ì œ ì‹¤í–‰

### ğŸ” **Redis ìƒíƒœ ê²€ì¦**

```python
def redis_health_check_task(**kwargs):
    """Redis ìƒíƒœ ë° ë°ì´í„° ê²€ì¦"""
    from redis_client import RedisClient
    
    redis_client = RedisClient()
    
    # 1. ì„œë²„ ìƒíƒœ í™•ì¸
    info = redis_client.redis_client.info()
    memory_usage = info.get('used_memory_human', 'Unknown')
    
    # 2. ë°ì´í„° ê°œìˆ˜ í™•ì¸
    watchlist_keys = redis_client.redis_client.keys("watchlist_data:*")
    signal_keys = redis_client.redis_client.keys("signal_trigger:*")
    
    # 3. ìµœì†Œ ìš”êµ¬ì‚¬í•­ ê²€ì¦
    if len(watchlist_keys) < 10:
        raise Exception(f"ê´€ì‹¬ì¢…ëª© ë°ì´í„° ë¶€ì¡±: {len(watchlist_keys)}ê°œ")
    
    # 4. ìƒ˜í”Œ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
    if watchlist_keys:
        sample_data = redis_client.redis_client.get(watchlist_keys[0])
        if not sample_data:
            raise Exception("ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨")
    
    print(f"âœ… Redis ìƒíƒœ ì •ìƒ: {len(watchlist_keys)}ê°œ ê´€ì‹¬ì¢…ëª©")
    return "healthy"
```

### ğŸ§¹ **ìë™ ë°ì´í„° ì •ë¦¬**

```python
def cleanup_old_data_task(**kwargs):
    """ì˜¤ë˜ëœ Redis ë°ì´í„° ì •ë¦¬"""
    from redis_client import RedisClient
    
    redis_client = RedisClient()
    cutoff_date = datetime.now() - timedelta(days=7)
    
    # 1. ì˜¤ë˜ëœ ì‹ í˜¸ ì •ë¦¬
    old_signal_keys = redis_client.redis_client.keys("signal_trigger:*")
    cleaned_signals = 0
    
    for key in old_signal_keys:
        try:
            key_str = key.decode('utf-8')
            parts = key_str.split(':')
            if len(parts) >= 3:
                timestamp_str = parts[2]
                signal_date = datetime.fromisoformat(timestamp_str[:19])
                
                if signal_date < cutoff_date:
                    redis_client.redis_client.delete(key)
                    cleaned_signals += 1
        except:
            continue
    
    # 2. ì„ì‹œ ë°ì´í„° ì •ë¦¬
    temp_keys = redis_client.redis_client.keys("temp_*")
    for key in temp_keys:
        redis_client.redis_client.delete(key)
    
    print(f"ğŸ§¹ ì •ë¦¬ ì™„ë£Œ: ì‹ í˜¸ {cleaned_signals}ê°œ, ì„ì‹œ {len(temp_keys)}ê°œ")
    return "cleaned"
```

## ğŸ“Š 3. Task ì˜ì¡´ì„± ë° ì‹¤í–‰ íë¦„

### ğŸ”„ **nasdaq_daily_pipeline ì‹¤í–‰ íë¦„**

```python
# Task ì˜ì¡´ì„± ì •ì˜
collect_symbols >> collect_stock_data >> calculate_indicators >> watchlist_scan >> database_replication >> success_notification

# ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ Taskë“¤
[cleanup_old_files, generate_reports] >> final_notification
```

### âš¡ **redis_watchlist_sync ì‹¤í–‰ íë¦„**

```python
# ìˆœì°¨ ì‹¤í–‰
wait_for_nasdaq >> redis_sync >> redis_health_check >> [signal_prepare, cleanup_data] >> success_notification

# ìˆ˜ë™ ì‹¤í–‰ Task (ë…ë¦½ì )
manual_full_sync  # ì–¸ì œë“  ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥
```

## ğŸ¯ 4. í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ ë° ëª¨ë‹ˆí„°ë§

### ğŸ“ˆ **ì²˜ë¦¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­**

```python
# DAG ì„±ëŠ¥ ì¶”ì 
def track_performance(**kwargs):
    """DAG ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
    
    task_instance = kwargs['task_instance']
    execution_date = kwargs['execution_date']
    
    # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
    start_time = task_instance.start_date
    end_time = task_instance.end_date
    duration = (end_time - start_time).total_seconds()
    
    # ë©”íŠ¸ë¦­ ì €ì¥
    metrics = {
        'dag_id': task_instance.dag_id,
        'task_id': task_instance.task_id,
        'execution_date': execution_date,
        'duration_seconds': duration,
        'status': task_instance.state,
        'try_number': task_instance.try_number
    }
    
    # Redisì— ì„±ëŠ¥ ë°ì´í„° ì €ì¥
    redis_client.redis_client.lpush('dag_performance_metrics', json.dumps(metrics))
    
    return metrics
```

### ğŸ”” **ì•Œë¦¼ ë° ëª¨ë‹ˆí„°ë§**

```python
# ì‹¤íŒ¨ ì‹œ ì•Œë¦¼
def send_failure_notification(**kwargs):
    """DAG ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ ì „ì†¡"""
    
    task_instance = kwargs['task_instance']
    exception = kwargs.get('exception', 'Unknown error')
    
    notification = {
        'type': 'dag_failure',
        'dag_id': task_instance.dag_id,
        'task_id': task_instance.task_id,
        'execution_date': str(kwargs['execution_date']),
        'error_message': str(exception),
        'timestamp': datetime.now().isoformat()
    }
    
    # Slack/Discord ì›¹í›… (ì„¤ì • ì‹œ)
    if SLACK_WEBHOOK_URL:
        send_slack_notification(notification)
    
    # Redisì— ì•Œë¦¼ ë¡œê·¸ ì €ì¥
    redis_client.redis_client.lpush('system_alerts', json.dumps(notification))

# DAG ë ˆë²¨ ì‹¤íŒ¨ í•¸ë“¤ëŸ¬ ì„¤ì •
dag.on_failure_callback = send_failure_notification
```

ì´ Airflow ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì€ **ì™„ì „ ìë™í™”ëœ ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ**ìœ¼ë¡œ, ìˆ˜ì§‘ë¶€í„° ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤! ğŸš€
