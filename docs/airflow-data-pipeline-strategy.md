# ğŸ“Š Airflow ê¸°ë°˜ ì£¼ì‹ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì „ëµ

## ğŸ¯ ì „ì²´ ì•„í‚¤í…ì²˜ ê°œìš”

ì£¼ì‹ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì€ **Airflowë¥¼ ì‹œì‘ìœ¼ë¡œ í•œ ì™„ì „ ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤.

```mermaid
graph TD
    A[NASDAQ API] --> B[ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘]
    C[Yahoo Finance API] --> D[ì£¼ì‹ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘]
    B --> E[DuckDB ì €ì¥]
    D --> E
    E --> F[ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°]
    F --> G[ê´€ì‹¬ì¢…ëª© ìŠ¤ìº”]
    G --> H[Redis ë™ê¸°í™”]
    H --> I[ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€]
    I --> J[Kafka ìŠ¤íŠ¸ë¦¬ë°]
    J --> K[Streamlit ëŒ€ì‹œë³´ë“œ]
```

## ğŸš€ ì£¼ìš” DAG êµ¬ì„±

### 1. **nasdaq_daily_pipeline.py** - ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
**ì‹¤í–‰ ì‹œê°„**: ë§¤ì¼ ì˜¤ì „ 7ì‹œ (í•œêµ­ì‹œê°„, ë¯¸êµ­ ì¥ë§ˆê° í›„)
**ì†Œìš” ì‹œê°„**: ì•½ 2-3ì‹œê°„

```python
# DAG ìŠ¤ì¼€ì¤„ë§
schedule_interval='0 7 * * *'  # í•œêµ­ì‹œê°„ ì˜¤ì „ 7ì‹œ

# Task íë¦„
ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘ â†’ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ â†’ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° â†’ ê´€ì‹¬ì¢…ëª© ìŠ¤ìº” â†’ ë°ì´í„° ë³µì œ â†’ ì™„ë£Œ
```

### 2. **redis_watchlist_sync.py** - Redis ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™”
**ì‹¤í–‰ ì¡°ê±´**: `nasdaq_daily_pipeline` ì™„ë£Œ í›„ ìë™ íŠ¸ë¦¬ê±°
**ì†Œìš” ì‹œê°„**: í‰ì¼ 2-5ë¶„, ì£¼ë§ 10-15ë¶„

```python
# DAG ì˜ì¡´ì„± ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§
schedule_interval=None  # ìˆ˜ë™ íŠ¸ë¦¬ê±° (ì˜ì¡´ì„± ê¸°ë°˜)

# ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ê°ì§€
wait_for_nasdaq = FileSensor(
    task_id='wait_for_nasdaq_pipeline',
    filepath='/tmp/nasdaq_pipeline_complete.flag',
    poke_interval=300,  # 5ë¶„ë§ˆë‹¤ í™•ì¸
    timeout=3600       # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
)

# ì‹¤í–‰ ëª¨ë“œ ìë™ ì„ íƒ
if is_weekend:
    ì „ì²´_ì¬ë™ê¸°í™”()  # ì£¼ë§: ì™„ì „ ì¬ë¡œë”©
else:
    ìŠ¤ë§ˆíŠ¸_ì¦ë¶„_ì—…ë°ì´íŠ¸()  # í‰ì¼: ë³€ê²½ë¶„ë§Œ ì—…ë°ì´íŠ¸
```

## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì „ëµ

### ğŸ”„ **1ë‹¨ê³„: ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘**

```python
class NasdaqSymbolCollector:
    def collect_symbols(self):
        """NASDAQ APIì—ì„œ ì „ì²´ ì‹¬ë³¼ ëª©ë¡ ìˆ˜ì§‘"""
        
        # 1. NASDAQ API í˜¸ì¶œ
        symbols = self.fetch_from_nasdaq_api()
        
        # 2. ì‹œê°€ì´ì•¡ í•„í„°ë§
        filtered_symbols = self.filter_by_market_cap(symbols, min_cap=100_000_000)
        
        # 3. DuckDBì— ì €ì¥
        self.save_to_duckdb(filtered_symbols)
        
        return len(filtered_symbols)
```

**ë°ì´í„° ì†ŒìŠ¤**: NASDAQ Official API
**ì—…ë°ì´íŠ¸ ì£¼ê¸°**: ë§¤ì¼
**í•„í„°ë§ ì¡°ê±´**:
- ì‹œê°€ì´ì•¡ 1ì–µ ë‹¬ëŸ¬ ì´ìƒ
- í™œì„± ê±°ë˜ ì¢…ëª©ë§Œ
- ETF ì œì™¸

### ğŸ“ˆ **2ë‹¨ê³„: ì£¼ì‹ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘**

```python
def collect_stock_data_yfinance_task(**kwargs):
    """Yahoo Financeì—ì„œ ì£¼ì‹ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘"""
    
    # 1. ìˆ˜ì§‘ ëŒ€ìƒ ì‹¬ë³¼ ì¡°íšŒ
    symbols = get_symbols_from_duckdb()
    
    # 2. ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
    for batch in chunk_symbols(symbols, batch_size=100):
        
        # 3. ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘
        with ThreadPoolExecutor(max_workers=10) as executor:
            results = executor.map(fetch_stock_data, batch)
        
        # 4. DuckDB ì¼ê´„ ì €ì¥
        save_batch_to_duckdb(results)
    
    return len(symbols)
```

**ë°ì´í„° ì†ŒìŠ¤**: Yahoo Finance API (yfinance)
**ìˆ˜ì§‘ ë°ì´í„°**:
- OHLCV (ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€, ê±°ë˜ëŸ‰)
- ë°°ë‹¹ ì •ë³´
- ì£¼ì‹ ë¶„í•  ì´ë ¥

**ì„±ëŠ¥ ìµœì í™”**:
- ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
- ë³‘ë ¬ ì²˜ë¦¬ (10ê°œ ìŠ¤ë ˆë“œ)
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

### ğŸ“Š **3ë‹¨ê³„: ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°**

```python
def calculate_technical_indicators_task(**kwargs):
    """Sparkë¥¼ ì‚¬ìš©í•œ ëŒ€ìš©ëŸ‰ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    
    spark = SparkSession.builder.appName("TechnicalIndicators").getOrCreate()
    
    # 1. DuckDBì—ì„œ ë°ì´í„° ë¡œë”©
    stock_df = spark.read.format("parquet").load("/data/stock_data")
    
    # 2. ìœˆë„ìš° í•¨ìˆ˜ë¡œ ì§€í‘œ ê³„ì‚°
    window_spec = Window.partitionBy("symbol").orderBy("date")
    
    result_df = stock_df.withColumn(
        "rsi_14", rsi_udf(col("close")).over(window_spec)
    ).withColumn(
        "macd", macd_udf(col("close")).over(window_spec)
    ).withColumn(
        "bb_upper", bollinger_upper_udf(col("close")).over(window_spec)
    )
    
    # 3. ê²°ê³¼ ì €ì¥
    result_df.write.mode("overwrite").parquet("/data/technical_indicators")
```

**ê³„ì‚° ì§€í‘œ**:
- **RSI (14ì¼)**: ê³¼ë§¤ìˆ˜/ê³¼ë§¤ë„ ê°ì§€
- **MACD**: ì¶”ì„¸ ë³€í™” ê°ì§€
- **ë³¼ë¦°ì € ë°´ë“œ**: ë³€ë™ì„± ì¸¡ì •
- **ì´ë™í‰ê· ì„ **: ì¶”ì„¸ í™•ì¸

**ì²˜ë¦¬ ì—”ì§„**: Apache Spark
**ì¥ì **: ëŒ€ìš©ëŸ‰ ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬

### ğŸ¯ **4ë‹¨ê³„: ê´€ì‹¬ì¢…ëª© ìŠ¤ìº”**

```python
def watchlist_scan_task(**kwargs):
    """ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ê´€ì‹¬ì¢…ëª© ìŠ¤ìº”"""
    
    # ìŠ¤ìº” ì¡°ê±´ ì •ì˜
    scan_conditions = {
        'rsi_oversold': 'rsi_14 < 30',
        'bollinger_lower_touch': 'close <= bb_lower * 1.01',
        'high_volume': 'volume > avg_volume_20 * 2',
        'large_cap': 'market_cap > 1000000000'
    }
    
    # ì¡°ê±´ë³„ ì¢…ëª© ìŠ¤ìº”
    watchlist_results = []
    for condition_name, sql_condition in scan_conditions.items():
        
        query = f"""
            SELECT symbol, name, sector, market_cap,
                   close, rsi_14, bb_position, volume_ratio,
                   '{condition_name}' as scan_reason
            FROM technical_analysis_view
            WHERE {sql_condition}
            AND date = CURRENT_DATE
            ORDER BY market_cap DESC
            LIMIT 50
        """
        
        results = execute_duckdb_query(query)
        watchlist_results.extend(results)
    
    # ì¤‘ë³µ ì œê±° ë° ì €ì¥
    unique_watchlist = deduplicate_by_symbol(watchlist_results)
    save_daily_watchlist(unique_watchlist)
    
    return len(unique_watchlist)
```

**ìŠ¤ìº” ì¡°ê±´**:
1. **RSI ê³¼ë§¤ë„** (RSI < 30): ë°˜ë“± ê¸°ëŒ€
2. **ë³¼ë¦°ì € í•˜ë‹¨ í„°ì¹˜**: ê³¼ë§¤ë„ ë°˜ë“±
3. **ëŒ€ëŸ‰ ê±°ë˜**: ê¸‰ë“± ê°€ëŠ¥ì„±
4. **ëŒ€í˜•ì£¼ ìš°ì„ **: ì•ˆì •ì„± í™•ë³´

### ğŸ”„ **5ë‹¨ê³„: Redis ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™”**

```python
class WatchlistDataLoader:
    def smart_incremental_update(self):
        """ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ"""
        
        # 1. ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ í™•ì¸
        last_update = self.get_last_update_info()
        days_since = calculate_days_since_update(last_update)
        
        # 2. ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì •
        if days_since == 0:
            return self.update_new_symbols_only()  # ì‹ ê·œ ì¢…ëª©ë§Œ
        elif days_since <= 6:
            return self.incremental_update(days_since + 1)  # ì¦ë¶„ ì—…ë°ì´íŠ¸
        else:
            return self.full_reload()  # ì „ì²´ ì¬ë¡œë”©
    
    def incremental_update(self, days_to_update):
        """ë³€ê²½ëœ ë°ì´í„°ë§Œ íš¨ìœ¨ì  ì—…ë°ì´íŠ¸"""
        
        # ë³€ê²½ ì‚¬í•­ ë¶„ì„
        existing_symbols = self.get_redis_symbols()
        current_symbols = self.get_db_symbols()
        
        new_symbols = set(current_symbols) - set(existing_symbols)
        removed_symbols = set(existing_symbols) - set(current_symbols)
        unchanged_symbols = set(existing_symbols) & set(current_symbols)
        
        # íš¨ìœ¨ì  ì—…ë°ì´íŠ¸ ì‹¤í–‰
        self.remove_symbols(removed_symbols)  # ì œê±°
        self.add_full_data(new_symbols)       # ì‹ ê·œ ì¶”ê°€
        self.update_recent_data(unchanged_symbols, days_to_update)  # ì¦ë¶„
```

**ì„±ëŠ¥ í–¥ìƒ**:
- **ì²˜ë¦¬ ì‹œê°„**: 30ë¶„ â†’ 2-5ë¶„ (85% ë‹¨ì¶•)
- **ë„¤íŠ¸ì›Œí¬ íŠ¸ë˜í”½**: 90% ê°ì†Œ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì¼ì • ìˆ˜ì¤€ ìœ ì§€

## âš¡ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œìŠ¤í…œ

### ğŸ“Š **Redis ê¸°ë°˜ ì‹ í˜¸ ê°ì§€**

```python
class SignalDetector:
    def detect_signals_realtime(self, symbol):
        """ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€ ë° Kafka ì „ì†¡"""
        
        # 1. Redisì—ì„œ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì¡°íšŒ
        historical_data = self.redis.get_watchlist_data(symbol)
        
        # 2. ì‹¤ì‹œê°„ ê°€ê²©ê³¼ ê²°í•©
        current_price = self.get_realtime_price(symbol)
        combined_data = historical_data + [current_price]
        
        # 3. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        indicators = self.calculate_indicators(combined_data)
        
        # 4. ì‹ í˜¸ ê°ì§€ ë¡œì§
        signals = []
        if indicators['rsi'] < 30:
            signals.append('rsi_oversold')
        if current_price <= indicators['bb_lower']:
            signals.append('bollinger_lower_touch')
        
        # 5. Kafkaë¡œ ì‹ í˜¸ ì „ì†¡
        for signal in signals:
            self.kafka_producer.send('stock-signals', {
                'symbol': symbol,
                'signal_type': signal,
                'timestamp': datetime.now().isoformat(),
                'trigger_price': current_price,
                'indicators': indicators
            })
```

### ğŸŒŠ **Kafka ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸**

```python
# Producer: ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
def realtime_producer():
    while True:
        for symbol in watchlist_symbols:
            price_data = fetch_realtime_price(symbol)
            kafka_producer.send('realtime-stock', price_data)
        time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸

# Consumer: ì‹ í˜¸ ê°ì§€ ë° ì²˜ë¦¬
def signal_consumer():
    for message in kafka_consumer:
        stock_data = message.value
        signals = detect_signals(stock_data)
        
        if signals:
            # Redisì— ì‹ í˜¸ ì €ì¥
            save_signal_to_redis(signals)
            
            # ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
            update_streamlit_dashboard(signals)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### ğŸš€ **ì²˜ë¦¬ ì„±ëŠ¥**

| ì»´í¬ë„ŒíŠ¸ | ìµœì í™” ê¸°ë²• | ì„±ëŠ¥ í–¥ìƒ |
|----------|-------------|-----------|
| ë°ì´í„° ìˆ˜ì§‘ | ë³‘ë ¬ ì²˜ë¦¬ + ë°°ì¹˜ | 300% |
| ê¸°ìˆ ì  ì§€í‘œ | Spark ë¶„ì‚° ì²˜ë¦¬ | 500% |
| Redis ë™ê¸°í™” | ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸ | 85% ì‹œê°„ ë‹¨ì¶• |
| ì‹¤ì‹œê°„ ì²˜ë¦¬ | Kafka ìŠ¤íŠ¸ë¦¬ë° | ì‹¤ì‹œê°„ (<1ì´ˆ) |

### ğŸ’¾ **ë°ì´í„° ì €ì¥ ì „ëµ**

```python
# DuckDB: ë¶„ì„ìš© ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤
- íˆìŠ¤í† ë¦¬ì»¬ ì£¼ì‹ ë°ì´í„° (ì••ì¶• ì €ì¥)
- ê¸°ìˆ ì  ì§€í‘œ (ì‚¬ì „ ê³„ì‚° ì €ì¥)
- ê´€ì‹¬ì¢…ëª© ìŠ¤ìº” ê²°ê³¼

# Redis: ì‹¤ì‹œê°„ ì²˜ë¦¬ìš© ìºì‹œ
- ìµœê·¼ 30ì¼ ê°€ê²© ë°ì´í„° (ë¹ ë¥¸ ì ‘ê·¼)
- í™œì„± ì‹ í˜¸ (TTL ê´€ë¦¬)
- ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼

# Kafka: ìŠ¤íŠ¸ë¦¬ë° ë²„í¼
- ì‹¤ì‹œê°„ ê°€ê²© í”¼ë“œ
- ì‹ í˜¸ ë°œìƒ ì´ë²¤íŠ¸
- ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
```

## ğŸ”„ DAG ì˜ì¡´ì„± ë° ìŠ¤ì¼€ì¤„ë§

```python
# ì£¼ìš” íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ íë¦„
07:00 - nasdaq_daily_pipeline ì‹œì‘ (ì‹œê°„ ê¸°ë°˜ íŠ¸ë¦¬ê±°)
  â”œâ”€â”€ 07:00-08:00: ë‚˜ìŠ¤ë‹¥ ì‹¬ë³¼ ìˆ˜ì§‘
  â”œâ”€â”€ 08:00-09:30: ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬)
  â”œâ”€â”€ 09:30-10:00: ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (Spark)
  â”œâ”€â”€ 10:00-10:15: ê´€ì‹¬ì¢…ëª© ìŠ¤ìº”
  â”œâ”€â”€ 10:15-10:30: ë°ì´í„° ë³µì œ ë° ì •ë¦¬
  â””â”€â”€ 10:30: ì™„ë£Œ í”Œë˜ê·¸ íŒŒì¼ ìƒì„± (/tmp/nasdaq_pipeline_complete.flag)

ìë™ íŠ¸ë¦¬ê±° - redis_watchlist_sync ì‹œì‘ (ì˜ì¡´ì„± ê¸°ë°˜)
  â”œâ”€â”€ 10:30+: ë‚˜ìŠ¤ë‹¥ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ëŒ€ê¸°
  â”œâ”€â”€ ì™„ë£Œ ê°ì§€ í›„: Redis ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™” (2-5ë¶„)
  â”œâ”€â”€ ìƒíƒœ ê²€ì¦ ë° ì‹ í˜¸ ì‹œìŠ¤í…œ ì¤€ë¹„
  â””â”€â”€ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ

ì‹¤ì‹œê°„ - 24/7 ì—°ì† ì‹¤í–‰
  â”œâ”€â”€ Kafka Producer: ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
  â”œâ”€â”€ Kafka Consumer: ì‹ í˜¸ ê°ì§€ (Redis ë°ì´í„° í™œìš©)
  â””â”€â”€ Streamlit: ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸
```

### ğŸ”— **DAG ê°„ ì˜ì¡´ì„± ê´€ë¦¬**

```python
# nasdaq_daily_pipeline.py - ë©”ì¸ íŒŒì´í”„ë¼ì¸
def create_completion_flag(**kwargs):
    """íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í”Œë˜ê·¸ ìƒì„±"""
    flag_file = "/tmp/nasdaq_pipeline_complete.flag"
    
    with open(flag_file, 'w') as f:
        f.write(json.dumps({
            'completion_time': datetime.now().isoformat(),
            'dag_run_id': kwargs['dag_run'].run_id,
            'execution_date': str(kwargs['execution_date']),
            'processed_symbols': kwargs['ti'].xcom_pull(task_ids='collect_stock_data'),
            'watchlist_count': kwargs['ti'].xcom_pull(task_ids='watchlist_scan')
        }))
    
    print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í”Œë˜ê·¸ ìƒì„±: {flag_file}")

# Task ìˆœì„œ: ... â†’ database_replication â†’ create_completion_flag â†’ success_notification

# redis_watchlist_sync.py - ì˜ì¡´ì„± ê¸°ë°˜ ë™ê¸°í™”
wait_for_nasdaq = FileSensor(
    task_id='wait_for_nasdaq_pipeline',
    filepath='/tmp/nasdaq_pipeline_complete.flag',
    fs_conn_id='fs_default',
    poke_interval=300,  # 5ë¶„ë§ˆë‹¤ í™•ì¸
    timeout=3600,       # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
    soft_fail=False     # ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í•„ìˆ˜
)

def read_pipeline_completion_info(**kwargs):
    """ì™„ë£Œëœ íŒŒì´í”„ë¼ì¸ ì •ë³´ ì½ê¸°"""
    flag_file = "/tmp/nasdaq_pipeline_complete.flag"
    
    with open(flag_file, 'r') as f:
        completion_info = json.loads(f.read())
    
    print(f"ğŸ“Š ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì •ë³´:")
    print(f"   ì™„ë£Œ ì‹œê°„: {completion_info['completion_time']}")
    print(f"   ì²˜ë¦¬ ì‹¬ë³¼ ìˆ˜: {completion_info['processed_symbols']}")
    print(f"   ê´€ì‹¬ì¢…ëª© ìˆ˜: {completion_info['watchlist_count']}")
    
    return completion_info

# Task ìˆœì„œ: wait_for_nasdaq â†’ read_completion_info â†’ redis_smart_sync â†’ ...
```

## ğŸ›¡ï¸ ì•ˆì •ì„± ë° ëª¨ë‹ˆí„°ë§

### ğŸ“Š **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**

1. **Airflow UI** (`http://localhost:8081`)
   - DAG ì‹¤í–‰ ìƒíƒœ ë° ì„±ëŠ¥
   - Taskë³„ ë¡œê·¸ ë° ë©”íŠ¸ë¦­
   - ì‹¤íŒ¨ ì•Œë¦¼ ë° ì¬ì‹œë„

2. **Streamlit ëŒ€ì‹œë³´ë“œ** (`http://localhost:8501`)
   - ì‹¤ì‹œê°„ Redis ë°ì´í„° ìƒíƒœ
   - Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ ëª¨ë‹ˆí„°ë§
   - ì‹ í˜¸ ë°œìƒ ì¶”ì  ë° ì„±ê³¼

3. **Kafka UI** (`http://localhost:8080`)
   - í† í”½ë³„ ë©”ì‹œì§€ ì²˜ë¦¬ëŸ‰
   - Consumer Group ì§€ì—°ì‹œê°„
   - íŒŒí‹°ì…˜ ë°¸ëŸ°ì‹± ìƒíƒœ

### ğŸ”§ **ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬**

```python
# ìë™ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
default_args = {
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True
}

# Fallback ì „ëµ
try:
    ìŠ¤ë§ˆíŠ¸_ì¦ë¶„_ì—…ë°ì´íŠ¸()
except Exception:
    ì „ì²´_ì¬ë¡œë”©_ì‹¤í–‰()  # ì•ˆì „ ëª¨ë“œ

# ë°ì´í„° í’ˆì§ˆ ê²€ì¦
def validate_data_quality():
    if ê´€ì‹¬ì¢…ëª©_ìˆ˜ < ìµœì†Œ_ìš”êµ¬ì‚¬í•­:
        raise DataQualityError("ë°ì´í„° ë¶€ì¡±")
    
    if Redis_ì—°ê²°_ì‹¤íŒ¨:
        raise ConnectionError("Redis ì—°ê²° ì‹¤íŒ¨")
```

## ğŸ¯ í•µì‹¬ ì„±ê³¼ ì§€í‘œ

### ğŸ“ˆ **ì²˜ë¦¬ ì„±ëŠ¥**
- **ì¼ì¼ ì²˜ë¦¬ ì¢…ëª© ìˆ˜**: ~3,000ê°œ
- **ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„**: 2-3ì‹œê°„
- **ì‹¤ì‹œê°„ ì§€ì—°ì‹œê°„**: <1ì´ˆ
- **ì‹œìŠ¤í…œ ê°€ìš©ì„±**: 99.5%

### ğŸ’° **ë¹„ìš© íš¨ìœ¨ì„±**
- **ì¸í”„ë¼ ë¹„ìš©**: Docker ê¸°ë°˜ ë¡œì»¬ ì‹¤í–‰
- **API ë¹„ìš©**: Yahoo Finance (ë¬´ë£Œ)
- **ìŠ¤í† ë¦¬ì§€**: DuckDB (ì••ì¶• íš¨ìœ¨)
- **í™•ì¥ì„±**: Kubernetes ë°°í¬ ê°€ëŠ¥

### ğŸ”„ **ìë™í™” ìˆ˜ì¤€**
- **ë°ì´í„° ìˆ˜ì§‘**: 100% ìë™í™”
- **ì‹ í˜¸ ê°ì§€**: ì‹¤ì‹œê°„ ìë™í™”
- **ëª¨ë‹ˆí„°ë§**: ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
- **ì¥ì•  ë³µêµ¬**: ìë™ ì¬ì‹œë„ ë° ì•Œë¦¼

ì´ ì‹œìŠ¤í…œì€ **ì™„ì „ ìë™í™”ëœ ì£¼ì‹ ë°ì´í„° íŒŒì´í”„ë¼ì¸**ìœ¼ë¡œ, ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤! ğŸš€
