#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Redis ê´€ì‹¬ì¢…ëª© ë°ì´í„° ë™ê¸°í™” DAG
- ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›
- ì‹¤ì‹œê°„ ì‹ í˜¸ ê°ì§€ ì‹œìŠ¤í…œì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from datetime import datetime, timedelta
import sys
import os

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, '/opt/airflow/common')
sys.path.insert(0, '/opt/airflow/plugins')

# ê¸°ë³¸ ì¸ìˆ˜ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 7, 30),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1
}

# DAG ì •ì˜
dag = DAG(
    'redis_watchlist_sync',
    default_args=default_args,
    description='ğŸ”„ Redis ê´€ì‹¬ì¢…ëª© ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™” (ìµœì í™”ëœ ëŒ€ëŸ‰ ìˆ˜ì§‘ ì™„ë£Œ í›„)',
    schedule_interval=None,  # ìˆ˜ë™ íŠ¸ë¦¬ê±° (ì˜ì¡´ì„± ê¸°ë°˜)
    catchup=False,
    max_active_runs=1,
    tags=['redis', 'watchlist', 'incremental', 'smart-update', 'signal-detection', 'triggered', 'optimized']
)

def read_pipeline_completion_info(**kwargs):
    """ì™„ë£Œëœ íŒŒì´í”„ë¼ì¸ ì •ë³´ ì½ê¸° (ëŒ€ëŸ‰ ìˆ˜ì§‘ ë˜ëŠ” ì¼ë°˜ íŒŒì´í”„ë¼ì¸)"""
    import json
    
    try:
        # 1. ëŒ€ëŸ‰ ìˆ˜ì§‘ ì™„ë£Œ í”Œë˜ê·¸ í™•ì¸ (ìš°ì„ ìˆœìœ„)
        bulk_flag_file = "/tmp/nasdaq_bulk_collection_complete.flag"
        
        if os.path.exists(bulk_flag_file):
            with open(bulk_flag_file, 'r') as f:
                completion_info = json.loads(f.read())
            
            print(f"ğŸ“Š ëŒ€ëŸ‰ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì •ë³´:")
            print(f"   ì™„ë£Œ ì‹œê°„: {completion_info['completion_time']}")
            print(f"   DAG Run ID: {completion_info['dag_run_id']}")
            print(f"   ìˆ˜ì§‘ íƒ€ì…: {completion_info['collection_type']}")
            
            # ìˆ˜ì§‘ í†µê³„
            final_stats = completion_info.get('final_stats', {})
            main_table = final_stats.get('main_table', {})
            print(f"   ì´ ë ˆì½”ë“œ: {main_table.get('record_count', 0):,}")
            print(f"   ì¢…ëª© ìˆ˜: {main_table.get('symbol_count', 0):,}")
            
            date_range = main_table.get('date_range', {})
            print(f"   ë°ì´í„° ë²”ìœ„: {date_range.get('start', 'Unknown')} ~ {date_range.get('end', 'Unknown')}")
            
            # Redis ë™ê¸°í™”ìš© ì •ë³´ ì¤€ë¹„
            pipeline_results = {
                'collected_symbols': main_table.get('symbol_count', 0),
                'collected_ohlcv': main_table.get('record_count', 0),
                'calculated_indicators': 0,  # ê¸°ìˆ ì  ì§€í‘œëŠ” ë³„ë„ ê³„ì‚°
                'scanned_watchlist': 0,     # ê´€ì‹¬ì¢…ëª©ì€ Redisì—ì„œ ë³„ë„ ê´€ë¦¬
                'data_source': 'bulk_collection',
                'data_quality': completion_info.get('validation_results', {})
            }
            
            completion_info['pipeline_results'] = pipeline_results
            
            # XComì— ì •ë³´ ì €ì¥
            kwargs['ti'].xcom_push(key='pipeline_completion_info', value=completion_info)
            
            return completion_info
        
        # 2. ê¸°ì¡´ ì¼ë°˜ íŒŒì´í”„ë¼ì¸ í”Œë˜ê·¸ í™•ì¸ (ë°±ì—…)
        flag_file = "/tmp/nasdaq_pipeline_complete.flag"
        
        if os.path.exists(flag_file):
            with open(flag_file, 'r') as f:
                completion_info = json.loads(f.read())
            
            print(f"ğŸ“Š ì¼ë°˜ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì •ë³´:")
            print(f"   ì™„ë£Œ ì‹œê°„: {completion_info['completion_time']}")
            print(f"   DAG Run ID: {completion_info['dag_run_id']}")
            
            results = completion_info.get('pipeline_results', {})
            print(f"   ì²˜ë¦¬ ì‹¬ë³¼ ìˆ˜: {results.get('collected_symbols', 0)}")
            print(f"   ìˆ˜ì§‘ OHLCV: {results.get('collected_ohlcv', 0)}")
            print(f"   ê³„ì‚° ì§€í‘œ: {results.get('calculated_indicators', 0)}")
            print(f"   ê´€ì‹¬ì¢…ëª© ìˆ˜: {results.get('scanned_watchlist', 0)}")
            
            # XComì— ì •ë³´ ì €ì¥
            kwargs['ti'].xcom_push(key='pipeline_completion_info', value=completion_info)
            
            return completion_info
        
        # 3. ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì˜¤ë¥˜
        raise FileNotFoundError("íŒŒì´í”„ë¼ì¸ ì™„ë£Œ í”Œë˜ê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì •ë³´ ì½ê¸° ì‹¤íŒ¨: {e}")
        raise

def redis_smart_sync_task(**kwargs):
    """ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰"""
    try:
        from load_watchlist_to_redis import WatchlistDataLoader
        
        print("ğŸ§  Redis ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™” ì‹œì‘...")
        
        # ì‹¤í–‰ ëª¨ë“œ ê²°ì • (execution_date ê¸°ë°˜)
        execution_date = kwargs['execution_date']
        current_date = datetime.now()
        
        # ì£¼ë§ì´ë©´ ì „ì²´ ì¬ë™ê¸°í™”
        is_weekend = execution_date.weekday() in [5, 6]  # í† , ì¼
        force_full = kwargs.get('dag_run').conf.get('force_full', False) if kwargs.get('dag_run') and kwargs.get('dag_run').conf else False
        
        loader = WatchlistDataLoader()
        
        if force_full or is_weekend:
            print("ğŸ”„ ì£¼ë§ ì „ì²´ ì¬ë™ê¸°í™” ëª¨ë“œ")
            success = loader.load_watchlist_to_redis(days_back=30)
            loader.set_last_update_info(1, 1, "weekly_full_sync")
        else:
            print("âš¡ í‰ì¼ ìŠ¤ë§ˆíŠ¸ ì¦ë¶„ ì—…ë°ì´íŠ¸")
            success = loader.smart_incremental_update()
        
        if success:
            print("âœ… Redis ë™ê¸°í™” ì„±ê³µ!")
            return "success"
        else:
            print("âŒ Redis ë™ê¸°í™” ì‹¤íŒ¨!")
            raise Exception("Redis ë™ê¸°í™” ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"ğŸ’¥ Redis ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
        raise

def redis_health_check_task(**kwargs):
    """Redis ìƒíƒœ ë° ë°ì´í„° ê²€ì¦"""
    try:
        from redis_client import RedisClient
        
        redis_client = RedisClient()
        
        # 1. Redis ì—°ê²° í™•ì¸
        info = redis_client.redis_client.info()
        print(f"ğŸ“Š Redis ì„œë²„ ìƒíƒœ: {info.get('redis_version', 'Unknown')}")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {info.get('used_memory_human', 'Unknown')}")
        
        # 2. ê´€ì‹¬ì¢…ëª© ë°ì´í„° ê²€ì¦
        watchlist_keys = redis_client.redis_client.keys("watchlist_data:*")
        signal_keys = redis_client.redis_client.keys("signal_trigger:*")
        
        print(f"ğŸ¯ ê´€ì‹¬ì¢…ëª© ë°ì´í„°: {len(watchlist_keys)}ê°œ")
        print(f"ğŸ“ˆ í™œì„± ì‹ í˜¸: {len(signal_keys)}ê°œ")
        
        # 3. ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ ê²€ì¦
        if len(watchlist_keys) < 10:
            raise Exception(f"ê´€ì‹¬ì¢…ëª© ë°ì´í„° ë¶€ì¡±: {len(watchlist_keys)}ê°œ (ìµœì†Œ 10ê°œ í•„ìš”)")
        
        # 4. ìƒ˜í”Œ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        if watchlist_keys:
            sample_key = watchlist_keys[0]
            sample_data = redis_client.redis_client.get(sample_key)
            if not sample_data:
                raise Exception("ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨")
        
        print("âœ… Redis ìƒíƒœ ê²€ì¦ ì™„ë£Œ!")
        return "healthy"
        
    except Exception as e:
        print(f"âŒ Redis ìƒíƒœ ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise

def signal_detection_prepare_task(**kwargs):
    """ì‹ í˜¸ ê°ì§€ ì‹œìŠ¤í…œ ì¤€ë¹„"""
    try:
        from redis_client import RedisClient
        
        redis_client = RedisClient()
        
        # ì‹ í˜¸ ê°ì§€ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        watchlist_keys = redis_client.redis_client.keys("watchlist_data:*")
        
        signal_ready_symbols = []
        for key in watchlist_keys:
            symbol = key.decode('utf-8').split(':')[1]
            
            # ê° ì¢…ëª©ì˜ ë°ì´í„° ì¶©ë¶„ì„± í™•ì¸
            data = redis_client.get_watchlist_data(symbol)
            if data and data.get('historical_data'):
                historical_data = data['historical_data']
                if len(historical_data) >= 20:  # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ì„ ìœ„í•œ ìµœì†Œ ë°ì´í„°
                    signal_ready_symbols.append(symbol)
        
        print(f"ğŸš¨ ì‹ í˜¸ ê°ì§€ ì¤€ë¹„ ì™„ë£Œ: {len(signal_ready_symbols)}ê°œ ì¢…ëª©")
        
        # ì‹ í˜¸ ê°ì§€ ì¤€ë¹„ ìƒíƒœ ì €ì¥
        ready_status = {
            'timestamp': datetime.now().isoformat(),
            'ready_symbols_count': len(signal_ready_symbols),
            'total_symbols': len(watchlist_keys),
            'readiness_rate': len(signal_ready_symbols) / len(watchlist_keys) if watchlist_keys else 0
        }
        
        redis_client.redis_client.setex(
            "signal_detection_ready",
            3600,  # 1ì‹œê°„ TTL
            str(ready_status)
        )
        
        return "prepared"
        
    except Exception as e:
        print(f"âŒ ì‹ í˜¸ ê°ì§€ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        raise

def cleanup_old_data_task(**kwargs):
    """ì˜¤ë˜ëœ Redis ë°ì´í„° ì •ë¦¬"""
    try:
        from redis_client import RedisClient
        
        redis_client = RedisClient()
        
        # 1. ì˜¤ë˜ëœ ì‹ í˜¸ ë°ì´í„° ì •ë¦¬ (7ì¼ ì´ìƒ)
        old_signal_keys = redis_client.redis_client.keys("signal_trigger:*")
        cleaned_signals = 0
        
        cutoff_date = datetime.now() - timedelta(days=7)
        
        for key in old_signal_keys:
            try:
                # í‚¤ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œí•˜ì—¬ í™•ì¸
                key_str = key.decode('utf-8')
                # signal_trigger:SYMBOL:TIMESTAMP í˜•ì‹
                parts = key_str.split(':')
                if len(parts) >= 3:
                    timestamp_str = parts[2]
                    signal_date = datetime.fromisoformat(timestamp_str[:19])
                    
                    if signal_date < cutoff_date:
                        redis_client.redis_client.delete(key)
                        cleaned_signals += 1
            except:
                continue
        
        # 2. ì„ì‹œ ë¶„ì„ ë°ì´í„° ì •ë¦¬
        temp_keys = redis_client.redis_client.keys("temp_*")
        for key in temp_keys:
            redis_client.redis_client.delete(key)
        
        print(f"ğŸ§¹ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ:")
        print(f"   ğŸ“ˆ ì •ë¦¬ëœ ì˜¤ë˜ëœ ì‹ í˜¸: {cleaned_signals}ê°œ")
        print(f"   ğŸ—‘ï¸ ì •ë¦¬ëœ ì„ì‹œ ë°ì´í„°: {len(temp_keys)}ê°œ")
        
        return "cleaned"
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {e}")
        raise

# 1. ëŒ€ëŸ‰ ìˆ˜ì§‘ ë˜ëŠ” ì¼ë°˜ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ëŒ€ê¸°
wait_for_pipeline = FileSensor(
    task_id='wait_for_pipeline_completion',
    filepath='/tmp/nasdaq_bulk_collection_complete.flag',  # ìš°ì„  ëŒ€ëŸ‰ ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸
    fs_conn_id='fs_default',
    poke_interval=300,  # 5ë¶„ë§ˆë‹¤ í™•ì¸
    timeout=7200,  # 2ì‹œê°„ íƒ€ì„ì•„ì›ƒ (ëŒ€ëŸ‰ ìˆ˜ì§‘ì€ ì˜¤ë˜ ê±¸ë¦¼)
    soft_fail=True,   # ì‹¤íŒ¨ ì‹œ ì¼ë°˜ íŒŒì´í”„ë¼ì¸ í”Œë˜ê·¸ë¡œ ëŒ€ì²´
    dag=dag
)

# 1-1. ë°±ì—…: ì¼ë°˜ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ëŒ€ê¸° (ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ)
wait_for_nasdaq_backup = FileSensor(
    task_id='wait_for_nasdaq_pipeline_backup',
    filepath='/tmp/nasdaq_pipeline_complete.flag',
    fs_conn_id='fs_default',
    poke_interval=300,
    timeout=3600,
    soft_fail=False,
    trigger_rule='one_failed',  # ëŒ€ëŸ‰ ìˆ˜ì§‘ ëŒ€ê¸°ê°€ ì‹¤íŒ¨í–ˆì„ ë•Œë§Œ ì‹¤í–‰
    dag=dag
)

# 2. íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì •ë³´ ì½ê¸°
read_completion_info = PythonOperator(
    task_id='read_pipeline_completion_info',
    python_callable=read_pipeline_completion_info,
    provide_context=True,
    dag=dag
)

# 3. Redis ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™”
redis_sync = PythonOperator(
    task_id='redis_smart_sync',
    python_callable=redis_smart_sync_task,
    provide_context=True,
    dag=dag
)

# 3. Redis ìƒíƒœ ê²€ì¦
redis_health_check = PythonOperator(
    task_id='redis_health_check',
    python_callable=redis_health_check_task,
    provide_context=True,
    dag=dag
)

# 4. ì‹ í˜¸ ê°ì§€ ì‹œìŠ¤í…œ ì¤€ë¹„
signal_prepare = PythonOperator(
    task_id='prepare_signal_detection',
    python_callable=signal_detection_prepare_task,
    provide_context=True,
    dag=dag
)

# 5. ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
cleanup_data = PythonOperator(
    task_id='cleanup_old_data',
    python_callable=cleanup_old_data_task,
    provide_context=True,
    dag=dag
)

# 6. ì„±ê³µ ì•Œë¦¼ (ì„ íƒì )
success_notification = BashOperator(
    task_id='success_notification',
    bash_command='''
    echo "âœ… Redis ê´€ì‹¬ì¢…ëª© ë™ê¸°í™” ì™„ë£Œ!"
    echo "ğŸ• ì™„ë£Œ ì‹œê°„: $(date)"
    echo "ğŸ“Š ë¡œê·¸ëŠ” Airflow UIì—ì„œ í™•ì¸í•˜ì„¸ìš”."
    ''',
    dag=dag
)

# Task ì˜ì¡´ì„± ì„¤ì • - ìˆœì°¨ì  ì‹¤í–‰
[wait_for_pipeline, wait_for_nasdaq_backup] >> read_completion_info >> redis_sync >> redis_health_check >> [signal_prepare, cleanup_data] >> success_notification

# ìˆ˜ë™ ì‹¤í–‰ì„ ìœ„í•œ ë…ë¦½ì ì¸ task
manual_full_sync = PythonOperator(
    task_id='manual_full_sync',
    python_callable=lambda **kwargs: redis_smart_sync_task(force_full=True, **kwargs),
    provide_context=True,
    dag=dag
)

if __name__ == "__main__":
    dag.cli()
