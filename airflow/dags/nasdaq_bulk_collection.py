#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ìµœì í™”ëœ ë‚˜ìŠ¤ë‹¥ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ DAG
- FinanceDataReader ê¸°ë°˜ 5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘
- HDD ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
- Parquet ê¸°ë°˜ ê³ ì† ë¡œë“œ
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta
import sys
import os

# ê³µí†µ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, '/opt/airflow/common')
sys.path.insert(0, '/opt/airflow/plugins')

# ê¸°ë³¸ ì¸ìˆ˜ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'max_active_runs': 1,
    'execution_timeout': timedelta(hours=2),  # 2ì‹œê°„ íƒ€ì„ì•„ì›ƒ
    'task_concurrency': 1  # íƒœìŠ¤í¬ ë™ì‹œ ì‹¤í–‰ ì œí•œ
}

# DAG ì •ì˜
dag = DAG(
    'nasdaq_bulk_data_collection',
    default_args=default_args,
    description='ğŸš€ ë‚˜ìŠ¤ë‹¥ 5ë…„ì¹˜ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (FinanceDataReader + ìµœì í™”)',
    schedule_interval='0 2 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ 02:00 (ì£¼ë§ ëŒ€ëŸ‰ ì²˜ë¦¬)
    catchup=False,
    max_active_runs=1,
    tags=['nasdaq', 'bulk-collection', 'financedatareader', 'parquet', 'optimized']
)

def check_system_resources(**kwargs):
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ë° ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘ í”Œë˜ê·¸ ìƒì„±"""
    import psutil
    import shutil
    from utils.dag_coordination import create_bulk_collection_running_flag
    
    # ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘ í”Œë˜ê·¸ ìƒì„±
    create_bulk_collection_running_flag(**kwargs)
    
    # ë©”ëª¨ë¦¬ í™•ì¸
    memory = psutil.virtual_memory()
    available_memory_gb = memory.available / (1024 ** 3)
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    disk_usage = shutil.disk_usage('/data')
    available_disk_gb = disk_usage.free / (1024 ** 3)
    
    print(f"ğŸ’¾ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ:")
    print(f"   ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {available_memory_gb:.1f} GB")
    print(f"   ì‚¬ìš© ê°€ëŠ¥ ë””ìŠ¤í¬: {available_disk_gb:.1f} GB")
    print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent:.1f}%")
    
    # ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    if available_memory_gb < 1.0:
        raise Exception(f"ë©”ëª¨ë¦¬ ë¶€ì¡±: {available_memory_gb:.1f}GB (ìµœì†Œ 1GB í•„ìš”)")
    
    if available_disk_gb < 50.0:
        raise Exception(f"ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {available_disk_gb:.1f}GB (ìµœì†Œ 50GB í•„ìš”)")
    
    return {
        'available_memory_gb': available_memory_gb,
        'available_disk_gb': available_disk_gb,
        'memory_usage_percent': memory.percent
    }

def initialize_optimized_storage(**kwargs):
    """ìµœì í™”ëœ ì €ì¥ì†Œ ì´ˆê¸°í™”"""
    from batch_scheduler import DuckDBBatchScheduler, CheckpointConfig
    from parquet_loader import ParquetDataLoader
    
    # ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (HDD ìµœì í™”)
    config = CheckpointConfig(
        interval_seconds=300,        # 5ë¶„ë§ˆë‹¤ ì²´í¬
        wal_size_threshold_mb=200,   # 200MB WAL ì„ê³„ê°’
        memory_threshold_percent=75, # 75% ë©”ëª¨ë¦¬ ì„ê³„ê°’
        force_checkpoint_interval=1800  # 30ë¶„ë§ˆë‹¤ ê°•ì œ ì²´í¬í¬ì¸íŠ¸
    )
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    scheduler = DuckDBBatchScheduler("/data/duckdb/stock_data.db", config)
    scheduler.start()
    
    # Parquet ë¡œë” ì´ˆê¸°í™”
    parquet_loader = ParquetDataLoader("/data/parquet_storage")
    
    # Append-only êµ¬ì¡° ìƒì„±
    success = parquet_loader.create_append_only_structure(
        "/data/duckdb/stock_data.db", 
        "stock_data"
    )
    
    if not success:
        raise Exception("Append-only êµ¬ì¡° ìƒì„± ì‹¤íŒ¨")
    
    print("âœ… ìµœì í™”ëœ ì €ì¥ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # XComì— ì„¤ì • ì •ë³´ ì €ì¥
    kwargs['ti'].xcom_push(key='scheduler_config', value=config.__dict__)
    kwargs['ti'].xcom_push(key='storage_initialized', value=True)
    
    return success

def collect_nasdaq_symbols_bulk(**kwargs):
    """ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ëŒ€ëŸ‰ ìˆ˜ì§‘"""
    from bulk_data_collector import BulkDataCollector
    
    try:
        # ëŒ€ëŸ‰ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = BulkDataCollector(
            db_path="/data/duckdb/stock_data.db",
            batch_size=2000,  # HDD í™˜ê²½ì— ë§ì¶° ì¡°ì •
            max_workers=3     # ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤
        )
        
        print("ğŸ“Š ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì¤‘...")
        
        # ì „ì²´ ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ìˆ˜ì§‘
        symbols = collector.get_nasdaq_symbols()
        
        if not symbols:
            raise Exception("ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        print(f"âœ… ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ: {len(symbols)}ê°œ")
        
        # XComì— ì €ì¥ (ë‹¤ìŒ taskì—ì„œ ì‚¬ìš©)
        kwargs['ti'].xcom_push(key='nasdaq_symbols', value=symbols)
        kwargs['ti'].xcom_push(key='symbol_count', value=len(symbols))
        
        return len(symbols)
        
    except Exception as e:
        print(f"âŒ ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        if 'collector' in locals():
            collector.close()

def collect_5year_data_batch_1(**kwargs):
    """5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ - ë°°ì¹˜ 1 (ìƒìœ„ 1/3 ì¢…ëª©)"""
    return _collect_data_batch(kwargs, batch_num=1, total_batches=3)

def collect_5year_data_batch_2(**kwargs):
    """5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ - ë°°ì¹˜ 2 (ì¤‘ê°„ 1/3 ì¢…ëª©)"""
    return _collect_data_batch(kwargs, batch_num=2, total_batches=3)

def collect_5year_data_batch_3(**kwargs):
    """5ë…„ì¹˜ ë°ì´í„° ìˆ˜ì§‘ - ë°°ì¹˜ 3 (í•˜ìœ„ 1/3 ì¢…ëª©)"""
    return _collect_data_batch(kwargs, batch_num=3, total_batches=3)

def _collect_data_batch(kwargs, batch_num: int, total_batches: int):
    """ë°ì´í„° ë°°ì¹˜ ìˆ˜ì§‘ ê³µí†µ í•¨ìˆ˜"""
    from bulk_data_collector import BulkDataCollector
    from parquet_loader import ParquetDataLoader
    import math
    
    try:
        # ì´ì „ taskì—ì„œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        symbols = kwargs['ti'].xcom_pull(key='nasdaq_symbols', task_ids='collect_nasdaq_symbols')
        
        if not symbols:
            raise Exception("ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        # ë°°ì¹˜ ë¶„í• 
        batch_size = math.ceil(len(symbols) / total_batches)
        start_idx = (batch_num - 1) * batch_size
        end_idx = min(start_idx + batch_size, len(symbols))
        batch_symbols = symbols[start_idx:end_idx]
        
        print(f"ğŸ”„ ë°°ì¹˜ {batch_num} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘:")
        print(f"   ëŒ€ìƒ ì¢…ëª©: {len(batch_symbols)}ê°œ ({start_idx+1}-{end_idx})")
        print(f"   ìˆ˜ì§‘ ê¸°ê°„: 2020-01-01 ~ í˜„ì¬ (ì•½ 5ë…„)")
        
        # ëŒ€ëŸ‰ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” (ë” ë³´ìˆ˜ì ì¸ ì„¤ì •)
        collector = BulkDataCollector(
            db_path="/data/duckdb/stock_data.db",
            batch_size=500,   # ë°°ì¹˜ í¬ê¸°ë¥¼ ë” ì‘ê²Œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            max_workers=1     # ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ ì•ˆì •ì„± í™•ë³´
        )
        
        # Parquet ë¡œë” ì´ˆê¸°í™”
        parquet_loader = ParquetDataLoader("/data/parquet_storage")
        
        # ë°ì´í„° ìˆ˜ì§‘ (5ë…„ì¹˜)
        success = collector.collect_bulk_data(
            symbols=batch_symbols,
            start_date='2020-01-01',  # 5ë…„ ì „
            batch_symbols=20  # í•œ ë²ˆì— 20ê°œì”© ì²˜ë¦¬
        )
        
        if not success:
            raise Exception(f"ë°°ì¹˜ {batch_num} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        # ìˆ˜ì§‘ í†µê³„
        stats = collector.get_collection_stats()
        print(f"âœ… ë°°ì¹˜ {batch_num} ìˆ˜ì§‘ ì™„ë£Œ:")
        print(f"   ì¢…ëª© ìˆ˜: {stats.get('symbol_count', 0)}")
        print(f"   ì´ ë ˆì½”ë“œ: {stats.get('total_records', 0)}")
        print(f"   ë‚ ì§œ ë²”ìœ„: {stats.get('date_range', {})}")
        
        # XComì— ê²°ê³¼ ì €ì¥
        kwargs['ti'].xcom_push(key=f'batch_{batch_num}_stats', value=stats)
        
        return stats
        
    except Exception as e:
        print(f"âŒ ë°°ì¹˜ {batch_num} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        if 'collector' in locals():
            collector.close()

def merge_and_optimize_data(**kwargs):
    """ìˆ˜ì§‘ëœ ë°ì´í„° ë³‘í•© ë° ìµœì í™”"""
    from parquet_loader import ParquetDataLoader
    from batch_scheduler import DuckDBBatchScheduler, CheckpointConfig
    
    try:
        print("ğŸ”§ ë°ì´í„° ë³‘í•© ë° ìµœì í™” ì‹œì‘...")
        
        # Parquet ë¡œë” ì´ˆê¸°í™”
        parquet_loader = ParquetDataLoader("/data/parquet_storage")
        
        # ëª¨ë“  ë°°ì¹˜ë¥¼ ë©”ì¸ í…Œì´ë¸”ë¡œ ë³‘í•©
        success = parquet_loader.merge_batches_to_main(
            table_name="stock_data",
            db_path="/data/duckdb/stock_data.db",
            max_batches=50  # ìµœëŒ€ 50ê°œ ë°°ì¹˜ê¹Œì§€ í•œ ë²ˆì— ë³‘í•©
        )
        
        if not success:
            raise Exception("ë°°ì¹˜ ë³‘í•© ì‹¤íŒ¨")
        
        # ì €ì¥ì†Œ ìµœì í™” (ì¸ë±ìŠ¤, ì••ì¶• ë“±)
        parquet_loader.optimize_storage(
            table_name="stock_data",
            db_path="/data/duckdb/stock_data.db"
        )
        
        # Parquet ì•„ì¹´ì´ë¸Œ ìƒì„± (ë°±ì—…)
        archive_path = parquet_loader.export_to_parquet_archive(
            table_name="stock_data",
            db_path="/data/duckdb/stock_data.db",
            partition_by="date"
        )
        
        # ìµœì¢… í†µê³„
        final_stats = parquet_loader.get_storage_stats(
            table_name="stock_data", 
            db_path="/data/duckdb/stock_data.db"
        )
        
        print("âœ… ë°ì´í„° ë³‘í•© ë° ìµœì í™” ì™„ë£Œ:")
        print(f"   ë©”ì¸ í…Œì´ë¸” ë ˆì½”ë“œ: {final_stats.get('main_table', {}).get('record_count', 0)}")
        print(f"   ì¢…ëª© ìˆ˜: {final_stats.get('main_table', {}).get('symbol_count', 0)}")
        print(f"   ì•„ì¹´ì´ë¸Œ ê²½ë¡œ: {archive_path}")
        
        # XComì— ê²°ê³¼ ì €ì¥
        kwargs['ti'].xcom_push(key='final_stats', value=final_stats)
        kwargs['ti'].xcom_push(key='archive_path', value=archive_path)
        
        return final_stats
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë³‘í•© ë° ìµœì í™” ì‹¤íŒ¨: {e}")
        raise

def validate_data_quality(**kwargs):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    import duckdb
    from datetime import datetime, timedelta
    
    try:
        print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        with duckdb.connect("/data/duckdb/stock_data.db") as conn:
            # 1. ê¸°ë³¸ í†µê³„
            basic_stats = conn.execute("""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT symbol) as unique_symbols,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM stock_data
            """).fetchone()
            
            # 2. ë°ì´í„° ì™„ì •ì„± í™•ì¸
            data_quality = conn.execute("""
                SELECT 
                    COUNT(CASE WHEN open IS NULL THEN 1 END) as null_open,
                    COUNT(CASE WHEN high IS NULL THEN 1 END) as null_high,
                    COUNT(CASE WHEN low IS NULL THEN 1 END) as null_low,
                    COUNT(CASE WHEN close IS NULL THEN 1 END) as null_close,
                    COUNT(CASE WHEN volume IS NULL THEN 1 END) as null_volume
                FROM stock_data
            """).fetchone()
            
            # 3. ë…¼ë¦¬ì  ì˜¤ë¥˜ í™•ì¸
            logical_errors = conn.execute("""
                SELECT 
                    COUNT(CASE WHEN high < low THEN 1 END) as invalid_high_low,
                    COUNT(CASE WHEN open < 0 OR high < 0 OR low < 0 OR close < 0 THEN 1 END) as negative_prices,
                    COUNT(CASE WHEN volume < 0 THEN 1 END) as negative_volume
                FROM stock_data
            """).fetchone()
            
            # 4. ìµœê·¼ ë°ì´í„° í™•ì¸
            recent_data = conn.execute("""
                SELECT COUNT(DISTINCT symbol) as symbols_with_recent_data
                FROM stock_data 
                WHERE date >= current_date - INTERVAL 30 DAY
            """).fetchone()
        
        # ê²€ì¦ ê²°ê³¼ ì •ë¦¬
        validation_results = {
            'basic_stats': {
                'total_records': basic_stats[0],
                'unique_symbols': basic_stats[1],
                'date_range': {
                    'start': basic_stats[2],
                    'end': basic_stats[3]
                }
            },
            'data_quality': {
                'null_counts': {
                    'open': data_quality[0],
                    'high': data_quality[1],
                    'low': data_quality[2],
                    'close': data_quality[3],
                    'volume': data_quality[4]
                }
            },
            'logical_errors': {
                'invalid_high_low': logical_errors[0],
                'negative_prices': logical_errors[1],
                'negative_volume': logical_errors[2]
            },
            'recent_data': {
                'symbols_with_recent_data': recent_data[0]
            }
        }
        
        # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
        quality_issues = []
        
        if validation_results['basic_stats']['total_records'] < 100000:
            quality_issues.append("ì´ ë ˆì½”ë“œ ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ")
        
        if validation_results['basic_stats']['unique_symbols'] < 100:
            quality_issues.append("ìˆ˜ì§‘ëœ ì¢…ëª© ìˆ˜ê°€ ë„ˆë¬´ ì ìŒ")
        
        if validation_results['logical_errors']['invalid_high_low'] > 0:
            quality_issues.append("ê³ ê°€ < ì €ê°€ì¸ ì˜ëª»ëœ ë°ì´í„° ì¡´ì¬")
        
        if validation_results['logical_errors']['negative_prices'] > 0:
            quality_issues.append("ìŒìˆ˜ ê°€ê²© ë°ì´í„° ì¡´ì¬")
        
        print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
        print(f"   ì´ ë ˆì½”ë“œ: {validation_results['basic_stats']['total_records']:,}")
        print(f"   ì¢…ëª© ìˆ˜: {validation_results['basic_stats']['unique_symbols']:,}")
        print(f"   ë‚ ì§œ ë²”ìœ„: {validation_results['basic_stats']['date_range']['start']} ~ {validation_results['basic_stats']['date_range']['end']}")
        print(f"   ìµœê·¼ ë°ì´í„° ì¢…ëª©: {validation_results['recent_data']['symbols_with_recent_data']}")
        
        if quality_issues:
            print("âš ï¸ í’ˆì§ˆ ì´ìŠˆ:")
            for issue in quality_issues:
                print(f"   - {issue}")
        else:
            print("âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í†µê³¼")
        
        # XComì— ê²€ì¦ ê²°ê³¼ ì €ì¥
        kwargs['ti'].xcom_push(key='validation_results', value=validation_results)
        kwargs['ti'].xcom_push(key='quality_issues', value=quality_issues)
        
        return validation_results
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise

def create_completion_flag(**kwargs):
    """ì™„ë£Œ í”Œë˜ê·¸ ìƒì„± ë° ì‹¤í–‰ ì¤‘ í”Œë˜ê·¸ ì œê±°"""
    import json
    from utils.dag_coordination import remove_bulk_collection_running_flag
    
    try:
        # ì‹¤í–‰ ì¤‘ í”Œë˜ê·¸ ì œê±°
        remove_bulk_collection_running_flag(**kwargs)
        
        # ëª¨ë“  í†µê³„ ì •ë³´ ìˆ˜ì§‘
        final_stats = kwargs['ti'].xcom_pull(key='final_stats', task_ids='merge_and_optimize_data')
        validation_results = kwargs['ti'].xcom_pull(key='validation_results', task_ids='validate_data_quality')
        
        completion_info = {
            'completion_time': datetime.now().isoformat(),
            'dag_run_id': kwargs['dag_run'].run_id,
            'collection_type': 'bulk_5year_data',
            'final_stats': final_stats,
            'validation_results': validation_results,
            'success': True
        }
        
        # ì™„ë£Œ í”Œë˜ê·¸ íŒŒì¼ ìƒì„±
        flag_file = "/tmp/nasdaq_bulk_collection_complete.flag"
        with open(flag_file, 'w') as f:
            json.dump(completion_info, f, indent=2, default=str)
        
        print(f"âœ… ì™„ë£Œ í”Œë˜ê·¸ ìƒì„±: {flag_file}")
        return completion_info
        
    except Exception as e:
        print(f"âŒ ì™„ë£Œ í”Œë˜ê·¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# Task ì •ì˜
check_resources = PythonOperator(
    task_id='check_system_resources',
    python_callable=check_system_resources,
    provide_context=True,
    dag=dag
)

init_storage = PythonOperator(
    task_id='initialize_optimized_storage',
    python_callable=initialize_optimized_storage,
    provide_context=True,
    dag=dag
)

collect_symbols = PythonOperator(
    task_id='collect_nasdaq_symbols',
    python_callable=collect_nasdaq_symbols_bulk,
    provide_context=True,
    dag=dag
)

# ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ (3ê°œ ë°°ì¹˜)
collect_batch_1 = PythonOperator(
    task_id='collect_5year_data_batch_1',
    python_callable=collect_5year_data_batch_1,
    provide_context=True,
    dag=dag
)

collect_batch_2 = PythonOperator(
    task_id='collect_5year_data_batch_2',
    python_callable=collect_5year_data_batch_2,
    provide_context=True,
    dag=dag
)

collect_batch_3 = PythonOperator(
    task_id='collect_5year_data_batch_3',
    python_callable=collect_5year_data_batch_3,
    provide_context=True,
    dag=dag
)

# ë³‘í•© ë° ìµœì í™”
merge_optimize = PythonOperator(
    task_id='merge_and_optimize_data',
    python_callable=merge_and_optimize_data,
    provide_context=True,
    dag=dag
)

# ë°ì´í„° í’ˆì§ˆ ê²€ì¦
validate_quality = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    provide_context=True,
    dag=dag
)

# ì™„ë£Œ í”Œë˜ê·¸ ìƒì„±
create_flag = PythonOperator(
    task_id='create_completion_flag',
    python_callable=create_completion_flag,
    provide_context=True,
    dag=dag
)

# ì„±ê³µ ì•Œë¦¼
success_notification = BashOperator(
    task_id='success_notification',
    bash_command='''
    echo "ğŸ‰ ë‚˜ìŠ¤ë‹¥ 5ë…„ì¹˜ ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
    echo "ğŸ• ì™„ë£Œ ì‹œê°„: $(date)"
    echo "ğŸ’¾ ì €ì¥ ìœ„ì¹˜: /data/duckdb/stock_data.db"
    echo "ğŸ“ˆ Parquet ì•„ì¹´ì´ë¸Œ: /data/parquet_storage/archive/"
    echo "ğŸ“Š ìì„¸í•œ ê²°ê³¼ëŠ” Airflow UIì—ì„œ í™•ì¸í•˜ì„¸ìš”."
    ''',
    dag=dag
)

# Task ì˜ì¡´ì„± ì„¤ì • - ë°°ì¹˜ ìˆœì°¨ ì‹¤í–‰ìœ¼ë¡œ DuckDB ë½ ì¶©ëŒ ë°©ì§€
check_resources >> init_storage >> collect_symbols >> collect_batch_1 >> collect_batch_2 >> collect_batch_3 >> merge_optimize >> validate_quality >> create_flag >> success_notification

if __name__ == "__main__":
    dag.cli()
