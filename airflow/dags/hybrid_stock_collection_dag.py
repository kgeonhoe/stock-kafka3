#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
í†µí•© ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ DAG (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
- ì´ˆê¸° ìˆ˜ì§‘: FinanceDataReaderë¡œ 5ë…„ì¹˜ ëŒ€ëŸ‰ ìˆ˜ì§‘ (1íšŒ ì‹¤í–‰)
- ì¼ë³„ ìš´ì˜: yfinanceë¡œ ì¦ë¶„ ìˆ˜ì§‘ (ë§¤ì¼ ì‹¤í–‰)
- ì™„ë²½í•œ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, '/opt/airflow/common')
sys.path.insert(0, '/home/grey1/stock-kafka3/common')

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'stock-pipeline',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'hybrid_stock_data_collection',
    default_args=default_args,
    description='í•˜ì´ë¸Œë¦¬ë“œ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ (FDR + yfinance)',
    schedule_interval='0 2 * * 1-5',  # í‰ì¼ ì˜¤ì „ 2ì‹œ
    catchup=False,
    max_active_runs=1,
    tags=['stock', 'data-collection', 'hybrid']
)

# ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
DB_PATH = "/data/duckdb/stock_data.db"

def check_initial_data_exists(**context):
    """
    ì´ˆê¸° ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    
    Returns:
        'bulk_collection' ë˜ëŠ” 'daily_collection'
    """
    try:
        import duckdb
        
        with duckdb.connect(DB_PATH) as conn:
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì¡´ì¬ ë° ë°ì´í„° í™•ì¸
            try:
                result = conn.execute("""
                    SELECT COUNT(DISTINCT symbol), MIN(date), MAX(date)
                    FROM stock_data
                """).fetchone()
                
                symbol_count = result[0] if result else 0
                min_date = result[1] if result else None
                max_date = result[2] if result else None
                
                print(f"ğŸ” ê¸°ì¡´ ë°ì´í„° í™•ì¸: {symbol_count}ê°œ ì¢…ëª©, {min_date} ~ {max_date}")
                
                # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¼ë³„ ìˆ˜ì§‘ìœ¼ë¡œ ë¶„ê¸°
                if symbol_count >= 1:  # ì„ì‹œë¡œ 1ê°œ ì¢…ëª©ë§Œ ìˆì–´ë„ daily collection ì‹¤í–‰
                    print("âœ… ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìŒ -> ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘ ì‹¤í–‰")
                    return 'daily_incremental_collection'
                else:
                    print("âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¶€ì¡± -> ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹¤í–‰")
                    return 'bulk_collection_fdr'
                    
            except Exception as e:
                print(f"ğŸ†• ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ -> ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹¤í–‰: {e}")
                return 'bulk_collection_fdr'
                
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸ ì‹¤íŒ¨ -> ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹¤í–‰: {e}")
        return 'bulk_collection_fdr'

def bulk_collection_fdr_task(**context):
    """
    FinanceDataReaderë¡œ 5ë…„ì¹˜ ëŒ€ëŸ‰ ìˆ˜ì§‘
    """
    from bulk_data_collector import BulkDataCollector
    
    print("ğŸš€ FinanceDataReader ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹œì‘ (5ë…„ì¹˜ ë°ì´í„°)")
    
    try:
        # ëŒ€ëŸ‰ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = BulkDataCollector(
            db_path=DB_PATH,
            batch_size=1000,
            max_workers=4
        )
        
        # ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        symbols = collector.get_nasdaq_symbols(limit=None)  # ì „ì²´ ì¢…ëª©
        
        if not symbols:
            raise ValueError("ë‚˜ìŠ¤ë‹¥ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        print(f"ğŸ“Š ìˆ˜ì§‘ ëŒ€ìƒ: {len(symbols)}ê°œ ì¢…ëª©")
        
        # ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ (5ë…„ì¹˜)
        success = collector.collect_bulk_data(
            symbols=symbols,
            start_date='2019-01-01',  # 5ë…„ì¹˜
            batch_symbols=50
        )
        
        if not success:
            raise ValueError("ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
        print("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì¤‘...")
        collector.optimize_database()
        
        # í†µê³„ í™•ì¸
        stats = collector.get_collection_stats()
        print(f"ğŸ“ˆ ìˆ˜ì§‘ ì™„ë£Œ í†µê³„: {stats}")
        
        # XComì— ê²°ê³¼ ì €ì¥
        context['task_instance'].xcom_push(
            key='bulk_collection_result',
            value={
                'status': 'success',
                'symbols_collected': len(symbols),
                'statistics': stats
            }
        )
        
        print("âœ… FinanceDataReader ëŒ€ëŸ‰ ìˆ˜ì§‘ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ FinanceDataReader ëŒ€ëŸ‰ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        if 'collector' in locals():
            collector.close()

def daily_incremental_collection_task(**context):
    """
    yfinanceë¡œ ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘
    """
    from daily_incremental_collector import DailyIncrementalCollector
    
    print("ğŸ”„ yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        # ì¦ë¶„ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        collector = DailyIncrementalCollector(
            db_path=DB_PATH,
            max_workers=5,
            batch_size=50
        )
        
        # ì¼ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤í–‰
        result = collector.collect_daily_batch()
        
        if result['status'] != 'completed':
            raise ValueError(f"ì¼ë³„ ìˆ˜ì§‘ ì‹¤íŒ¨: {result.get('reason', 'unknown')}")
        
        print(f"ğŸ“Š ì¼ë³„ ìˆ˜ì§‘ ê²°ê³¼: {result['symbols_success']}/{result['symbols_attempted']} ì„±ê³µ")
        
        # í†µê³„ í™•ì¸
        stats = collector.get_collection_stats()
        print(f"ğŸ“ˆ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„: {stats}")
        
        # XComì— ê²°ê³¼ ì €ì¥
        context['task_instance'].xcom_push(
            key='daily_collection_result',
            value={
                'status': 'success',
                'collection_stats': result,
                'database_stats': stats
            }
        )
        
        print("âœ… yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        if 'collector' in locals():
            collector.close()

def validate_data_consistency_task(**context):
    """
    ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
    """
    import duckdb
    
    print("ğŸ” ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
    
    try:
        with duckdb.connect(DB_PATH) as conn:
            # 1. ê¸°ë³¸ í†µê³„
            basic_stats = conn.execute("""
                SELECT 
                    COUNT(DISTINCT symbol) as symbol_count,
                    COUNT(*) as total_records,
                    MIN(date) as earliest_date,
                    MAX(date) as latest_date
                FROM stock_data
            """).fetchone()
            
            print(f"ğŸ“Š ê¸°ë³¸ í†µê³„: {basic_stats[0]}ê°œ ì¢…ëª©, {basic_stats[1]}ê°œ ë ˆì½”ë“œ")
            print(f"ğŸ“… ë‚ ì§œ ë²”ìœ„: {basic_stats[2]} ~ {basic_stats[3]}")
            
            # 2. ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
            quality_check = conn.execute("""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(CASE WHEN close IS NULL OR close <= 0 THEN 1 END) as invalid_close,
                    COUNT(CASE WHEN volume < 0 THEN 1 END) as invalid_volume,
                    COUNT(CASE WHEN open > high OR low > close THEN 1 END) as invalid_ohlc
                FROM stock_data
            """).fetchone()
            
            print(f"ğŸ“‹ ë°ì´í„° í’ˆì§ˆ: ì´ {quality_check[0]}ê°œ ì¤‘")
            print(f"   - ì˜ëª»ëœ ì¢…ê°€: {quality_check[1]}ê°œ")
            print(f"   - ì˜ëª»ëœ ê±°ë˜ëŸ‰: {quality_check[2]}ê°œ") 
            print(f"   - ì˜ëª»ëœ OHLC: {quality_check[3]}ê°œ")
            
            # 3. ìµœì‹  ë°ì´í„° í™•ì¸
            recent_data = conn.execute("""
                SELECT symbol, MAX(date) as last_date
                FROM stock_data
                GROUP BY symbol
                ORDER BY last_date DESC
                LIMIT 5
            """).fetchall()
            
            print("ğŸ“ˆ ìµœì‹  ë°ì´í„° ìƒ˜í”Œ:")
            for symbol, last_date in recent_data:
                print(f"   - {symbol}: {last_date}")
            
            # 4. ëˆ„ë½ ë°ì´í„° í™•ì¸
            missing_check = conn.execute("""
                SELECT 
                    symbol,
                    COUNT(*) as record_count,
                    MAX(date) as last_date
                FROM stock_data
                GROUP BY symbol
                HAVING MAX(date) < CURRENT_DATE - INTERVAL '2 days'
                ORDER BY last_date
                LIMIT 10
            """).fetchall()
            
            if missing_check:
                print("âš ï¸ ìµœì‹  ë°ì´í„°ê°€ ëˆ„ë½ëœ ì¢…ëª©ë“¤:")
                for symbol, count, last_date in missing_check:
                    print(f"   - {symbol}: {count}ê°œ ë ˆì½”ë“œ, ë§ˆì§€ë§‰ ë°ì´í„° {last_date}")
            
            # ê²€ì¦ ê²°ê³¼
            validation_result = {
                'basic_stats': {
                    'symbol_count': basic_stats[0],
                    'total_records': basic_stats[1],
                    'date_range': f"{basic_stats[2]} ~ {basic_stats[3]}"
                },
                'quality_issues': {
                    'invalid_close': quality_check[1],
                    'invalid_volume': quality_check[2],
                    'invalid_ohlc': quality_check[3]
                },
                'missing_data_symbols': len(missing_check)
            }
            
            # XComì— ê²°ê³¼ ì €ì¥
            context['task_instance'].xcom_push(
                key='validation_result',
                value=validation_result
            )
            
            print("âœ… ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ")
            
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise

def create_data_replica_task(**context):
    """
    ì½ê¸° ì „ìš© ë°ì´í„° ë³µì œë³¸ ìƒì„±
    """
    import shutil
    
    print("ğŸ“„ ì½ê¸° ì „ìš© ë°ì´í„° ë³µì œë³¸ ìƒì„± ì¤‘...")
    
    try:
        # ë³µì œë³¸ ê²½ë¡œ
        replica_path = DB_PATH.replace('.db', '_replica.db')
        
        # ê¸°ì¡´ ë³µì œë³¸ ì œê±°
        if os.path.exists(replica_path):
            os.remove(replica_path)
        
        # íŒŒì¼ ë³µì‚¬
        shutil.copy2(DB_PATH, replica_path)
        
        # ì½ê¸° ì „ìš© ê¶Œí•œ ì„¤ì •
        os.chmod(replica_path, 0o444)
        
        print(f"âœ… ë³µì œë³¸ ìƒì„± ì™„ë£Œ: {replica_path}")
        
        # XComì— ê²°ê³¼ ì €ì¥
        context['task_instance'].xcom_push(
            key='replica_path',
            value=replica_path
        )
        
    except Exception as e:
        print(f"âŒ ë³µì œë³¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# íƒœìŠ¤í¬ ì •ì˜
check_data = BranchPythonOperator(
    task_id='check_initial_data',
    python_callable=check_initial_data_exists,
    dag=dag,
    doc_md="""
    ## ğŸ” ì´ˆê¸° ë°ì´í„° í™•ì¸
    
    **ëª©ì **: ê¸°ì¡´ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ ìˆ˜ì§‘ ë°©ì‹ ê²°ì •
    
    **ë¶„ê¸° ë¡œì§**:
    - ê¸°ì¡´ ë°ì´í„° ì¶©ë¶„ (100ê°œ ì´ìƒ ì¢…ëª©) â†’ ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘
    - ê¸°ì¡´ ë°ì´í„° ë¶€ì¡± ë˜ëŠ” ì—†ìŒ â†’ FinanceDataReader ëŒ€ëŸ‰ ìˆ˜ì§‘
    
    **ì¶œë ¥**: 'bulk_collection_fdr' ë˜ëŠ” 'daily_incremental_collection'
    """
)

bulk_collection = PythonOperator(
    task_id='bulk_collection_fdr',
    python_callable=bulk_collection_fdr_task,
    dag=dag,
    doc_md="""
    ## ğŸš€ FinanceDataReader ëŒ€ëŸ‰ ìˆ˜ì§‘
    
    **ëª©ì **: 5ë…„ì¹˜ ê³¼ê±° ë°ì´í„°ë¥¼ ê³ ì†ìœ¼ë¡œ ëŒ€ëŸ‰ ìˆ˜ì§‘
    
    **ì²˜ë¦¬ ê³¼ì •**:
    1. ë‚˜ìŠ¤ë‹¥ ì „ì²´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    2. 5ë…„ì¹˜ OHLCV ë°ì´í„° ë³‘ë ¬ ìˆ˜ì§‘
    3. yfinance í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì €ì¥
    4. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤í–‰
    
    **íŠ¹ì§•**: Adjusted Close ê¸°ì¤€ìœ¼ë¡œ yfinanceì™€ ì™„ë²½ í˜¸í™˜
    """
)

daily_collection = PythonOperator(
    task_id='daily_incremental_collection',
    python_callable=daily_incremental_collection_task,
    dag=dag,
    doc_md="""
    ## ğŸ”„ yfinance ì¼ë³„ ì¦ë¶„ ìˆ˜ì§‘
    
    **ëª©ì **: ê¸°ì¡´ ë°ì´í„°ì— ìµœì‹  ë°ì´í„°ë¥¼ ì¦ë¶„ ì¶”ê°€
    
    **ì²˜ë¦¬ ê³¼ì •**:
    1. ê¸°ì¡´ ì¢…ëª©ë³„ ë§ˆì§€ë§‰ ë°ì´í„° ë‚ ì§œ í™•ì¸
    2. ëˆ„ë½ëœ ë‚ ì§œë¶€í„° í˜„ì¬ê¹Œì§€ yfinanceë¡œ ìˆ˜ì§‘
    3. ì¤‘ë³µ ì œê±° í›„ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸
    
    **íŠ¹ì§•**: auto_adjust=Trueë¡œ ê¸°ì¡´ FDR ë°ì´í„°ì™€ ì¼ê´€ì„± ë³´ì¥
    """
)

validate_data = PythonOperator(
    task_id='validate_data_consistency',
    python_callable=validate_data_consistency_task,
    dag=dag,
    trigger_rule='none_failed_or_skipped',
    doc_md="""
    ## ğŸ” ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
    
    **ëª©ì **: ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆ ë° ì¼ê´€ì„± ê²€ì¦
    
    **ê²€ì¦ í•­ëª©**:
    - ê¸°ë³¸ í†µê³„ (ì¢…ëª© ìˆ˜, ë ˆì½”ë“œ ìˆ˜, ë‚ ì§œ ë²”ìœ„)
    - ë°ì´í„° í’ˆì§ˆ (ì˜ëª»ëœ ê°€ê²©, ê±°ë˜ëŸ‰ ë“±)
    - ìµœì‹ ì„± (ëˆ„ë½ëœ ìµœì‹  ë°ì´í„°)
    - OHLC ë…¼ë¦¬ ê²€ì¦
    """
)

create_replica = PythonOperator(
    task_id='create_data_replica',
    python_callable=create_data_replica_task,
    dag=dag,
    doc_md="""
    ## ğŸ“„ ì½ê¸° ì „ìš© ë³µì œë³¸ ìƒì„±
    
    **ëª©ì **: Kafka Producer ë“± ì½ê¸° ì „ìš© ì ‘ê·¼ì„ ìœ„í•œ DB ë³µì œë³¸ ìƒì„±
    
    **ì²˜ë¦¬ ê³¼ì •**:
    1. ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë³µì‚¬
    2. ì½ê¸° ì „ìš© ê¶Œí•œ ì„¤ì •
    3. ë™ì‹œì„± ë¬¸ì œ ë°©ì§€
    """
)

# ë”ë¯¸ íƒœìŠ¤í¬ (ë¶„ê¸° ì¢…ë£Œìš©)
end_task = DummyOperator(
    task_id='end_pipeline',
    dag=dag,
    trigger_rule='none_failed_or_skipped'
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
check_data >> [bulk_collection, daily_collection]
bulk_collection >> validate_data
daily_collection >> validate_data
validate_data >> create_replica >> end_task
