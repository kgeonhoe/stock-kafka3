#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional

# Spark ê´€ë ¨ ìž„í¬íŠ¸ (ì¡°ê±´ë¶€)
try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, lag, avg, stddev, when, expr, sum
    from pyspark.sql.window import Window
    SPARK_AVAILABLE = True
    print("âœ… PySpark ìž„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"âš ï¸ PySpark ìž„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("ðŸ“Œ Docker í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    SPARK_AVAILABLE = False
    # ë”ë¯¸ í´ëž˜ìŠ¤ë“¤ ì •ì˜
    class SparkSession:
        pass
    class Window:
        pass

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, '/opt/airflow')

from common.database import DuckDBManager

class TechnicalIndicators:
    """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° í´ëž˜ìŠ¤ (Spark ê¸°ë°˜ - ê³ ì„±ëŠ¥ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬)"""
    
    def __init__(self, db_path: str = "/data/duckdb/stock_data.db"):
        """
        Spark ê¸°ë°˜ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° í´ëž˜ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
        """
        self.db = DuckDBManager(db_path)
        
        # Spark ì„¸ì…˜ ìƒì„± (ë¶„ì‚° ì²˜ë¦¬ë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ ì—”ì§„)
        if SPARK_AVAILABLE:
            self.spark = self._create_spark_session()
            print("ðŸš€ Spark ì„¸ì…˜ ìƒì„± ì™„ë£Œ - ê³ ì„±ëŠ¥ ë¶„ì‚° ì²˜ë¦¬ ì¤€ë¹„")
        else:
            self.spark = None
            print("âŒ Spark ì„¸ì…˜ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Docker í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    def _create_spark_session(self):
        """
        ê³ ì„±ëŠ¥ Spark ì„¸ì…˜ ìƒì„±
        
        Returns:
            SparkSession ì¸ìŠ¤í„´ìŠ¤ - ë¶„ì‚° ë°ì´í„° ì²˜ë¦¬ ì—”ì§„
        """
        if not SPARK_AVAILABLE:
            return None
            
        return (SparkSession.builder
                .appName("StockTechnicalIndicators")
                .config("spark.driver.memory", "4g")          # ë“œë¼ì´ë²„ ë©”ëª¨ë¦¬ ì¦ê°€
                .config("spark.executor.memory", "4g")        # ì‹¤í–‰ìž ë©”ëª¨ë¦¬ ì¦ê°€  
                .config("spark.executor.cores", "4")          # CPU ì½”ì–´ ìˆ˜ ì„¤ì •
                .config("spark.sql.adaptive.enabled", "true") # ì ì‘í˜• ì¿¼ë¦¬ ìµœì í™”
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .getOrCreate())
    
    def load_stock_data(self, symbol: Optional[str] = None) -> pd.DataFrame:
        """
        ì£¼ê°€ ë°ì´í„° ë¡œë“œ (DuckDB â†’ Pandas â†’ Spark ë³€í™˜ìš©)
        
        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼ (Noneì´ë©´ ëª¨ë“  ì¢…ëª©)
            
        Returns:
            ì£¼ê°€ ë°ì´í„° DataFrame (Spark ë³€í™˜ì„ ìœ„í•œ ì¤‘ê°„ ë‹¨ê³„)
        """
        if symbol:
            query = f"""
                SELECT * FROM stock_data
                WHERE symbol = '{symbol}'
                ORDER BY date
            """
        else:
            query = """
                SELECT * FROM stock_data
                ORDER BY symbol, date
            """
        
        result = self.db.conn.execute(query).fetchall()
        columns = ["symbol", "date", "open", "high", "low", "close", "volume"]
        df = pd.DataFrame(result, columns=columns)
        
        return df
    
    def calculate_indicators(self, symbol: Optional[str] = None):
        """
        Spark ê¸°ë°˜ ê³ ì„±ëŠ¥ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë° ì €ìž¥
        - ë¶„ì‚° ì²˜ë¦¬ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ê³ ì† ì²˜ë¦¬
        - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìœˆë„ìš° í•¨ìˆ˜ í™œìš©
        - ë³‘ë ¬ ê³„ì‚°ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ìµœì†Œí™”
        
        Args:
            symbol: ì¢…ëª© ì‹¬ë³¼ (Noneì´ë©´ ëª¨ë“  ì¢…ëª©ì„ ë³‘ë ¬ ì²˜ë¦¬)
        """
        # Spark ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not SPARK_AVAILABLE or self.spark is None:
            print("âŒ Sparkê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. Docker í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # ì£¼ê°€ ë°ì´í„° ë¡œë“œ
        stock_data = self.load_stock_data(symbol)
        
        if stock_data.empty:
            print(f"âš ï¸ {'ëª¨ë“  ì¢…ëª©' if symbol is None else symbol}ì˜ ì£¼ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ï¿½ Spark ë¶„ì‚° ì²˜ë¦¬ë¡œ {'ëª¨ë“  ì¢…ëª©' if symbol is None else symbol}ì˜ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œìž‘")
        print(f"ðŸ“Š ì²˜ë¦¬ ëŒ€ìƒ ë ˆì½”ë“œ: {len(stock_data):,}ê°œ")
        
        # Spark DataFrameìœ¼ë¡œ ë³€í™˜ (ê³ ì„±ëŠ¥ ë¶„ì‚° ì²˜ë¦¬ ì‹œìž‘)
        spark_df = self.spark.createDataFrame(stock_data)
        
        # ë°ì´í„° ìºì‹±ìœ¼ë¡œ ë°˜ë³µ ì—°ì‚° ì„±ëŠ¥ í–¥ìƒ
        spark_df.cache()
        
        print("âš¡ Spark DataFrame ìºì‹± ì™„ë£Œ - ì—°ì‚° ì„±ëŠ¥ ìµœì í™”")
        
        # ì¢…ëª©ë³„ ìœˆë„ìš° ì •ì˜ (Spark ë¶„ì‚° ìœˆë„ìš° í•¨ìˆ˜ - ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²˜ë¦¬)
        window_spec = Window.partitionBy("symbol").orderBy("date")
        
        print("ðŸ“ˆ 1/7 ë‹¨ê³„: ì´ë™í‰ê· ì„ (SMA) ë³‘ë ¬ ê³„ì‚°...")
        # 1. ì´ë™í‰ê· ì„  (SMA) - Spark ìœˆë„ìš° í•¨ìˆ˜ë¡œ ê³ ì† ì²˜ë¦¬
        for period in [5, 20, 60, 112, 224, 448]:
            window_avg = Window.partitionBy("symbol").orderBy("date").rowsBetween(-(period-1), 0)
            spark_df = spark_df.withColumn(f"sma_{period}", avg("close").over(window_avg))
        
        print("ðŸ“Š 2/7 ë‹¨ê³„: ì§€ìˆ˜ì´ë™í‰ê· (EMA) ë³‘ë ¬ ê³„ì‚°...")
        # 2. ì§€ìˆ˜ì´ë™í‰ê·  (EMA) - ìž¬ê·€ì  ê³„ì‚°ì„ Sparkë¡œ ìµœì í™”
        for period in [5, 20, 60]:
            spark_df = self._calculate_ema(spark_df, period)
        
        print("ðŸ“‰ 3/7 ë‹¨ê³„: ë³¼ë¦°ì € ë°´ë“œ ë³‘ë ¬ ê³„ì‚°...")
        # 3. ë³¼ë¦°ì € ë°´ë“œ (ê¸°ì¤€: 20ì¼ ì´ë™í‰ê· , í‘œì¤€íŽ¸ì°¨ 2) - Spark í†µê³„ í•¨ìˆ˜ í™œìš©
        window_std = Window.partitionBy("symbol").orderBy("date").rowsBetween(-19, 0)
        spark_df = spark_df.withColumn("bb_middle", col("sma_20"))
        spark_df = spark_df.withColumn("bb_std", stddev("close").over(window_std))
        spark_df = spark_df.withColumn("bb_upper", col("bb_middle") + (col("bb_std") * 2))
        spark_df = spark_df.withColumn("bb_lower", col("bb_middle") - (col("bb_std") * 2))
        
        print("âš¡ 4/7 ë‹¨ê³„: MACD ì§€í‘œ ë³‘ë ¬ ê³„ì‚°...")
        # 4. MACD (12ì¼ EMA, 26ì¼ EMA, 9ì¼ ì‹ í˜¸ì„ ) - Spark ê³ ì„±ëŠ¥ ê³„ì‚°
        spark_df = self._calculate_ema(spark_df, 12, "ema_12")
        spark_df = self._calculate_ema(spark_df, 26, "ema_26")
        spark_df = spark_df.withColumn("macd", col("ema_12") - col("ema_26"))
        
        # MACD ì‹ í˜¸ì„  (9ì¼ EMA) - Spark SQL í‘œí˜„ì‹ìœ¼ë¡œ ìµœì í™”
        window_signal = Window.partitionBy("symbol").orderBy("date")
        spark_df = spark_df.withColumn("macd_signal", 
                                       expr("avg(macd) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN 8 PRECEDING AND CURRENT ROW)"))
        spark_df = spark_df.withColumn("macd_histogram", col("macd") - col("macd_signal"))
        
        print("ðŸŽ¯ 5/7 ë‹¨ê³„: RSI ì§€í‘œ ë³‘ë ¬ ê³„ì‚°...")
        # 5. RSI (14ì¼) - Spark ìœˆë„ìš° í•¨ìˆ˜ë¡œ íš¨ìœ¨ì  ê³„ì‚°
        spark_df = self._calculate_rsi(spark_df, 14)
        
        print("ðŸ“Š 6/7 ë‹¨ê³„: CCI ì§€í‘œ ë³‘ë ¬ ê³„ì‚°...")
        # 6. CCI (ìƒí’ˆì±„ë„ì§€ìˆ˜, 20ì¼) - Spark ìˆ˜í•™ í•¨ìˆ˜ í™œìš©
        spark_df = self._calculate_cci(spark_df, 20)
        
        print("ðŸ“ˆ 7/7 ë‹¨ê³„: OBV ì§€í‘œ ë³‘ë ¬ ê³„ì‚°...")
        # 7. OBV (On Balance Volume) - Spark ëˆ„ì  í•¨ìˆ˜ë¡œ ê³ ì† ì²˜ë¦¬
        spark_df = self._calculate_obv(spark_df)
        
        print("ðŸ”¥ ëª¨ë“  ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ! ê²°ê³¼ ì •ë¦¬ ì¤‘...")
        
        # í•„ìš”í•œ ì—´ë§Œ ì„ íƒí•˜ì—¬ ìµœì¢… ê²°ê³¼ ìƒì„± (Spark ì»¬ëŸ¼ ì„ íƒìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”)
        result_df = spark_df.select(
            "symbol", "date", 
            "bb_upper", "bb_middle", "bb_lower",
            "macd", "macd_signal", "macd_histogram",
            "rsi",
            "sma_5", "sma_20", "sma_60",
            "ema_5", "ema_20", "ema_60",
            "cci", "obv",
            col("sma_112").alias("ma_112"),
            col("sma_224").alias("ma_224"),
            col("sma_448").alias("ma_448")
        )
        
        # Sparkì—ì„œ Pandasë¡œ ë³€í™˜ (ìµœì¢… ì €ìž¥ì„ ìœ„í•œ ë³€í™˜)
        print("ðŸ’¾ Spark ê²°ê³¼ë¥¼ DuckDB ì €ìž¥ìš©ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        pandas_df = result_df.toPandas()
        
        # NaN ê°’ì„ Noneìœ¼ë¡œ ë³€í™˜ (DuckDB í˜¸í™˜ì„±)
        pandas_df = pandas_df.where(pd.notnull(pandas_df), None)
        
        # DuckDBì— ë°°ì¹˜ ì €ìž¥ (ê³ ì„±ëŠ¥ ì €ìž¥)
        print("ðŸ’¾ DuckDBì— ë°°ì¹˜ ì €ìž¥ ì¤‘...")
        saved_count = 0
        for _, row in pandas_df.iterrows():
            try:
                self.db.save_technical_indicators(row.to_dict())
                saved_count += 1
            except Exception as e:
                print(f"âŒ ê¸°ìˆ ì  ì§€í‘œ ì €ìž¥ ì˜¤ë¥˜: {e}")
        
        # ìºì‹œ í•´ì œë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬
        spark_df.unpersist()
        
        print(f"ðŸŽ‰ Spark ê³ ì„±ëŠ¥ ì²˜ë¦¬ ì™„ë£Œ! ê¸°ìˆ ì  ì§€í‘œ {saved_count:,}ê°œ ì €ìž¥")
        print(f"ðŸ“ˆ ì²˜ë¦¬ ì„±ëŠ¥: {len(stock_data):,} â†’ {saved_count:,} ë ˆì½”ë“œ ë³€í™˜")
    
    def _calculate_ema(self, df, period, col_name=None):
        """
        Spark ê¸°ë°˜ ì§€ìˆ˜ì´ë™í‰ê· (EMA) ê³ ì„±ëŠ¥ ê³„ì‚°
        
        Args:
            df: Spark DataFrame
            period: ê³„ì‚° ê¸°ê°„
            col_name: ê²°ê³¼ ì»¬ëŸ¼ëª…
            
        Returns:
            EMA ì»¬ëŸ¼ì´ ì¶”ê°€ëœ Spark DataFrame (ë¶„ì‚° ì²˜ë¦¬ë¨)
        """
        if col_name is None:
            col_name = f"ema_{period}"
            
        # ì´ˆê¸°ê°’ìœ¼ë¡œ SMA ì‚¬ìš© (Spark ìœˆë„ìš° í•¨ìˆ˜)
        window_avg = Window.partitionBy("symbol").orderBy("date").rowsBetween(-(period-1), 0)
        df = df.withColumn(f"{col_name}_init", avg("close").over(window_avg))
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚° (EMA ê³µì‹)
        alpha = 2.0 / (period + 1.0)
        
        # EMA ê³„ì‚° (Spark SQL í‘œí˜„ì‹ìœ¼ë¡œ ìµœì í™”)
        return df.withColumn(col_name, 
                             when(col("close").isNull(), None)
                             .otherwise(expr(f"""
                                 CASE 
                                     WHEN {col_name}_init IS NULL THEN NULL
                                     ELSE {col_name}_init * (1 - {alpha}) + close * {alpha} 
                                 END
                             """)))
    
    def _calculate_rsi(self, df, period=14):
        """
        Spark ê¸°ë°˜ RSI(Relative Strength Index) ê³ ì„±ëŠ¥ ê³„ì‚°
        
        Args:
            df: Spark DataFrame
            period: ê³„ì‚° ê¸°ê°„ (ê¸°ë³¸ 14ì¼)
            
        Returns:
            RSI ì»¬ëŸ¼ì´ ì¶”ê°€ëœ Spark DataFrame (ë¶„ì‚° ì²˜ë¦¬ë¨)
        """
        # ì „ì¼ ì¢…ê°€ ê³„ì‚° (Spark lag ìœˆë„ìš° í•¨ìˆ˜)
        window_lag = Window.partitionBy("symbol").orderBy("date")
        df = df.withColumn("prev_close", lag("close", 1).over(window_lag))
        
        # ìƒìŠ¹í­ê³¼ í•˜ë½í­ ê³„ì‚° (Spark when ì¡°ê±´ë¬¸)
        df = df.withColumn("price_diff", col("close") - col("prev_close"))
        df = df.withColumn("gain", when(col("price_diff") > 0, col("price_diff")).otherwise(0))
        df = df.withColumn("loss", when(col("price_diff") < 0, -col("price_diff")).otherwise(0))
        
        # ì´ë™í‰ê·  ê³„ì‚° (Spark ìœˆë„ìš° í•¨ìˆ˜ë¡œ ê³ ì† ì²˜ë¦¬)
        window_avg = Window.partitionBy("symbol").orderBy("date").rowsBetween(-(period-1), 0)
        df = df.withColumn("avg_gain", avg("gain").over(window_avg))
        df = df.withColumn("avg_loss", avg("loss").over(window_avg))
        
        # RSI ê³„ì‚° (Spark ì»¬ëŸ¼ ì—°ì‚°)
        df = df.withColumn("rs", col("avg_gain") / col("avg_loss"))
        df = df.withColumn("rsi", 100 - (100 / (1 + col("rs"))))
        
        return df
    
    def _calculate_cci(self, df, period=20):
        """
        Spark ê¸°ë°˜ CCI(Commodity Channel Index) ê³ ì„±ëŠ¥ ê³„ì‚°
        
        Args:
            df: Spark DataFrame
            period: ê³„ì‚° ê¸°ê°„ (ê¸°ë³¸ 20ì¼)
            
        Returns:
            CCI ì»¬ëŸ¼ì´ ì¶”ê°€ëœ Spark DataFrame (ë¶„ì‚° ì²˜ë¦¬ë¨)
        """
        # ì „í˜•ì  ê°€ê²© ê³„ì‚° (Spark ì»¬ëŸ¼ ì—°ì‚°)
        df = df.withColumn("typical_price", (col("high") + col("low") + col("close")) / 3)
        
        # ì´ë™í‰ê·  ê³„ì‚° (Spark ìœˆë„ìš° í•¨ìˆ˜)
        window_avg = Window.partitionBy("symbol").orderBy("date").rowsBetween(-(period-1), 0)
        df = df.withColumn("tp_sma", avg("typical_price").over(window_avg))
        
        # íŽ¸ì°¨ì˜ ì ˆëŒ€ê°’ í‰ê·  ê³„ì‚° (Spark SQL í‘œí˜„ì‹ìœ¼ë¡œ ìµœì í™”)
        df = df.withColumn("mean_deviation", 
                           expr(f"avg(abs(typical_price - tp_sma)) OVER (PARTITION BY symbol ORDER BY date ROWS BETWEEN {-(period-1)} PRECEDING AND CURRENT ROW)"))
        
        # CCI ê³„ì‚° (Spark ìˆ˜í•™ ì—°ì‚°)
        df = df.withColumn("cci", (col("typical_price") - col("tp_sma")) / (0.015 * col("mean_deviation")))
        
        return df
    
    def _calculate_obv(self, df):
        """
        Spark ê¸°ë°˜ OBV(On Balance Volume) ê³ ì„±ëŠ¥ ê³„ì‚°
        
        Args:
            df: Spark DataFrame
            
        Returns:
            OBV ì»¬ëŸ¼ì´ ì¶”ê°€ëœ Spark DataFrame (ë¶„ì‚° ì²˜ë¦¬ë¨)
        """
        # ì „ì¼ ì¢…ê°€ ê³„ì‚° (Spark lag ìœˆë„ìš° í•¨ìˆ˜)
        window = Window.partitionBy("symbol").orderBy("date")
        df = df.withColumn("prev_close", lag("close", 1).over(window))
        
        # OBV ê³„ì‚° ê·œì¹™ (Spark when ì¡°ê±´ë¬¸ìœ¼ë¡œ ìµœì í™”)
        df = df.withColumn("obv_value", 
                           when(col("close") > col("prev_close"), col("volume"))
                           .when(col("close") < col("prev_close"), -col("volume"))
                           .otherwise(0))
        
        # ëˆ„ì í•© ê³„ì‚° (Spark sum ìœˆë„ìš° í•¨ìˆ˜ë¡œ ê³ ì† ì²˜ë¦¬)
        df = df.withColumn("obv", sum("obv_value").over(window))
        
        return df
    
    def close(self):
        """Spark ì„¸ì…˜ ë° ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.db.close()
        if SPARK_AVAILABLE and self.spark is not None:
            self.spark.stop()
            print("ðŸ”¥ Spark ì„¸ì…˜ ì¢…ë£Œ - ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# Airflow íƒœìŠ¤í¬ í•¨ìˆ˜
def calculate_technical_indicators_task(**kwargs):
    """
    Airflow íƒœìŠ¤í¬: Spark ê¸°ë°˜ ê³ ì„±ëŠ¥ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
    
    Args:
        **kwargs: Airflow ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        ì„±ê³µ ì—¬ë¶€ (True/False)
    """
    indicators = TechnicalIndicators()
    
    try:
        print("ðŸš€ Spark ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹œìž‘!")
        # ëª¨ë“  ì¢…ëª©ì— ëŒ€í•´ ë³‘ë ¬ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        indicators.calculate_indicators()
        indicators.close()
        print("ðŸŽ‰ Spark ê³ ì„±ëŠ¥ ì²˜ë¦¬ ì™„ë£Œ!")
        return True
    
    except Exception as e:
        indicators.close()
        print(f"âŒ Spark ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return False
