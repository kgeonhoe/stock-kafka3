#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ (ìƒˆë¡œìš´ ë²„ì „)
- ë‹¤ì¤‘ í† í”½ ì§€ì›
- ì›Œì»¤ ìˆ˜ëŸ‰ ì¡°ì ˆ
- API í˜¸ì¶œ í…ŒìŠ¤íŠ¸
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import json
import time
import threading
import random
import uuid
from datetime import datetime
from typing import Dict, List, Optional
from dataclasses import dataclass
from kafka import KafkaProducer, KafkaConsumer
from kafka.errors import KafkaError
import logging
import concurrent.futures
import psutil
import requests
from threading import Lock
from collections import defaultdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LoadTestConfig:
    """ë¶€í•˜í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    duration_minutes: int = 5
    kafka_producer_workers: int = 10  # Kafka Producer ì›Œì»¤ ìˆ˜
    kafka_consumer_workers: int = 5   # Kafka Consumer ì›Œì»¤ ìˆ˜
    api_call_workers: int = 4         # API í˜¸ì¶œ ì›Œì»¤ ìˆ˜ (BulkDataCollectorì˜ max_workers)
    messages_per_worker: int = 1000
    topics: List[str] = None
    api_endpoints: List[str] = None
    signal_probability: float = 0.3
    bootstrap_servers: List[str] = None  # Kafka ë¸Œë¡œì»¤ ë¦¬ìŠ¤íŠ¸ (ì—†ìœ¼ë©´ ìë™ ê²°ì •)
    
    def __post_init__(self):
        if self.topics is None:
            self.topics = ["realtime-stock", "yfinance-stock-data"]
        if self.api_endpoints is None:
            import os, socket
            # í™˜ê²½ë³€ìˆ˜ë¡œ ì»¤ìŠ¤í…€ (ì‰¼í‘œ ë¶„ë¦¬)
            env_api = os.getenv("LOAD_TEST_API_ENDPOINTS")
            if env_api:
                self.api_endpoints = [e.strip() for e in env_api.split(',') if e.strip()]
            else:
                # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì—¬ë¶€ ê°ì§€ (/.dockerenv ì¡´ì¬ ë“±)
                in_container = os.path.exists('/.dockerenv') or os.getenv('KUBERNETES_SERVICE_HOST') is not None
                # ê¸°ë³¸: Airflow webserver í—¬ìŠ¤ì²´í¬ (ë‚´ë¶€ í˜¸ìŠ¤íŠ¸ëª…), í•„ìš”ì‹œ ì‚¬ìš©ì ì„œë¹„ìŠ¤ ì¶”ê°€
                if in_container:
                    self.api_endpoints = [
                        "http://airflow-webserver:8080/health"  # Airflow ë‚´ë¶€ health
                    ]
                else:
                    # ë¡œì»¬ í˜¸ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì‹¤í–‰ ì‹œ í¬íŠ¸ ë§¤í•‘ ê¸°ì¤€(í˜¸ìŠ¤íŠ¸ 8081 -> ì»¨í…Œì´ë„ˆ 8080)
                    self.api_endpoints = [
                        "http://localhost:8081/health"
                    ]
        # ì˜ëª»ëœ Redis HTTP endpoint ìë™ ì œê±° (:6379 HTTP ì‹œë„)
        cleaned = []
        for ep in self.api_endpoints:
            if ep.startswith(('http://', 'https://')) and ':6379' in ep:
                try:
                    logger.warning(f"ì œê±°: Redis í¬íŠ¸ 6379ì— ëŒ€í•œ HTTP endpointëŠ” ìœ íš¨í•˜ì§€ ì•ŠìŒ -> {ep}")
                except Exception:
                    pass
                continue
            cleaned.append(ep)
        self.api_endpoints = cleaned
    # NOTE: í•„ìš” ì‹œ redis:// í˜•íƒœë¥¼ ë³„ë„ Redis ping ì›Œì»¤ë¡œ ì§€ì› ê°€ëŠ¥
        if self.bootstrap_servers is None:
            # í™˜ê²½ë³€ìˆ˜(KAFKA_BOOTSTRAP_SERVERS) ìš°ì„  ì‚¬ìš© (ì‰¼í‘œ êµ¬ë¶„)
            import os
            env_val = os.getenv("KAFKA_BOOTSTRAP_SERVERS")
            if env_val:
                self.bootstrap_servers = [s.strip() for s in env_val.split(',') if s.strip()]
            else:
                self.bootstrap_servers = ["localhost:9092"]

@dataclass
class WorkerStats:
    """ì›Œì»¤ í†µê³„"""
    worker_id: str
    worker_type: str
    success_count: int = 0
    error_count: int = 0
    total_messages: int = 0
    duration: float = 0.0
    throughput: float = 0.0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []

class KafkaLoadTester:
    """Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

@dataclass
class SLOCriteria:
    """SLO ê¸°ì¤€ (ìš´ì˜ ì˜í–¥ íŒë‹¨ìš©)"""
    min_success_rate: float = 99.0          # ìµœì†Œ ì„±ê³µë¥  (%)
    max_error_count: int = 0                # í—ˆìš© ì´ ì—ëŸ¬ ìˆ˜ (0ì´ë©´ ì˜¤ë¥˜ ì—†ëŠ”ì§€)
    max_avg_cpu_usage: float = 70.0         # í‰ê·  CPU ì‚¬ìš©ë¥  ìƒí•œ (%)
    max_consumer_producer_gap_pct: float = 5.0  # consumer/producer ëˆ„ì  ì°¨ì´ í—ˆìš© í¸ì°¨ (%)
    min_overall_throughput: float = 0.0     # ìµœì†Œ ì „ì²´ ì²˜ë¦¬ëŸ‰ (msg/sec)


class KafkaLoadTester:
    """Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""

    def __init__(self, config: LoadTestConfig, slo_criteria: Optional[SLOCriteria] = None):
        """ì´ˆê¸°í™”"""
        self.config = config
        self.slo_criteria = slo_criteria  # ì—†ìœ¼ë©´ í‰ê°€ ìƒëµ
        # ì‹¤í–‰ ìƒíƒœ
        self.results: List[WorkerStats] = []
        self.is_running: bool = False
        self.start_time: Optional[float] = None
        self.run_id: Optional[str] = None  # ê° í…ŒìŠ¤íŠ¸ ê³ ìœ  ID
        # ë™ì‹œì„± & ì¹´ìš´í„°
        self._lock = Lock()
        self._producer_sent = 0
        self._consumer_received = 0
        self._api_success = 0
        self._skipped_consumed = 0  # run_id ë¶ˆì¼ì¹˜ë¡œ ë¬´ì‹œëœ ì†Œë¹„
        # ìŠ¤ëƒ…ìƒ· (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§)
        self.metric_snapshots: List[Dict] = []
        self._last_snapshot_time = 0.0
        self.snapshot_interval_sec = 1.0
        self.recent_errors: List[str] = []
        # ì—”ë“œí¬ì¸íŠ¸ë³„ ì„±ê³µ/ì‹¤íŒ¨
        self.endpoint_success = defaultdict(int)
        self.endpoint_error = defaultdict(int)
        # Latency ê¸°ë¡ (consumer ë§¤ì¹­ ë©”ì‹œì§€)
        self._latencies: List[float] = []
        self._latency_sample_limit = 200000  # ë©”ëª¨ë¦¬ ë³´í˜¸

    def generate_stock_message(self, worker_id: str, message_id: int) -> Dict:
        """ì£¼ì‹ ë°ì´í„° ë©”ì‹œì§€ ìƒì„±"""
        symbols = [
            'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'NFLX',
            'AMD', 'INTC', 'CRM', 'ORCL', 'IBM', 'CSCO', 'QCOM'
        ]
        
        symbol = random.choice(symbols)
        base_price = random.uniform(100, 500)
        
        message = {
            'symbol': symbol,
            'price': round(base_price + random.uniform(-10, 10), 2),
            'volume': random.randint(1000, 1000000),
            'timestamp': time.time(),
            'datetime': datetime.now().isoformat(),
            'worker_id': worker_id,
            'message_id': message_id,
            'test_type': 'load_test',
            'run_id': self.run_id,
            'change': round(random.uniform(-5, 5), 2),
            'change_percent': round(random.uniform(-10, 10), 2),
            # ê¸°ìˆ ì  ì§€í‘œ (ì‹œë®¬ë ˆì´ì…˜)
            'rsi': random.uniform(20, 80),
            'macd': random.uniform(-2, 2),
            'bb_upper': round(base_price * 1.05, 2),
            'bb_lower': round(base_price * 0.95, 2),
            # ì‹ í˜¸ ê°ì§€ (í™•ë¥  ê¸°ë°˜)
            'signal': self._generate_signal() if random.random() < self.config.signal_probability else None
        }
        
        return message
    
    def _generate_signal(self) -> Dict:
        """ì‹ í˜¸ ìƒì„±"""
        signal_types = [
            'bollinger_upper_touch', 'bollinger_lower_touch',
            'rsi_overbought', 'rsi_oversold',
            'macd_bullish_crossover', 'macd_bearish_crossover',
            'volume_spike', 'price_breakout'
        ]
        
        return {
            'type': random.choice(signal_types),
            'strength': random.uniform(0.5, 1.0),
            'timestamp': time.time()
        }
    
    def kafka_producer_worker(self, worker_id: str, topic: str, message_count: int) -> WorkerStats:
        """Kafka Producer ì›Œì»¤"""
        stats = WorkerStats(
            worker_id=f"producer_{worker_id}",
            worker_type="kafka_producer"
        )
        
        try:
            producer = KafkaProducer(
                bootstrap_servers=self.config.bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                retries=3,
                retry_backoff_ms=1000,
                compression_type='gzip',  # ì••ì¶• í™œì„±í™”
                batch_size=16384,        # ë°°ì¹˜ í¬ê¸° ìµœì í™”
                linger_ms=10             # ë°°ì¹˜ ëŒ€ê¸° ì‹œê°„
            )
            
            start_time = time.time()
            
            for i in range(message_count):
                if not self.is_running:
                    break
                    
                try:
                    message = self.generate_stock_message(worker_id, i)
                    
                    # ë©”ì‹œì§€ ì „ì†¡
                    future = producer.send(topic, message)
                    
                    # ë…¼ë¸”ë¡œí‚¹ìœ¼ë¡œ ê²°ê³¼ í™•ì¸ (ì„ íƒì )
                    if i % 100 == 0:  # 100ê°œë§ˆë‹¤ í™•ì¸
                        future.get(timeout=5)
                    
                    stats.success_count += 1
                    with self._lock:
                        self._producer_sent += 1
                    self._maybe_snapshot()
                    
                    # ì²˜ë¦¬ëŸ‰ ì¡°ì ˆ (CPU ë¶€í•˜ ë°©ì§€)
                    if i % 50 == 0:
                        time.sleep(0.01)
                        
                except KafkaError as e:
                    stats.error_count += 1
                    stats.errors.append(f"Kafka error: {str(e)}")
                    logger.error(f"Producer {worker_id} Kafka error: {e}")
                    with self._lock:
                        self.recent_errors.append(f"producer:{worker_id}:{e}")
                        self.recent_errors = self.recent_errors[-50:]
                    self._maybe_snapshot(force=True)
                    
                except Exception as e:
                    stats.error_count += 1
                    stats.errors.append(f"General error: {str(e)}")
                    logger.error(f"Producer {worker_id} error: {e}")
                    with self._lock:
                        self.recent_errors.append(f"producer:{worker_id}:{e}")
                        self.recent_errors = self.recent_errors[-50:]
                    self._maybe_snapshot(force=True)
            
            producer.flush(timeout=30)  # ëª¨ë“  ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ ëŒ€ê¸°
            producer.close()
            
            end_time = time.time()
            stats.duration = end_time - start_time
            stats.total_messages = message_count
            stats.throughput = stats.success_count / stats.duration if stats.duration > 0 else 0
            
            logger.info(f"âœ… Producer {worker_id} ì™„ë£Œ: {stats.success_count}/{message_count}, {stats.throughput:.2f} msg/sec")
            
        except Exception as e:
            stats.error_count = message_count
            stats.errors.append(f"Worker initialization error: {str(e)}")
            logger.error(f"âŒ Producer {worker_id} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        return stats
    
    def kafka_consumer_worker(self, worker_id: str, topic: str, timeout_seconds: int) -> WorkerStats:
        """Kafka Consumer ì›Œì»¤"""
        stats = WorkerStats(
            worker_id=f"consumer_{worker_id}",
            worker_type="kafka_consumer"
        )
        
        try:
            consumer = KafkaConsumer(
                topic,
                bootstrap_servers=self.config.bootstrap_servers,
                group_id=f'load_test_consumer_group_{int(self.start_time or time.time())}',
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                auto_offset_reset='latest',  # run_id ê¸°ë°˜ í•„í„°ë§ ì‹œ backlog ì œì™¸
                enable_auto_commit=True,
                consumer_timeout_ms=1000  # 1ì´ˆ íƒ€ì„ì•„ì›ƒ
            )
            
            start_time = time.time()
            end_time = start_time + timeout_seconds
            
            empty_polls = 0
            while time.time() < end_time and self.is_running:
                try:
                    message_batch = consumer.poll(timeout_ms=300)
                    if not message_batch:
                        empty_polls += 1
                        if empty_polls % 10 == 0:
                            logger.debug(f"Consumer {worker_id} ì•„ì§ ìˆ˜ì‹  ì—†ìŒ (empty polls={empty_polls})")
                        # ì£¼ê¸°ì ìœ¼ë¡œ ìŠ¤ëƒ…ìƒ·
                        if empty_polls % 5 == 0:
                            self._maybe_snapshot()
                        continue
                    empty_polls = 0
                    for topic_partition, messages in message_batch.items():
                        for message in messages:
                            val = message.value
                            # run_id í•„í„°: í˜„ì¬ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì•„ë‹Œ ê²½ìš° skip
                            if val.get('run_id') != self.run_id:
                                with self._lock:
                                    self._skipped_consumed += 1
                                continue
                            stats.success_count += 1
                            recv_time = time.time()
                            with self._lock:
                                self._consumer_received += 1
                                if 'timestamp' in val and len(self._latencies) < self._latency_sample_limit:
                                    self._latencies.append(recv_time - float(val['timestamp']))
                            if 'signal' in message.value and message.value['signal']:
                                logger.debug(f"Consumer {worker_id} ì‹ í˜¸ ê°ì§€: {message.value['signal']}")
                            # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                            # time.sleep(0.001)  # í•„ìš”ì‹œ ì§€ì—°
                            self._maybe_snapshot()
                except Exception as e:
                    stats.error_count += 1
                    stats.errors.append(f"Consumption error: {str(e)}")
                    logger.warning(f"Consumer {worker_id} error: {e}")
                    with self._lock:
                        self.recent_errors.append(f"consumer:{worker_id}:{e}")
                        self.recent_errors = self.recent_errors[-50:]
                    self._maybe_snapshot(force=True)
            
            consumer.close()
            
            stats.duration = time.time() - start_time
            stats.total_messages = stats.success_count + stats.error_count
            stats.throughput = stats.success_count / stats.duration if stats.duration > 0 else 0
            
            logger.info(f"âœ… Consumer {worker_id} ì™„ë£Œ: {stats.success_count} consumed, {stats.throughput:.2f} msg/sec")
            
        except Exception as e:
            stats.errors.append(f"Consumer initialization error: {str(e)}")
            logger.error(f"âŒ Consumer {worker_id} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        return stats
    
    def api_call_worker(self, worker_id: str, endpoints: List[str], calls_per_endpoint: int) -> WorkerStats:
        """API í˜¸ì¶œ ì›Œì»¤ (BulkDataCollector ìŠ¤íƒ€ì¼)"""
        stats = WorkerStats(
            worker_id=f"api_{worker_id}",
            worker_type="api_caller"
        )
        
        session = requests.Session()
        session.timeout = 5
        
        start_time = time.time()
        
        try:
            for endpoint in endpoints:
                for i in range(calls_per_endpoint):
                    if not self.is_running:
                        break
                        
                    try:
                        # API í˜¸ì¶œ (ì‹¤ì œë¡œëŠ” yfinance, KIS API ë“±)
                        # Connection refused ë°œìƒ ì‹œ:
                        # 1) í•´ë‹¹ endpoint ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜
                        # 2) ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ 'localhost' ë¡œ ì ‘ê·¼í–ˆì§€ë§Œ ì‹¤ì œ ì„œë¹„ìŠ¤ëŠ” ë‹¤ë¥¸ ì»¨í…Œì´ë„ˆ(hostname)ì—ì„œ ì—´ë ¤ìˆê±°ë‚˜
                        # 3) í¬íŠ¸ ë§¤í•‘ì´ í˜¸ìŠ¤íŠ¸->ì»¨í…Œì´ë„ˆ ë‹¤ë¥´ê²Œ ì„¤ì •ëœ ê²½ìš°
                        # docker-compose ë‚´ë¶€ì—ì„œëŠ” http://<service-name>:<port>/ í˜•íƒœ ì‚¬ìš© ê¶Œì¥.
                        response = session.get(
                            endpoint,
                            timeout=5,
                            params={
                                'test': True,
                                'worker_id': worker_id,
                                'call_id': i
                            }
                        )
                        
                        if response.status_code == 200:
                            stats.success_count += 1
                            with self._lock:
                                self._api_success += 1
                                try:
                                    self.endpoint_success[endpoint] += 1
                                except Exception:
                                    pass
                            self._maybe_snapshot()
                        else:
                            stats.error_count += 1
                            stats.errors.append(f"HTTP {response.status_code}: {endpoint}")
                            with self._lock:
                                try:
                                    self.endpoint_error[endpoint] += 1
                                except Exception:
                                    pass
                            
                    except requests.RequestException as e:
                        stats.error_count += 1
                        stats.errors.append(f"Request error: {str(e)}")
                        logger.warning(f"API Worker {worker_id} error: {e}")
                        with self._lock:
                            self.recent_errors.append(f"api:{worker_id}:{e}")
                            self.recent_errors = self.recent_errors[-50:]
                            try:
                                self.endpoint_error[endpoint] += 1
                            except Exception:
                                pass
                        self._maybe_snapshot(force=True)
                    
                    # API í˜¸ì¶œ ê°„ê²© (ì‹¤ì œë¡œëŠ” rate limiting)
                    time.sleep(0.1)
        
        except Exception as e:
            stats.errors.append(f"API worker error: {str(e)}")
            logger.error(f"âŒ API Worker {worker_id} ì „ì²´ ì‹¤íŒ¨: {e}")
        
        finally:
            session.close()
        
        stats.duration = time.time() - start_time
        stats.total_messages = stats.success_count + stats.error_count
        stats.throughput = stats.success_count / stats.duration if stats.duration > 0 else 0
        
        logger.info(f"âœ… API Worker {worker_id} ì™„ë£Œ: {stats.success_count}/{stats.total_messages}, {stats.throughput:.2f} calls/sec")
        
        return stats
    
    def run_load_test(self) -> Dict:
        """ì „ì²´ ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Kafka + API ë¶€í•˜í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        logger.info(f"âš™ï¸ ì„¤ì •: {self.config}")
        self.is_running = True
        self.start_time = time.time()
        # run_id ë°œê¸‰
        self.run_id = f"{int(self.start_time)}_{uuid.uuid4().hex[:8]}"
        logger.info(f"ğŸ†” run_id={self.run_id}")
        # ì´ˆê¸° ìŠ¤ëƒ…ìƒ·
        self._maybe_snapshot(force=True)
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        system_stats = self._start_system_monitoring()

        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹¤í–‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
            futures = []

            # 1. Kafka Consumer ì›Œì»¤ë“¤ (ë¨¼ì € ì‹œì‘)
            test_duration = self.config.duration_minutes * 60
            for i in range(self.config.kafka_consumer_workers):
                topic = self.config.topics[i % len(self.config.topics)]
                future = executor.submit(
                    self.kafka_consumer_worker,
                    f"cons_{i}",
                    topic,
                    test_duration
                )
                futures.append(future)

            time.sleep(0.5)

            # 2. Kafka Producer ì›Œì»¤ë“¤
            for i in range(self.config.kafka_producer_workers):
                topic = self.config.topics[i % len(self.config.topics)]
                future = executor.submit(
                    self.kafka_producer_worker,
                    f"prod_{i}",
                    topic,
                    self.config.messages_per_worker
                )
                futures.append(future)

            # 3. API í˜¸ì¶œ ì›Œì»¤ë“¤
            calls_per_worker = 100
            for i in range(self.config.api_call_workers):
                future = executor.submit(
                    self.api_call_worker,
                    f"api_{i}",
                    self.config.api_endpoints,
                    calls_per_worker
                )
                futures.append(future)

            logger.info(f"â±ï¸ {self.config.duration_minutes}ë¶„ê°„ ì‹¤í–‰ ì¤‘ (ì¤‘ë‹¨ ê°€ëŠ¥)...")
            end_time = time.time() + self.config.duration_minutes * 60
            while self.is_running and time.time() < end_time:
                time.sleep(0.5)
                self._maybe_snapshot()
            self.is_running = False
            logger.info("ğŸ›‘ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡...")

            for future in concurrent.futures.as_completed(futures, timeout=120):
                try:
                    result = future.result()
                    self.results.append(result)
                except Exception as e:
                    logger.error(f"ì›Œì»¤ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

        final_system_stats = self._stop_system_monitoring(system_stats)
        summary = self._generate_test_summary(final_system_stats)
        logger.info("âœ… ë¶€í•˜í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ê²°ê³¼ ìš”ì•½: {summary['total_messages']} ë©”ì‹œì§€, ì„±ê³µë¥  {summary['success_rate']:.1f}%")
        return summary

    # -------- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ê´€ë ¨ ìœ í‹¸ ---------
    def _maybe_snapshot(self, force: bool = False):
        """ìŠ¤ëƒ…ìƒ· ì£¼ê¸° ì¡°ê±´ ì¶©ì¡± ì‹œ ê¸°ë¡ (force=True ì‹œ ì¦‰ì‹œ)"""
        now = time.time()
        if force or (now - self._last_snapshot_time >= self.snapshot_interval_sec):
            self._last_snapshot_time = now
            with self._lock:
                elapsed = now - (self.start_time or now)
                snapshot = {
                    'timestamp': now,
                    'elapsed_sec': elapsed,
                    'producer_sent': self._producer_sent,
                    'consumer_received': self._consumer_received,
                    'api_success': self._api_success
                }
                self.metric_snapshots.append(snapshot)

    def get_live_metrics(self) -> Dict:
        """í˜„ì¬ê¹Œì§€ ëˆ„ì  ë° ìµœê·¼ ì²˜ë¦¬ëŸ‰(ì´ˆë‹¹) ê³„ì‚°"""
        with self._lock:
            if len(self.metric_snapshots) >= 2:
                last = self.metric_snapshots[-1]
                prev = self.metric_snapshots[-2]
                interval = last['elapsed_sec'] - prev['elapsed_sec'] or 1
                return {
                    'producer_sent_total': self._producer_sent,
                    'consumer_received_total': self._consumer_received,
                    'api_success_total': self._api_success,
                    'producer_tps': (last['producer_sent'] - prev['producer_sent']) / interval,
                    'consumer_tps': (last['consumer_received'] - prev['consumer_received']) / interval,
                    'api_tps': (last['api_success'] - prev['api_success']) / interval,
                    'snapshots': list(self.metric_snapshots[-300:]),  # ìµœê·¼ 5ë¶„(ì´ˆë‹¨ìœ„) ì œí•œ
                    'recent_errors': list(self.recent_errors[-10:])
                }
            else:
                return {
                    'producer_sent_total': self._producer_sent,
                    'consumer_received_total': self._consumer_received,
                    'api_success_total': self._api_success,
                    'producer_tps': 0.0,
                    'consumer_tps': 0.0,
                    'api_tps': 0.0,
                    'snapshots': list(self.metric_snapshots),
                    'recent_errors': list(self.recent_errors[-10:])
                }
    
    def _start_system_monitoring(self) -> Dict:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        return {
            'start_cpu_percent': psutil.cpu_percent(),
            'start_memory': psutil.virtual_memory(),
            'start_time': time.time()
        }
    
    def _stop_system_monitoring(self, start_stats: Dict) -> Dict:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ"""
        end_stats = {
            'end_cpu_percent': psutil.cpu_percent(),
            'end_memory': psutil.virtual_memory(),
            'end_time': time.time()
        }
        
        return {**start_stats, **end_stats}
    
    def _generate_test_summary(self, system_stats: Dict) -> Dict:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total_success = sum(r.success_count for r in self.results)
        total_errors = sum(r.error_count for r in self.results)
        total_messages = total_success + total_errors
        
        # ì›Œì»¤ íƒ€ì…ë³„ ì§‘ê³„
        producer_results = [r for r in self.results if r.worker_type == 'kafka_producer']
        consumer_results = [r for r in self.results if r.worker_type == 'kafka_consumer']
        api_results = [r for r in self.results if r.worker_type == 'api_caller']
        
        summary = {
            'test_config': self.config,
            'total_duration': time.time() - self.start_time,
            'total_messages': total_messages,
            'total_success': total_success,
            'total_errors': total_errors,
            'success_rate': (total_success / total_messages * 100) if total_messages > 0 else 0,
            'overall_throughput': total_success / (time.time() - self.start_time) if self.start_time else 0,
            'api_endpoint_breakdown': {
                'success': dict(self.endpoint_success),
                'error': dict(self.endpoint_error)
            },
            'run_id': self.run_id,
            'skipped_messages': self._skipped_consumed,
            # ì›Œì»¤ë³„ í†µê³„
            'producer_stats': {
                'worker_count': len(producer_results),
                'avg_throughput': (sum(r.throughput for r in producer_results) / len(producer_results)) if producer_results else 0,
                'total_sent': sum(r.success_count for r in producer_results)
            },
            'consumer_stats': {
                'worker_count': len(consumer_results),
                'avg_throughput': (sum(r.throughput for r in consumer_results) / len(consumer_results)) if consumer_results else 0,
                'total_consumed': sum(r.success_count for r in consumer_results)
            },
            'api_stats': {
                'worker_count': len(api_results),
                'avg_throughput': (sum(r.throughput for r in api_results) / len(api_results)) if api_results else 0,
                'total_calls': sum(r.success_count for r in api_results)
            },
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
            'system_resources': {
                'avg_cpu_usage': (system_stats['start_cpu_percent'] + system_stats['end_cpu_percent']) / 2,
                'memory_used_gb': system_stats['end_memory'].used / (1024**3),
                'memory_percent': system_stats['end_memory'].percent
            },
            # ê°œë³„ ì›Œì»¤ ê²°ê³¼
            'worker_results': [
                {
                    'worker_id': r.worker_id,
                    'worker_type': r.worker_type,
                    'success_count': r.success_count,
                    'error_count': r.error_count,
                    'throughput': r.throughput,
                    'duration': r.duration,
                    'error_sample': r.errors[:3] if r.errors else []
                }
                for r in self.results
            ]
        }
        # SLO í‰ê°€
        if self.slo_criteria:
            checks = []
            sc = self.slo_criteria
            # 1. ì„±ê³µë¥ 
            checks.append({
                'name': 'ì„±ê³µë¥ ',
                'passed': summary['success_rate'] >= sc.min_success_rate,
                'actual': round(summary['success_rate'], 2),
                'expected': f">= {sc.min_success_rate}"
            })
            # 2. ì˜¤ë¥˜ ìˆ˜
            checks.append({
                'name': 'ì´ ì˜¤ë¥˜ ìˆ˜',
                'passed': summary['total_errors'] <= sc.max_error_count,
                'actual': summary['total_errors'],
                'expected': f"<= {sc.max_error_count}"
            })
            # 3. CPU ì‚¬ìš©ë¥ 
            cpu_actual = summary['system_resources']['avg_cpu_usage']
            checks.append({
                'name': 'í‰ê·  CPU ì‚¬ìš©ë¥ ',
                'passed': cpu_actual <= sc.max_avg_cpu_usage,
                'actual': round(cpu_actual, 2),
                'expected': f"<= {sc.max_avg_cpu_usage}"
            })
            # 4. Consumer/Producer í¸ì°¨
            prod_total = summary['producer_stats']['total_sent'] or 0
            cons_total = summary['consumer_stats']['total_consumed'] or 0
            if prod_total > 0:
                gap_pct = abs(cons_total - prod_total) / prod_total * 100
                checks.append({
                    'name': 'Consumer/Producer ëˆ„ì  í¸ì°¨%',
                    'passed': gap_pct <= sc.max_consumer_producer_gap_pct,
                    'actual': round(gap_pct, 2),
                    'expected': f"<= {sc.max_consumer_producer_gap_pct}"
                })
            else:
                checks.append({
                    'name': 'Consumer/Producer ëˆ„ì  í¸ì°¨%',
                    'passed': True,
                    'actual': 'N/A',
                    'expected': f"<= {sc.max_consumer_producer_gap_pct} (producer=0)"
                })
            # 5. ì „ì²´ ì²˜ë¦¬ëŸ‰
            checks.append({
                'name': 'ì „ì²´ ì²˜ë¦¬ëŸ‰(msg/s)',
                'passed': summary['overall_throughput'] >= sc.min_overall_throughput,
                'actual': round(summary['overall_throughput'], 2),
                'expected': f">= {sc.min_overall_throughput}"
            })
            summary['slo'] = {
                'criteria': sc,
                'checks': checks,
                'passed': all(c['passed'] for c in checks)
            }
        # Latency í†µê³„ ì¶”ê°€
        with self._lock:
            lats = list(self._latencies)
        if lats:
            lats_sorted = sorted(lats)
            def pct(p):
                if not lats_sorted:
                    return None
                k = (len(lats_sorted)-1) * p/100
                f = int(k)
                c = min(f+1, len(lats_sorted)-1)
                if f == c:
                    return lats_sorted[f]
                return lats_sorted[f] + (lats_sorted[c]-lats_sorted[f])*(k-f)
            summary['latency_stats'] = {
                'count': len(lats_sorted),
                'avg_ms': (sum(lats_sorted)/len(lats_sorted))*1000,
                'p50_ms': pct(50)*1000,
                'p95_ms': pct(95)*1000,
                'p99_ms': pct(99)*1000,
                'max_ms': max(lats_sorted)*1000
            }
        else:
            summary['latency_stats'] = {
                'count': 0,
                'avg_ms': None,
                'p50_ms': None,
                'p95_ms': None,
                'p99_ms': None,
                'max_ms': None
            }

        return summary

def main():
    """ë©”ì¸ í•¨ìˆ˜ - CLI í…ŒìŠ¤íŠ¸ìš©"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Kafka + API ë¶€í•˜í…ŒìŠ¤íŠ¸")
    parser.add_argument("--duration", type=int, default=5, help="í…ŒìŠ¤íŠ¸ ì§€ì†ì‹œê°„ (ë¶„)")
    parser.add_argument("--kafka-producers", type=int, default=10, help="Kafka Producer ì›Œì»¤ ìˆ˜")
    parser.add_argument("--kafka-consumers", type=int, default=5, help="Kafka Consumer ì›Œì»¤ ìˆ˜")
    parser.add_argument("--api-workers", type=int, default=4, help="API í˜¸ì¶œ ì›Œì»¤ ìˆ˜")
    parser.add_argument("--messages-per-worker", type=int, default=1000, help="ì›Œì»¤ë‹¹ ë©”ì‹œì§€ ìˆ˜")
    parser.add_argument("--topics", nargs='+', default=["realtime-stock", "yfinance-stock-data"], help="í…ŒìŠ¤íŠ¸ í† í”½ ëª©ë¡")
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config = LoadTestConfig(
        duration_minutes=args.duration,
        kafka_producer_workers=args.kafka_producers,
        kafka_consumer_workers=args.kafka_consumers,
        api_call_workers=args.api_workers,
        messages_per_worker=args.messages_per_worker,
        topics=args.topics
    )
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = KafkaLoadTester(config)
    results = tester.run_load_test()
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ“Š KAFKA + API ë¶€í•˜í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ì „ì²´ ë©”ì‹œì§€: {results['total_messages']:,}")
    print(f"ì„±ê³µ: {results['total_success']:,}")
    print(f"ì‹¤íŒ¨: {results['total_errors']:,}")
    print(f"ì„±ê³µë¥ : {results['success_rate']:.1f}%")
    print(f"ì „ì²´ ì²˜ë¦¬ëŸ‰: {results['overall_throughput']:.2f} msg/sec")
    print(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {results['total_duration']:.2f}ì´ˆ")
    print(f"CPU ì‚¬ìš©ë¥ : {results['system_resources']['avg_cpu_usage']:.1f}%")
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {results['system_resources']['memory_percent']:.1f}%")
    
    # ì›Œì»¤ë³„ í†µê³„
    print(f"\nğŸ“¤ Producer: {results['producer_stats']['total_sent']:,} sent, {results['producer_stats']['avg_throughput']:.2f} avg msg/sec")
    print(f"ğŸ“¥ Consumer: {results['consumer_stats']['total_consumed']:,} consumed, {results['consumer_stats']['avg_throughput']:.2f} avg msg/sec")
    print(f"ğŸŒ API: {results['api_stats']['total_calls']:,} calls, {results['api_stats']['avg_throughput']:.2f} avg calls/sec")

if __name__ == "__main__":
    main()
