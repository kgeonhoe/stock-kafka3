#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¶€í•˜í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë„êµ¬
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import glob
import json
import argparse
from datetime import datetime
import os

# í•œê¸€ í°íŠ¸ ì„¤ì • (í•œêµ­ì–´ ê·¸ë˜í”„ìš©)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

def analyze_locust_results(csv_pattern="load_test_results_*.csv"):
    """Locust CSV ê²°ê³¼ ë¶„ì„"""
    
    print("ğŸ“Š Locust ë¶€í•˜í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„")
    print("=" * 50)
    
    # CSV íŒŒì¼ ì°¾ê¸°
    csv_files = glob.glob(csv_pattern)
    if not csv_files:
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_pattern}")
        return
    
    latest_csv = max(csv_files, key=os.path.getctime)
    print(f"ğŸ“ ë¶„ì„ íŒŒì¼: {latest_csv}")
    
    try:
        # CSV ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(latest_csv)
        
        print(f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„:")
        print(f"  - ì´ ìš”ì²­ ìˆ˜: {df['Request Count'].sum():,}")
        print(f"  - ì´ ì‹¤íŒ¨ ìˆ˜: {df['Failure Count'].sum():,}")
        print(f"  - ì „ì²´ ì˜¤ë¥˜ìœ¨: {(df['Failure Count'].sum() / df['Request Count'].sum() * 100):.2f}%")
        print(f"  - í‰ê·  ì‘ë‹µì‹œê°„: {df['Average Response Time'].mean():.2f}ms")
        print(f"  - ìµœëŒ€ ì‘ë‹µì‹œê°„: {df['Max Response Time'].max():,}ms")
        print(f"  - í‰ê·  ì²˜ë¦¬ëŸ‰: {df['Requests/s'].mean():.2f} req/s")
        
        # ìƒìœ„ ëŠë¦° ìš”ì²­ë“¤
        print(f"\nğŸŒ ê°€ì¥ ëŠë¦° ìš”ì²­ TOP 5:")
        slow_requests = df.nlargest(5, 'Average Response Time')[['Name', 'Average Response Time', 'Request Count']]
        for _, row in slow_requests.iterrows():
            print(f"  - {row['Name']}: {row['Average Response Time']:.2f}ms ({row['Request Count']} ìš”ì²­)")
        
        # ì˜¤ë¥˜ìœ¨ì´ ë†’ì€ ìš”ì²­ë“¤
        df['Error Rate'] = (df['Failure Count'] / df['Request Count'] * 100).fillna(0)
        print(f"\nâŒ ì˜¤ë¥˜ìœ¨ì´ ë†’ì€ ìš”ì²­ TOP 5:")
        error_requests = df[df['Error Rate'] > 0].nlargest(5, 'Error Rate')[['Name', 'Error Rate', 'Failure Count']]
        for _, row in error_requests.iterrows():
            print(f"  - {row['Name']}: {row['Error Rate']:.2f}% ({int(row['Failure Count'])} ì‹¤íŒ¨)")
        
        # ê·¸ë˜í”„ ìƒì„±
        create_performance_charts(df)
        
    except Exception as e:
        print(f"âŒ CSV ë¶„ì„ ì˜¤ë¥˜: {e}")

def create_performance_charts(df):
    """ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„± ì¤‘...")
    
    # ì°¨íŠ¸ ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Stock Kafka Load Test Results', fontsize=16, fontweight='bold')
    
    # 1. ì‘ë‹µì‹œê°„ ë¶„í¬
    axes[0, 0].bar(range(len(df)), df['Average Response Time'], color='skyblue', alpha=0.7)
    axes[0, 0].set_title('Average Response Time by Request')
    axes[0, 0].set_xlabel('Request Index')
    axes[0, 0].set_ylabel('Response Time (ms)')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ì²˜ë¦¬ëŸ‰ ë¶„í¬  
    axes[0, 1].bar(range(len(df)), df['Requests/s'], color='lightgreen', alpha=0.7)
    axes[0, 1].set_title('Throughput by Request')
    axes[0, 1].set_xlabel('Request Index')
    axes[0, 1].set_ylabel('Requests/s')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ìš”ì²­ ìˆ˜ vs ì˜¤ë¥˜ìœ¨
    df['Error Rate'] = (df['Failure Count'] / df['Request Count'] * 100).fillna(0)
    scatter = axes[1, 0].scatter(df['Request Count'], df['Error Rate'], 
                                c=df['Average Response Time'], cmap='viridis', alpha=0.7)
    axes[1, 0].set_title('Request Count vs Error Rate')
    axes[1, 0].set_xlabel('Request Count')
    axes[1, 0].set_ylabel('Error Rate (%)')
    axes[1, 0].grid(True, alpha=0.3)
    plt.colorbar(scatter, ax=axes[1, 0], label='Avg Response Time (ms)')
    
    # 4. ì„±ëŠ¥ ìš”ì•½ (í…ìŠ¤íŠ¸)
    axes[1, 1].axis('off')
    summary_text = f"""
    ğŸ“Š Performance Summary
    
    Total Requests: {df['Request Count'].sum():,}
    Total Failures: {df['Failure Count'].sum():,}
    
    Average Response Time: {df['Average Response Time'].mean():.2f}ms
    Max Response Time: {df['Max Response Time'].max():,}ms
    
    Average Throughput: {df['Requests/s'].mean():.2f} req/s
    Max Throughput: {df['Requests/s'].max():.2f} req/s
    
    Overall Error Rate: {(df['Failure Count'].sum() / df['Request Count'].sum() * 100):.2f}%
    
    Test Duration: {datetime.now().strftime('%Y-%m-%d %H:%M')}
    """
    
    axes[1, 1].text(0.1, 0.9, summary_text, fontsize=11, verticalalignment='top',
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.5))
    
    plt.tight_layout()
    
    # ì°¨íŠ¸ ì €ì¥
    chart_filename = f"performance_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
    plt.savefig(chart_filename, dpi=300, bbox_inches='tight')
    print(f"ğŸ’¾ ì°¨íŠ¸ ì €ì¥ë¨: {chart_filename}")
    
    # ì°¨íŠ¸ í‘œì‹œ (í™˜ê²½ì— ë”°ë¼)
    try:
        plt.show()
    except:
        print("ğŸ’¡ GUI í™˜ê²½ì´ ì•„ë‹ˆì–´ì„œ ì°¨íŠ¸ë¥¼ í™”ë©´ì— í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def analyze_performance_logs(log_file="performance.log"):
    """ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„"""
    
    print(f"\nğŸ“ ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„: {log_file}")
    
    if not os.path.exists(log_file):
        print(f"âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
        return
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        print(f"ğŸ“„ ë¡œê·¸ ë¼ì¸ ìˆ˜: {len(lines):,}")
        
        # ì£¼ìš” ì´ë²¤íŠ¸ ì¹´ìš´íŠ¸
        events = {
            'API ìš”ì²­': 0,
            'Kafka ë©”ì‹œì§€': 0,
            'DB ì‘ì—…': 0,
            'ê²½ê³ ': 0,
            'ì˜¤ë¥˜': 0
        }
        
        response_times = []
        
        for line in lines:
            if 'ğŸŒ ìš”ì²­:' in line or 'API ìš”ì²­' in line:
                events['API ìš”ì²­'] += 1
                # ì‘ë‹µì‹œê°„ ì¶”ì¶œ ì‹œë„
                if '(' in line and 'ì´ˆ)' in line:
                    try:
                        time_str = line.split('(')[1].split('ì´ˆ)')[0]
                        response_times.append(float(time_str))
                    except:
                        pass
            elif 'ğŸ“¤ Kafka' in line or 'Kafka ë©”ì‹œì§€' in line:
                events['Kafka ë©”ì‹œì§€'] += 1
            elif 'ğŸ—ƒï¸ DB' in line or 'DB ì‘ì—…' in line:
                events['DB ì‘ì—…'] += 1
            elif 'WARNING' in line or 'âš ï¸' in line:
                events['ê²½ê³ '] += 1
            elif 'ERROR' in line or 'âŒ' in line:
                events['ì˜¤ë¥˜'] += 1
        
        print(f"\nğŸ“Š ì´ë²¤íŠ¸ í†µê³„:")
        for event, count in events.items():
            print(f"  - {event}: {count:,}ê°œ")
        
        if response_times:
            print(f"\nâ±ï¸ ì‘ë‹µì‹œê°„ í†µê³„:")
            print(f"  - í‰ê· : {sum(response_times)/len(response_times):.3f}ì´ˆ")
            print(f"  - ìµœì†Œ: {min(response_times):.3f}ì´ˆ")
            print(f"  - ìµœëŒ€: {max(response_times):.3f}ì´ˆ")
            print(f"  - ì¤‘ì•™ê°’: {sorted(response_times)[len(response_times)//2]:.3f}ì´ˆ")
        
        # ì˜¤ë¥˜ìœ¨ ê³„ì‚°
        total_operations = events['API ìš”ì²­'] + events['Kafka ë©”ì‹œì§€'] + events['DB ì‘ì—…']
        if total_operations > 0:
            error_rate = (events['ì˜¤ë¥˜'] / total_operations) * 100
            print(f"  - ì „ì²´ ì˜¤ë¥˜ìœ¨: {error_rate:.2f}%")
        
    except Exception as e:
        print(f"âŒ ë¡œê·¸ ë¶„ì„ ì˜¤ë¥˜: {e}")

def generate_load_test_report():
    """ì¢…í•© ë¶€í•˜í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
    
    print(f"\nğŸ“‹ ì¢…í•© ë¶€í•˜í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f"load_test_report_{timestamp}.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"""# Stock Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ

## ğŸ“… í…ŒìŠ¤íŠ¸ ì •ë³´
- **ì‹¤í–‰ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **í…ŒìŠ¤íŠ¸ í™˜ê²½**: Stock Kafka3 í”„ë¡œì íŠ¸
- **ë¶„ì„ ë„êµ¬**: Python, Locust, Prometheus

## ğŸ¯ í…ŒìŠ¤íŠ¸ ëª©í‘œ
1. **API ì‘ë‹µì„±ëŠ¥** ì¸¡ì •
2. **Kafka ë©”ì‹œì§€ ì²˜ë¦¬ëŸ‰** í™•ì¸  
3. **ë°ì´í„°ë² ì´ìŠ¤ ë™ì‹œì„±** í…ŒìŠ¤íŠ¸
4. **ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ** ëª¨ë‹ˆí„°ë§
5. **ì¥ì•  ëŒ€ì‘ ë©”ì»¤ë‹ˆì¦˜** ê²€ì¦

## ğŸ“Š ì£¼ìš” ì„±ê³¼ ì§€í‘œ (KPI)

### ì‘ë‹µì‹œê°„ (Response Time)
- ëª©í‘œ: í‰ê·  < 500ms, 95% < 1,000ms
- ê²°ê³¼: [ì‹¤ì œ í…ŒìŠ¤íŠ¸ í›„ ì…ë ¥]

### ì²˜ë¦¬ëŸ‰ (Throughput)  
- ëª©í‘œ: > 100 req/s
- ê²°ê³¼: [ì‹¤ì œ í…ŒìŠ¤íŠ¸ í›„ ì…ë ¥]

### ì˜¤ë¥˜ìœ¨ (Error Rate)
- ëª©í‘œ: < 1%
- ê²°ê³¼: [ì‹¤ì œ í…ŒìŠ¤íŠ¸ í›„ ì…ë ¥]

### ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
- CPU ì‚¬ìš©ë¥ : [ì‹¤ì œ í…ŒìŠ¤íŠ¸ í›„ ì…ë ¥]
- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : [ì‹¤ì œ í…ŒìŠ¤íŠ¸ í›„ ì…ë ¥]
- ë””ìŠ¤í¬ I/O: [ì‹¤ì œ í…ŒìŠ¤íŠ¸ í›„ ì…ë ¥]

## ğŸ” í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### 1. API ë¶€í•˜í…ŒìŠ¤íŠ¸
- **ë„êµ¬**: Locust
- **ì‹œë‚˜ë¦¬ì˜¤**: ë™ì‹œ ì‚¬ìš©ì ì¦ê°€
- **ì¸¡ì • í•­ëª©**: ì‘ë‹µì‹œê°„, ì²˜ë¦¬ëŸ‰, ì˜¤ë¥˜ìœ¨

### 2. Kafka ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
- **ë„êµ¬**: Python ë©€í‹°ìŠ¤ë ˆë”©
- **ì‹œë‚˜ë¦¬ì˜¤**: ëŒ€ëŸ‰ ë©”ì‹œì§€ ë™ì‹œ ì „ì†¡
- **ì¸¡ì • í•­ëª©**: ë©”ì‹œì§€ ì²˜ë¦¬ëŸ‰, ì§€ì—°ì‹œê°„, ì†ì‹¤ë¥ 

### 3. ë°ì´í„°ë² ì´ìŠ¤ ë¶€í•˜í…ŒìŠ¤íŠ¸
- **ë„êµ¬**: DuckDB ë™ì‹œ ì—°ê²°
- **ì‹œë‚˜ë¦¬ì˜¤**: ë³µì¡í•œ ì¿¼ë¦¬ ë™ì‹œ ì‹¤í–‰
- **ì¸¡ì • í•­ëª©**: ì¿¼ë¦¬ ì‹¤í–‰ì‹œê°„, ë™ì‹œ ì—°ê²° ìˆ˜

## ğŸ›¡ï¸ ì¥ì•  ëŒ€ì‘ ë©”ì»¤ë‹ˆì¦˜

### 1. ì¬ì‹œë„ ë¡œì§ (Retry)
- **ì ìš© ìœ„ì¹˜**: API í˜¸ì¶œ, DB ì—°ê²°, Kafka ì „ì†¡
- **ì¬ì‹œë„ íšŸìˆ˜**: 3íšŒ
- **ë°±ì˜¤í”„ ì „ëµ**: ì§€ìˆ˜ì  ì¦ê°€ (1s â†’ 2s â†’ 4s)

### 2. Circuit Breaker
- **ì‹¤íŒ¨ ì„ê³„ê°’**: 5íšŒ ì—°ì† ì‹¤íŒ¨
- **íƒ€ì„ì•„ì›ƒ**: 60ì´ˆ
- **ë°˜ê°œë°© í…ŒìŠ¤íŠ¸**: ìë™ ë³µêµ¬ ì‹œë„

### 3. ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ
- **Prometheus ë©”íŠ¸ë¦­**: ì‹¤ì‹œê°„ ìˆ˜ì§‘
- **ë¡œê¹… ì‹œìŠ¤í…œ**: êµ¬ì¡°í™”ëœ ë¡œê·¸
- **ì„±ëŠ¥ ì„ê³„ê°’**: CPU 80%, ë©”ëª¨ë¦¬ 85%

## ğŸ“ˆ ê°œì„  ì‚¬í•­

### ì„±ëŠ¥ ìµœì í™”
1. **ë°ì´í„°ë² ì´ìŠ¤**
   - DuckDB ë°°ì¹˜ ì²˜ë¦¬ë¡œ 20-300ë°° ì„±ëŠ¥ í–¥ìƒ
   - ì¸ë±ìŠ¤ ìµœì í™”

2. **ë¹„ë™ê¸° ì²˜ë¦¬**
   - asyncioë¥¼ í™œìš©í•œ ë™ì‹œì„± í™•ë³´
   - Kafka ë°°ì¹˜ ì „ì†¡

3. **ìºì‹± ì „ëµ**
   - Redis ìºì‹œ í™œìš© (ê³„íš)
   - ìì£¼ ì¡°íšŒë˜ëŠ” ë°ì´í„° ìºì‹±

### ëª¨ë‹ˆí„°ë§ ê°•í™”
1. **ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ**
   - Grafana ì—°ë™ (ê³„íš)
   - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

2. **ì•ŒëŒ ì‹œìŠ¤í…œ**
   - ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼
   - ì¥ì•  ìë™ ê°ì§€

## ğŸ¯ í–¥í›„ ê³„íš

### ë‹¨ê¸° (1-2ì£¼)
- [ ] Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- [ ] ì•ŒëŒ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë¶€í•˜í…ŒìŠ¤íŠ¸ ìë™í™”

### ì¤‘ê¸° (1ê°œì›”)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ìˆ˜ë¦½
- [ ] ì§€ì†ì  ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] ì¥ì•  ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸

### ì¥ê¸° (3ê°œì›”)
- [ ] ìë™ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„
- [ ] ë‹¤ì¤‘ ì§€ì—­ ë°°í¬ í…ŒìŠ¤íŠ¸
- [ ] ì¬í•´ ë³µêµ¬ ê³„íš ìˆ˜ë¦½

## ğŸ“ ê²°ë¡ 

[í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ì‘ì„±]

---
**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ë¶„ì„ ë„êµ¬**: Stock Kafka Load Test Analyzer v1.0
""")
    
    print(f"ğŸ“„ ë³´ê³ ì„œ ìƒì„±ë¨: {report_file}")
    return report_file

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='ë¶€í•˜í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë„êµ¬')
    parser.add_argument('--csv-pattern', default='load_test_results_*.csv', 
                       help='Locust CSV íŒŒì¼ íŒ¨í„´')
    parser.add_argument('--log-file', default='performance.log',
                       help='ì„±ëŠ¥ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--generate-report', action='store_true',
                       help='ì¢…í•© ë³´ê³ ì„œ ìƒì„±')
    
    args = parser.parse_args()
    
    print("ğŸ“Š Stock Kafka ë¶€í•˜í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ê¸°")
    print("=" * 60)
    
    # 1. Locust ê²°ê³¼ ë¶„ì„
    analyze_locust_results(args.csv_pattern)
    
    # 2. ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„  
    analyze_performance_logs(args.log_file)
    
    # 3. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    if args.generate_report:
        generate_load_test_report()
    
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“… ë¶„ì„ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main()
