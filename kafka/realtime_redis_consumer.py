#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
import os
import sys

# ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œëŠ” /appì´ ê¸°ë³¸ ê²½ë¡œì´ë¯€ë¡œ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, '/app')

# PySpark Structured Streaming
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType

# ë¡œì»¬ íŒ¨í‚¤ì§€ë“¤
from common.redis_manager import RedisManager
from common.technical_indicator_calculator import TechnicalIndicatorCalculator
from config.kafka_config import KafkaConfig

def process_realtime_data_with_spark():
    """Sparkë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ë° Redis ì €ì¥ + ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
    try:
        print("ğŸš€ Redis + ê¸°ìˆ ì  ì§€í‘œ í†µí•© Spark ì²˜ë¦¬ ì‹œì‘")
        
        # Redis ë° ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        redis_manager = RedisManager()
        indicator_calculator = TechnicalIndicatorCalculator()
        
        # Redis ì—°ê²° í…ŒìŠ¤íŠ¸
        if redis_manager.redis_client.ping():
            print("âœ… Redis ì—°ê²° ì„±ê³µ")
        else:
            print("âŒ Redis ì—°ê²° ì‹¤íŒ¨")
            return
        
        # Spark ì„¤ì •
        spark = SparkSession.builder \
            .appName("RealTimeStockDataProcessorWithRedis") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        
        # Kafka ì„¤ì • (config íŒŒì¼ ì‚¬ìš©)
        kafka_bootstrap_servers = "kafka:29092"  # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì£¼ì†Œ
        topics = [KafkaConfig.TOPIC_KIS_STOCK, KafkaConfig.TOPIC_YFINANCE_STOCK]
        consumer_group = KafkaConfig.CONSUMER_GROUP_REALTIME
        
        print(f"ğŸ“¡ Kafka ì„œë²„: {kafka_bootstrap_servers}")
        print(f"ğŸ“‹ êµ¬ë… í† í”½: {topics}")
        print(f"ğŸ‘¥ Consumer Group: {consumer_group}")
        
        # JSON ìŠ¤í‚¤ë§ˆ ì •ì˜
        schema = StructType([
            StructField("symbol", StringType(), True),
            StructField("price", DoubleType(), True),
            StructField("source", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("volume", StringType(), True),
            StructField("change", StringType(), True),
            StructField("change_percent", StringType(), True)
        ])
        
        def process_batch(batch_df, batch_id):
            """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜"""
            try:
                print(f"\nğŸ“¦ ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹œì‘...")
                
                # ë°°ì¹˜ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                count = batch_df.count()
                if count == 0:
                    print(f"ğŸ“¦ ë°°ì¹˜ {batch_id}: ë°ì´í„° ì—†ìŒ")
                    return
                
                print(f"ğŸ“¦ ë°°ì¹˜ {batch_id}: {count}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬")
                
                # ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
                rows = batch_df.collect()
                processed_count = 0
                indicator_count = 0
                
                for row in rows:
                    try:
                        # ë©”ì‹œì§€ íŒŒì‹±
                        message_data = json.loads(row.message)
                        symbol = message_data.get('symbol')
                        price = message_data.get('price')
                        source = message_data.get('source', 'unknown')
                        
                        if not symbol or not price:
                            print(f"âš ï¸ í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: symbol={symbol}, price={price}")
                            continue
                        
                        current_price = float(price)
                        print(f"ğŸ“Š ì²˜ë¦¬ì¤‘: {symbol} = ${current_price:.2f} ({source})")
                        
                        # 1. Redisì— ì‹¤ì‹œê°„ ë°ì´í„° ì €ì¥
                        redis_data = {
                            'symbol': symbol,
                            'price': current_price,
                            'source': source,
                            'timestamp': datetime.now().isoformat(),
                            'kafka_timestamp': row.kafka_timestamp.isoformat() if row.kafka_timestamp else None,
                            'volume': message_data.get('volume'),
                            'change': message_data.get('change'),
                            'change_percent': message_data.get('change_percent')
                        }
                        
                        # Redis ì €ì¥
                        success = redis_manager.store_realtime_data(symbol, redis_data)
                        if success:
                            processed_count += 1
                        
                        # 2. ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (ê³¼ê±° ë°ì´í„° + í˜„ì¬ ê°€ê²©)
                        try:
                            indicators = indicator_calculator.calculate_all_indicators(
                                symbol, 
                                current_price=current_price
                            )
                            
                            if indicators:
                                # Redisì— ê¸°ìˆ ì  ì§€í‘œ ì €ì¥
                                redis_manager.store_technical_indicators(symbol, indicators)
                                indicator_count += 1
                                
                                # ì£¼ìš” ì§€í‘œ ë¡œê¹…
                                rsi = indicators.get('rsi')
                                macd = indicators.get('macd')
                                sentiment = indicators.get('overall_sentiment', 'neutral')
                                strength = indicators.get('strength', 0)
                                
                                print(f"  ğŸ“ˆ {symbol} ì§€í‘œ: RSI={rsi}, MACD={macd:.4f if macd else None}, "
                                      f"ì‹ í˜¸={sentiment}({strength})")
                                
                                # ì¤‘ìš”í•œ ë§¤ë§¤ ì‹ í˜¸ë§Œ ì¶œë ¥
                                signals = indicators.get('signals', [])
                                if signals and abs(strength) > 30:  # ê°•í•œ ì‹ í˜¸ë§Œ
                                    print(f"  ğŸš¨ {symbol} ì¤‘ìš”ì‹ í˜¸: {signals[0]}")
                                
                            else:
                                print(f"  âš ï¸ {symbol}: ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ë¶ˆê°€ (ë°ì´í„° ë¶€ì¡±)")
                                
                        except Exception as indicator_error:
                            print(f"âŒ {symbol} ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {indicator_error}")
                        
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                    except Exception as e:
                        print(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                print(f"âœ… ë°°ì¹˜ {batch_id} ì™„ë£Œ: Redis={processed_count}, ì§€í‘œ={indicator_count}")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {batch_id} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
        
        # Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸° (consumer group ì„¤ì • í¬í•¨)
        df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
            .option("subscribe", ",".join(topics)) \
            .option("kafka.group.id", "spark-realtime-consumer-group") \
            .option("kafka.client.id", "spark-realtime-client") \
            .option("startingOffsets", "latest") \
            .option("failOnDataLoss", "false") \
            .load()
        
        # ë©”ì‹œì§€ íŒŒì‹±
        parsed_df = df.select(
            col("topic").cast("string"),
            col("timestamp").cast("timestamp").alias("kafka_timestamp"),
            col("value").cast("string").alias("message")
        )
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ (consumer groupë³„ ì²´í¬í¬ì¸íŠ¸)
        query = parsed_df.writeStream \
            .outputMode("append") \
            .foreachBatch(process_batch) \
            .option("checkpointLocation", "/data/checkpoints/spark-realtime-consumer-group") \
            .trigger(processingTime="30 seconds") \
            .start()
        
        print("âœ… Redis + ê¸°ìˆ ì  ì§€í‘œ í†µí•© ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")
        print("ğŸ“Š 30ì´ˆë§ˆë‹¤ ë°°ì¹˜ ì²˜ë¦¬ë¨")
        print("ğŸ”„ Ctrl+Cë¡œ ì¢…ë£Œ...")
        
        # ì¿¼ë¦¬ ëŒ€ê¸°
        query.awaitTermination()
        
    except Exception as e:
        print(f"âŒ Spark ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            if 'redis_manager' in locals():
                redis_manager.close()
                print("ğŸ”´ Redis ì—°ê²° ì¢…ë£Œ")
            if 'indicator_calculator' in locals():
                indicator_calculator.close()
                print("ğŸ“Š ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ê¸° ì¢…ë£Œ")
            if 'spark' in locals():
                spark.stop()
                print("âš¡ Spark ì„¸ì…˜ ì¢…ë£Œ")
        except:
            pass

# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Redis + ê¸°ìˆ ì  ì§€í‘œ í†µí•© ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œì‘")
    print(f"ğŸ“‚ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    try:
        process_realtime_data_with_spark()
    except KeyboardInterrupt:
        print("\nğŸ“¢ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­ ê°ì§€")
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    finally:
        print("ğŸ“¢ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")

if __name__ == "__main__":
    main()
