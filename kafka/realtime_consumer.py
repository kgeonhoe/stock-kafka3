#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional
import os
import sys

# ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œëŠ” /appì´ ê¸°ë³¸ ê²½ë¡œì´ë¯€ë¡œ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, '/app')

# Kafka íŒ¨í‚¤ì§€ import ì œê±° (PySpark Structured Streamingë§Œ ì‚¬ìš©)
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window, avg, expr, udf, from_json, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType

from common.postgresql_manager import PostgreSQLManager
from common.slack_notifier import SlackNotifier
from config.kafka_config import KafkaConfig

class RealtimeStockConsumer:
    """ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„° ì»¨ìŠˆë¨¸"""
    
    def __init__(self, bootstrap_servers: str = None,
                db_path: str = '/data/duckdb/stock_data.db'):
        """
        ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„° ì»¨ìŠˆë¨¸ ì´ˆê¸°í™”
        
        Args:
            bootstrap_servers: Kafka ì„œë²„ ì£¼ì†Œ
            db_path: DuckDB íŒŒì¼ ê²½ë¡œ
        """
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ Kafka ì„œë²„ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
        self.bootstrap_servers = bootstrap_servers or os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:29092')
        print(f"ğŸ”— Kafka ì„œë²„: {self.bootstrap_servers}")
        
        self.db_path = db_path
        self.slack = SlackNotifier()
        self.spark = self._create_spark_session()
        self.running = True
    
    def _create_spark_session(self) -> SparkSession:
        """
        ìŠ¤íŒŒí¬ ì„¸ì…˜ ìƒì„±
        
        Returns:
            SparkSession ì¸ìŠ¤í„´ìŠ¤
        """
        spark = (SparkSession.builder
                .appName("RealtimeStockAnalysis")
                .config("spark.driver.memory", "1g")
                .config("spark.executor.memory", "1g")
                .config("spark.sql.shuffle.partitions", "4")
                .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true")
                .config("spark.sql.streaming.kafka.useDeprecatedOffsetFetching", "false")
                .getOrCreate())
        
        spark.sparkContext.setLogLevel("WARN")
        print("âœ… Spark ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
        return spark
    
    def process_realtime_data_with_spark(self):
        """ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬"""
        print("ğŸš€ ìŠ¤íŒŒí¬ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘")
        
        # ìŠ¤í‚¤ë§ˆ ì •ì˜ (test_multi_source_producerì˜ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ)
        schema = StructType([
            StructField("symbol", StringType(), True),
            StructField("price", DoubleType(), True),
            StructField("current_price", DoubleType(), True),
            StructField("volume", IntegerType(), True),
            StructField("data_source", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("change", DoubleType(), True),
            StructField("change_rate", DoubleType(), True)
        ])
        
        try:
            # Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°
            print(f"ğŸ“¡ Kafka í† í”½ êµ¬ë…: {KafkaConfig.TOPIC_KIS_STOCK}, {KafkaConfig.TOPIC_YFINANCE_STOCK}")
            
            df = self.spark \
                .readStream \
                .format("kafka") \
                .option("kafka.bootstrap.servers", self.bootstrap_servers) \
                .option("subscribe", f"{KafkaConfig.TOPIC_KIS_STOCK},{KafkaConfig.TOPIC_YFINANCE_STOCK}") \
                .option("startingOffsets", "latest") \
                .option("kafka.group.id", "stock-consumer-group") \
                .option("kafka.session.timeout.ms", "30000") \
                .option("kafka.request.timeout.ms", "40000") \
                .option("failOnDataLoss", "false") \
                .load()
            
            print("âœ… Kafka ìŠ¤íŠ¸ë¦¼ ì—°ê²° ì„±ê³µ")
            
            # JSON íŒŒì‹±
            parsed_df = df \
                .select(
                    col("topic"),
                    col("timestamp").alias("kafka_timestamp"),
                    from_json(col("value").cast("string"), schema).alias("data")
                ) \
                .select("topic", "kafka_timestamp", "data.*")
            
            # ê°„ë‹¨í•œ ì½˜ì†” ì¶œë ¥ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            query = parsed_df \
                .writeStream \
                .outputMode("append") \
                .format("console") \
                .option("truncate", False) \
                .option("checkpointLocation", "/data/checkpoints/console") \
                .start()
            
            print("ğŸ¯ ì½˜ì†” ì¶œë ¥ ì‹œì‘ - ë°ì´í„° ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
            
            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì§„í–‰
            query.awaitTermination()
            
        except Exception as e:
            print(f"âŒ Kafka ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self, 'db') and self.db:
            self.db.close()
        if self.spark:
            self.spark.stop()
        self.running = False
        print("ğŸ“¢ ì»¨ìŠˆë¨¸ ì¢…ë£Œ")

# ë©”ì¸ í•¨ìˆ˜
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Realtime Stock Consumer ì‹œì‘")
    print(f"ğŸ“‚ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"ğŸ”§ Python ê²½ë¡œ: {sys.path}")
    
    consumer = None
    try:
        consumer = RealtimeStockConsumer()
        consumer.process_realtime_data_with_spark()
    except KeyboardInterrupt:
        print("\nğŸ“¢ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­ ê°ì§€")
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    finally:
        if consumer:
            consumer.close()
        print("ğŸ“¢ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")

# ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
if __name__ == "__main__":
    main()
